[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Empirical Finance",
    "section": "",
    "text": "About This Book\nThis book is aimed at students in the MSc in Finance program at HEC Montr√©al taking the course MATH 60230 Empirical Finance. I make it publicly available for anyone interested in learning Python for finance, but some examples and explanations are tailored to the HEC Montr√©al context.\nThe main objective of this book is to offer a thorough and accessible practical guide to empirical finance research using Python. To achieve this goal, I cover statistical and econometric techniques used in empirical finance, with a focus on practical applications using Python. I believe that learning by doing is the most effective way to master new skills. Thus, I present real-world scenarios and datasets, enabling you to see the power and efficacy of these techniques in action.\nNo prior programming experience is required. If you are already familiar with Python, I still recommend you at least skim Part I because I propose not only an introduction to Python, but an introduction to a modern workflow for data analysis with Python.\nMy goal is that by the end of this book, you will have an advanced understanding of the main econometric techniques used in empirical finance and a solid grounding in modern Python programming for data analysis. I look forward to guiding you through this exciting journey into the world of empirical finance, statistics, econometrics, and Python programming.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Empirical Finance",
    "section": "",
    "text": "Tip\n\n\n\nThis book is also available as a PDF file.\n\n\n\n\n\n\n\n\nNoteWork in Progress\n\n\n\nThis book is a work in progress. I am constantly adding new content and refining existing content. If you have any suggestions or feedback, please reach out at vincent.gregoire@hec.ca.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "Empirical Finance",
    "section": "Structure",
    "text": "Structure\nThis book is organized into seven parts plus appendices, each designed to build upon the knowledge from the previous section, ultimately guiding you to a robust understanding of empirical finance using Python.\nPart I: Python Fundamentals, Environment, and Best Practices\nThe first part of the book is devoted to familiarizing you with Python and setting up the necessary coding environment. We begin with instructions on installing Python and understanding its basic syntax. We then introduce various tools that are part of the programmer‚Äôs toolbox, including the terminal, VS Code, Git, and GitHub. You will learn about managing Python environments with uv, writing clean and well-documented code, testing your code, and object-oriented programming concepts.\nPart II: Working with Data\nIn the second part, we focus on data manipulation and analysis. You will learn how to load data from various sources and formats, work with DataFrames using pandas and Polars, clean and transform data, structure datasets for analysis, merge multiple data sources, and reshape data between wide and long formats.\nPart III: Visualization and Research Output\nThe third part covers how to communicate your findings effectively. We explore data visualization with matplotlib and seaborn, creating publication-quality tables for regression results and summary statistics, and using Quarto to create reproducible research documents that combine code, text, and results.\nPart IV: Statistical Foundations\nIn the fourth part, we dive into the statistical foundations needed for empirical finance. We cover numerical computing with NumPy, probability distributions and random number generation, and descriptive statistics for financial data.\nPart V: Regression Methods\nThe fifth part focuses on econometric techniques central to empirical finance research. We cover linear regression with statsmodels, panel data methods for working with firm-time observations, and instrumental variables estimation for addressing endogeneity.\nPart VI: Machine Learning and Artificial Intelligence in Research\nThe sixth part introduces modern AI and machine learning tools relevant to finance research. We discuss how to use AI assistants effectively for coding and research, machine learning techniques for prediction and classification, and natural language processing methods for analyzing textual data.\nPart VII: Reproducibility and Replication\nThe final part addresses the critical importance of reproducibility in research. We cover best practices for organizing research projects, managing dependencies, and ensuring that your results can be replicated by others.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#learning-approach",
    "href": "index.html#learning-approach",
    "title": "Empirical Finance",
    "section": "Learning Approach",
    "text": "Learning Approach\nThe learning approach adopted in this book is designed to be practical and closely linked with the real-world challenges encountered in empirical finance. My philosophy is grounded in the belief that the best way to learn is by doing, especially when it comes to mastering complex concepts like econometrics and programming.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#youtube-video-tutorials",
    "href": "index.html#youtube-video-tutorials",
    "title": "Empirical Finance",
    "section": "YouTube Video Tutorials",
    "text": "YouTube Video Tutorials\nThroughout the book, I provide links to YouTube videos that offer alternative explanations of the concepts covered in the chapters. These videos are not meant to replace the book, but rather to provide additional perspectives and clarifications. Some of these videos are created by me and are available on my YouTube channel, Vincent Codes Finance.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#tech-stack",
    "href": "index.html#tech-stack",
    "title": "Empirical Finance",
    "section": "Tech Stack",
    "text": "Tech Stack\nThis book is structured around a tech stack formed by a specific set of tools that has been carefully chosen based on their wide adoption, robustness, versatility, and compatibility with each other. While alternative tools exist and may be equally capable, the book takes an opinionated approach, focusing on this particular stack for clarity and consistency. It‚Äôs worth noting that the concepts and techniques covered in this book can be applied with other tools as well, but the specific examples and code use the following:\n\nPython 3.14 (released in October 2025)\nuv for managing Python versions and environments\nVisual Studio Code for writing code\nGit and GitHub for version control and collaboration\nClaude, ChatGPT, and Microsoft Copilot\nClaude Code, OpenAI Codex, and GitHub Copilot for coding assistance\nQuarto for writing technical content",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#use-of-ai",
    "href": "index.html#use-of-ai",
    "title": "Empirical Finance",
    "section": "Use of AI",
    "text": "Use of AI\nThis book was written with substantial assistance from AI tools, primarily ChatGPT and Claude Code. AI was used for all aspects of the book‚Äôs creation, including idea generation, creating outlines, drafting content, proofreading, and generating code examples. This reflects the modern reality of software development and technical writing, where AI assistants have become valuable collaborators.\nHowever, all content has been reviewed and edited by a human. I take full responsibility for the accuracy and quality of the material presented. Any errors or omissions remain my responsibility.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Empirical Finance",
    "section": "About the Author",
    "text": "About the Author\nI‚Äôm Vincent Gr√©goire, CFA, a Professor of Finance at HEC Montr√©al and the Canada Research Chair in Finance and Technology. I teach empirical finance with a strong emphasis on Python-based data analysis. I earned a Ph.D.¬†in Finance from the University of British Columbia, along with degrees in Computer Engineering and Financial Engineering from Universit√© Laval, and previously served as Chief Data Scientist at Berkindale Analytics, a fintech startup.\nMy work focuses on how information is produced, processed, and priced in financial markets. I study market structure through the lens of big data, machine learning, algorithmic trading, and cybersecurity, with an emphasis on methods that actually scale outside toy examples.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Empirical Finance",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI am grateful to Charles Martineau and Saad Ali Khan for their feedback and suggestions on the book.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "python/index.html",
    "href": "python/index.html",
    "title": "Python",
    "section": "",
    "text": "What is Python?\nThis part introduces Python, an open-source, high-level programming language that has become indispensable for empirical finance. I aim to provide readers with a foundational understanding of Python‚Äôs capabilities and how to leverage it for financial research. Starting from the basics, we will explore why Python is the go-to tool for researchers in finance.\nPython is a versatile and user-friendly programming language that emphasizes readability and simplicity. Its design philosophy promotes code that is easy to write and understand, making it an excellent choice for both beginners and experienced programmers. The name Python also refers to the interpreter, the program that runs Python code.",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "python/index.html#why-python-for-finance",
    "href": "python/index.html#why-python-for-finance",
    "title": "Python",
    "section": "Why Python for Finance?",
    "text": "Why Python for Finance?\nPython has been widely adopted by the finance industry and finance researchers for several compelling reasons:\nOpen source\nPython is free and open-source. Yes, that means you can use it for free, but open-source is much more than that. Python is distributed under the Python Software Foundation License, which is a permissive license that allows you to use, modify, and distribute the code and derived works based on it. This has led to a vibrant ecosystem of libraries and tools that are freely available to all, and to private forks of Python that are used internally by large financial institutions know collectively as bank Python.\nEase of learning\nPython‚Äôs syntax is designed to be intuitive and human-readable, making it an accessible language for beginners and experts alike. This simplicity allows finance professionals‚Äîmany of whom may not have a computer science background‚Äîto quickly learn Python and apply it to their work. Python code often reads like plain English, reducing the learning curve and enabling users to focus on problem-solving rather than struggling with the syntax.\nFor academic researchers, this ease of learning means that Python can be introduced in undergraduate or graduate programs with minimal friction. Students can rapidly transition from learning the basics of the language to applying it in real-world financial scenarios, such as data analysis, statistical modeling, and portfolio optimization.\nPowerful\nPython is exceptionally powerful due to its extensive library ecosystem. Libraries like NumPy, SciPy, and statsmodels provide robust tools for numerical and statistical computations, while pandas and polars facilitate data manipulation and analysis. These capabilities make Python an ideal choice for tasks ranging from simple data cleaning to complex econometric modeling.\nIn addition to its computational capabilities, Python integrates seamlessly with other programming languages and platforms, enabling finance practitioners to incorporate Python into larger, multi-language workflows. For example, it can call high-performance code written in C++ or Rust, interact with databases through SQL, or interface with scentific languages like R or Julia. This power and flexibility ensure that Python remains suitable for both small-scale analyses and enterprise-level financial systems.\nVersatile\nPython‚Äôs versatility allows it to handle a wide range of tasks, making it a one-stop solution for financial workflows. Analysts can use Python for tasks such as data acquisition from APIs or through web scraping, performing statistical analyses, creating visualizations, and even building predictive models using machine learning libraries like scikit-learn.\nWidely-used\nIn 2024, Python surpassed Javascript to become the most widely used programming language in the world according to GitHub‚Äôs Octoverse. This rise in use is attributed to the growing importance of AI and data science, for which Python is the most popular language.\nThis popularity ensures that Python skills are highly transferable and in demand, making it a valuable asset for finance professionals. Large financial institutions, such as JPMorgan Chase and Goldman Sachs, use Python extensively for data analysis, trading algorithms, and risk modeling. Financial data providers such as LSEG (formerly Refinitiv and Thomson Reuters) and WRDS, an academinc data provider, offer Python-based platforms for accessing and analyzing financial data. Python skills are now a must-have for finance professionals, so much so that the CFA Institute has added Python practical skills to its 2024 curriculum.\n\nOther languages\nPython is not the only game in town. R and Stata offer better capabilities for econometric and statistical modeling, and are also widely used in academic research. Julia and Matlab offer better performance for numerical computing, and C++ and Rust are the languages of choice for performance-critical parts of financial applications. Finally, SAS and many database softwares use a variant of SQL, while some use their own proprietary language like q for kdb+ which is a staple of high-frequency trading firms. Finally, OCaml is the language of choice at Jane Street, a large quantitative trading firm. Overall, each language has its strengths and weaknesses, and the choice of language depends on the specific task at hand, but Python is a very good choice for most tasks.",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "python/index.html#components-of-the-python-ecosystem",
    "href": "python/index.html#components-of-the-python-ecosystem",
    "title": "Python",
    "section": "Components of the Python Ecosystem",
    "text": "Components of the Python Ecosystem\nThe Python ecosystem consists of various tools and components that make it a powerful platform for data analysis. This section introduces the key elements of the ecosystem and their roles in creating efficient workflows.\n\nPython interpreter\nPython is an interpreted language, meaning that your code is executed by an interpreter when you run it rather than compiled ahead of time to an executable. The Python interpreter is responsiable for executing your Python code. It comes in various implementations, with the most common being CPython, the default implementation distributed with official Python releases, which is the one we will use in this book. Other variants include PyPy, a just-in-time (JIT) compiled interpreter that enhances performance for specific tasks, and Pyodide, a port of CPython to WebAssembly that allows Python to run in web browsers.\n\n\nPython libraries\nThe base Python language is simple, but it is extended through a large ecosystem of libraries. Python comes with a large standard library, that includes many features such as file input/output, basic data structures, and mathematical functions. However, most Python programs will leverage additional libraries. These libraries are pre-written modules that extend Python‚Äôs functionality. For empirical finance, some key libraries include:\n\npandas and polars: For data manipulation and analysis.\nNumPy and SciPy: For numerical computations.\nmatplotlib and seaborn: For data visualization.\nstatsmodels and linearmodels: For econometric modeling.\nscikit-learn: For machine learning.\n\nThese libraries form the backbone of financial analysis in Python, enabling everything from basic calculations to complex statistical modeling.\n\n\n\n\n\n\nWarning\n\n\n\nPython libraries are published on PyPI, the Python Package Index. Anyone can publish a library to PyPI, so it is important to check the library‚Äôs reputation before using it. Always keep in mind that librairies contain code that will be executed on your computer, so they can contain malware. Well-known libraries are less likely to contain vulnerabilities, but, as with any unreviewed software, there is always a risk. We will discuss security best practices in more detail in future chapters.\n\n\n\n\nEnvironment and package management\nPython versions are updated frequently.1 Libraries follow their own release cycles and are not always updated at the same time as the Python interpreter. Some libraries depend on specific versions of other libraries, so a chain of dependencies may need to be updated. To complicate matters, updates to libraries may break your code. This means that code that worked last month may not work this month if you always use the latest version of everything.\nTo minimize these issues, the best practice is to create a virtual environment for each project. This ensures that the versions of the Python interpreter and libraries are fixed and consistent for each project, and that you can easily update the libraries for a project without affecting your other projects.\nThere are several tools for managing Python environments. Python comes with venv, a built-in module for creating virtual environments, and pip, a package manager for installing and updating libraries. A popular alternative for data science projects is conda, part of the Anaconda distribution.2 Another popular tool is poetry. Conda and poetry act as both environment and package manager.\nIn this book, we will use uv, a tool that replaces venv, pip, and a suite of other tools. It brings a lot of modern features to the table, such as a lockfile to ensure reproducibility, ability to install and manage Python versions, and more. For me, its main advantage is its increadible speed which is in part due to an advanced caching mechanism, the use of hardlinks to avoid keeping multiple copies of each library, and a very fast dependancy resolver.3\n\n\nIntegrated Development Environments (IDEs)\nIDEs streamline coding by providing features such as syntax highlighting, debugging tools, and other tools to make your life easier. In this book, we will use Visual Studio Code (VS Code), an open-source code editor by Microsoft that is very popular in the data science community. It is not Python-specific, but it integrates seamlessly with Python through extensions. Because it is open-source, many forks have been created, such as Cursor, which adds a powerful AI engine to the editor, and Positron, a data science-oriented fork by Posit, the company behind RStudio and Quarto (still in beta at the time of writing).\nOther popular IDEs include PyCharm by JetBrains (commercial, but free for students and academics) and neovim, a terminal-based text editor with a steep learning curve that is popular among developers for its extensibility. Finally, Spyder, is an open-source IDE that was very popular in the Python scientific community, but has since been eclipsed by VS Code.\n\n\nNotebooks\nNotebooks, such as Jupyter Notebooks, are interactive environments where code, text, and visualizations can coexist. They have significant drawbacks for their use in robust, replicable research, but are nonetheless very popular in data science because of their simplicity. VS Code supports Jupyter notebooks natively with an extension. We will notebooks them in more details in this book, along with their shortcomings and ways to mitigate them. marimo is a new Python notebook interface that aims to address some of the issues with Jupyter Notebooks. While it has a long way to go to overtake Jupyter in the data science community, it shows a lot of promise.",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "python/index.html#footnotes",
    "href": "python/index.html#footnotes",
    "title": "Python",
    "section": "",
    "text": "The latest version at the time of writing is 3.14. Since 2018, Python releases have been annual in October.‚Ü©Ô∏é\nWhile conda is open-source, it uses by default the Anaconda Repository, which requires a paid subscription under some conditions. At the time of writing, it is free for academic use.‚Ü©Ô∏é\nThe main task of a package manager is to resolve dependencies, i.e.¬†to figure out which versions of libraries need to be installed to satisfy the dependencies of a project. This is a very complex task (NP-hard) that requires a lot of computation and heuristics.‚Ü©Ô∏é",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "python/install/index.html",
    "href": "python/install/index.html",
    "title": "1¬† Installing Python",
    "section": "",
    "text": "1.1 What you need for a complete Python environment\nIn this chapter, I cover installing Python 3.14 and the related tools for a complete coding environment.\nThe most common way to use Python is to install it locally on your computer. The instructions below will guide you through the process of installing the following tools:\nWe will also install the following tools that are not required to run Python code, but are useful when working on projects with code:",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#what-you-need-for-a-complete-python-environment",
    "href": "python/install/index.html#what-you-need-for-a-complete-python-environment",
    "title": "1¬† Installing Python",
    "section": "",
    "text": "uv: A package manager for Python. I use it to manage the external libraries used in projects. uv makes it easy to install and update libraries on a per-project basis, and to make sure all collaborators use the same library version.\nPython: The Python interpreter, which allows you to run Python code. We will install multiple versions using uv.\nVisual Studio Code: Visual Studio Code is a free source code editor made by Microsoft. Features include support for debugging, syntax highlighting and intelligent code completion. Users can install extensions that add additional functionality.\n\n\n\nGit and GitHub: I use Git to manage my code and GitHub to host my code online and collaborate with others. Git is a version control system that tracks code changes and keeps a full history of changes. GitHub is a website that hosts Git repositories and provides additional features for collaboration such as issue tracking and pull requests.\n\n\n\n\n\n\n\nNoteuv vs Poetry and Anaconda\n\n\n\n\n\nMost Python projects use external libraries. For example, we use the pandas library for data analysis. To manage these libraries, we need a package manager. I now recommend using uv instead of Poetry (also very good) and Anaconda. Anaconda was my package manager of choice for many years and it remains very popular, but like many I eventually switched to poetry and more recently to uv. Overall, uv brings many nice features, but the two main reasons why I settled on uv is that it is very fast and it lets you easily install and use multiple Python versions. While speed might seem a minor concern for a package manager, anyone who has spent minutes (plural) waiting for Anaconda to create a virtual environment and install all the dependencies will understand.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#installation",
    "href": "python/install/index.html#installation",
    "title": "1¬† Installing Python",
    "section": "1.2 Installation",
    "text": "1.2 Installation\n\n macOS Linux Windows\n\n\nuv\nThe simplest way to install uv on macOS is with their install script.1 First, you need to open the Terminal app. You can find it in the Applications/Utilities folder, or by using Spotlight (press Cmd+Space and type Terminal). Then run the following script:2\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nAlternatively, you can also install it using Homebrew3:\nbrew install uv\nTo check that uv is properly installed, run the following command in the Terminal app:\nuv --version\nVisual Studio Code\nDownload Visual Studio Code from code.visualstudio.com.\nGit and GitHub\nGit might already be installed on your Mac as a command-line tool if you have installed the Xcode tools. If not, you can get the official installer. You can also use Git directly in VS Code, or using a GUI client such as GitHub Desktop. I prefer to use the VS Code integration or the command-line tool, but many beginners prefer to use GitHub Desktop.\n\n\nTo be honest, if you‚Äôre using Linux, you probably already know how to install Python and other tools. The instructions below are for manual installation, but you probably want to use your distribution‚Äôs package manager instead.\nuv\nInstallation instructions can be found here.\nVisual Studio Code\nDownload Visual Studio Code from code.visualstudio.com.\nGit and GitHub\nGit is probably already installed on Linux as a command-line tool. You can also use Git directly in VS Code, or using a GUI client such as GitHub Desktop. I prefer to use the VS Code integration or the command-line tool, but many beginners prefer to use GitHub Desktop.\n\n\nuv\nThe simplest way to install uv on Windows is with their install script.4 First, you need to open Powershell. Then, run the following script:\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\nAlternatively, you can also install it using WinGet5:\nwinget install --id=astral-sh.uv  -e\nTo check that uv is properly installed, run the following command in Powershell (you may have to close and re-open Powershell):\nuv --version\nNote: If this does not work but you had a successful installation messsage, you may have to restart your computer for the uv command to be available. You may have to manually add the path to uv in your environment variables. Do to so, you first need to figure out where uv was installed on your computer. It will tell you after installing, but the default is C:\\Users\\YOURUSERNAME\\.local\\bin. Once you have that path, add it to enviroment variables (Control panel-&gt;Edit Environment variables).\nVisual Studio Code\nDownload Visual Studio Code from code.visualstudio.com.\nGit and GitHub\nTo use Git on Windows, you need to install the Git client, which is a command-line tool.\nYou can also use Git directly in VS Code, or using a GUI client such as GitHub Desktop, but you need to first install the Git client. I prefer to use the VS Code integration or the command-line tool, but many beginners prefer to use GitHub Desktop.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#creating-a-sandbox-environment",
    "href": "python/install/index.html#creating-a-sandbox-environment",
    "title": "1¬† Installing Python",
    "section": "1.3 Creating a sandbox environment",
    "text": "1.3 Creating a sandbox environment\nThe recommended way to work with environments in Python is to have unique enviromnents for each project. However, not everything is a project, so I like to have a ‚Äúsandbox‚Äù environment with all the libraries I use regularly. That way, when I want to try something quickly like reading a CSV file to explore it, I have this sandbox project ready to go. It used to be common to install these libraries in the default (or base) environment, but it can lead to issues when updating packages, so many Python distribution now lock the default environment to prevent you from installing packages.\nFor my sandbox environment, I will want at least the following libraries:\n\npandas: Data analysis library\nnumpy: Numerical computing library\nscipy: Scientific computing library\nmatplotlib: Plotting library\nseaborn: Plotting library\nstatsmodels: Statistical models\nscikit-learn: Machine learning library\nlinearmodels: Linear models for Python\npyarrow: Library for working with parquet files\njupyter: for Jupyter notebooks and the VS Code Python interactive window\npytest: Testing framework\n\nTo create this sandbox environment, I will use uv. First, I need to create a new directory for the environment. I will call it sandbox, but you can name name it whatever you want.6 Then, I need to create a new project in this directory:\n\n macOS Linux Windows\n\n\nmkdir ~/Documents/sandbox\ncd ~/Documents/sandbox\nuv init\n\n\nmkdir ~/sandbox\ncd ~/sandbox\nuv init\n\n\nFirst create a folder named sandbox where you want on your computer. Then, from Windows Explorer, open the folder in PowerShell using File-&gt;Open Windows PowerShell. You can then initialize your environment using uv:\nuv init\n\n\n\nThis creates a pyproject.toml file in the sandbox directory. This file contains the list of dependencies for the project (which will be empty for now).\nOnce the project is created, you can add the dependencies:\nuv add pandas numpy scipy matplotlib seaborn statsmodels scikit-learn linearmodels pyarrow jupyter pytest\nThis step updates the pyproject.toml file and creates a uv.lock file, which contains the exact version of each dependency. This file is used to make sure that all collaborators use the same version of each library. Note that because our dependencies are built on top of other libraries, uv will also install the dependencies of our dependencies.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#configuring-visual-studio-code",
    "href": "python/install/index.html#configuring-visual-studio-code",
    "title": "1¬† Installing Python",
    "section": "1.4 Configuring Visual Studio Code",
    "text": "1.4 Configuring Visual Studio Code\nVisual Studio Code is a free source code editor made by Microsoft. Features include support for debugging, syntax highlighting and intelligent code completion. While there are some built-in features for Python, most of the functionality comes from extensions. What I recommend is to use the profile feature of VS Code, which lets you define a set of extensions for each use case. For example, you can have a profile for Python development, another for R development, and another for LaTeX editing. This way, you can have a clean installation of VS Code and only install the extensions you need for each profile. Furthermore, each profile can have its specific settings and theming options.\nTo create a profile, click on the profile icon in the bottom left corner of the VS Code window. Then, under the Profiles section, click on Create Profile.\n\n\n\n\n\nGive the profile a name and select a distinctive icon. Make sure to copy from the Data Science template, which will install all the extensions you need for data analysis with Python.\n\n\n\n\n\nVS Code works best when you have a project (directory) open. To open a project, select Open Folder from the File menu and select the folder you want to open, for example, the sandbox folder we created earlier.\nTo open an interactive window, bring up the command palette by pressing Cmd+Shift+P (or Ctrl+Shift+P on Windows and Linux) and type Python: Create Interactive Window.\nAt this point, VS Code should have detected the virtual environment created by uv and should have asked you if you want to use it. If not, you can select it manually by clicking on the Python version in the top right corner of the interactive window.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#github.com-optional",
    "href": "python/install/index.html#github.com-optional",
    "title": "1¬† Installing Python",
    "section": "1.5 GitHub.com (optional)",
    "text": "1.5 GitHub.com (optional)\nYou do not need a GitHub account to have a complete Python environment. However, I recommend creating one because it will be useful later when we start working on projects.\nTo follow the some examples, you will need a GitHub account. You can create one for free at https://github.com/. GitHub offers many benefits to students and educators, including free access to GitHub Copilot and extra free hours for GitHub Codespaces. I highly recommend applying at GitHub Education if you are eligible.\nWhile GitHub is the leader in the space, GitLab is their main competitor offering similar features. Gitea is a fully open-source solution for those who prefer to self-host.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#github-codespaces",
    "href": "python/install/index.html#github-codespaces",
    "title": "1¬† Installing Python",
    "section": "1.6 Python in the cloud using Github Codespaces",
    "text": "1.6 Python in the cloud using Github Codespaces\nMany online platforms allow you to develop and run Python code without installing anything on your computer. If you want to use a cloud-based solution, I recommend using GitHub Codespaces.\nAll you need is a GitHub account. However, note that GitHub Codespaces is not free. At the time of this writing, you get 60 hours per month for free, or 90 hours if you signed up for the GitHub Student Developer Pack (this is for a 2-core machine, which is the smallest machine available). After that, you have to pay for it (the current rate is USD 0.18 per hour).\nMake sure to shut down your Codespace when you are not using it, otherwise you will run out of free hours very quickly.\n\n1.6.1 Other cloud alternatives\nThere are many other cloud-based alternatives. However, most are based on Jupyter notebooks, which can be interesting when you are learning Python, but are not ideal for robust, replicable research. Some of the most popular alternatives are:\n\nGoogle Colab\nCocalc\nWRDS Jupyter Hub (requires a WRDS subscription through your institution)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#whats-next",
    "href": "python/install/index.html#whats-next",
    "title": "1¬† Installing Python",
    "section": "1.7 What‚Äôs next?",
    "text": "1.7 What‚Äôs next?\nNow that you have a complete Python environment, you can start learning Python. The next chapter introduces the basic Python syntax.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#footnotes",
    "href": "python/install/index.html#footnotes",
    "title": "1¬† Installing Python",
    "section": "",
    "text": "See the uv website for more details and troubleshooting advice.‚Ü©Ô∏é\ncurl is a program that will download the script, and |, the pipe operator, will take the result (the downloaded script) and pass it as input to sh, which will execute the script.‚Ü©Ô∏é\nHomebrew is a package manager for macOS that allows you to install and update software from the command line. It simplifies the installation process and makes it easy to keep your software up-to-date.‚Ü©Ô∏é\nSee the uv website for more details and troubleshooting advice.‚Ü©Ô∏é\nWinGet is a package manager for Windows that allows you to install and update software from the command line. It simplifies the installation process and makes it easy to keep your software up-to-date.‚Ü©Ô∏é\nI avoid spaces and special characters as they can sometimes cause trouble.‚Ü©Ô∏é",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html",
    "href": "python/python-basics/index.html",
    "title": "2¬† Python Syntax",
    "section": "",
    "text": "2.1 Introduction\nIn this chapter, we lay the foundation for your programming skills by exploring the basic syntax of Python 3.14.\nMy aim is to make this process as accessible as possible for non-programmers while giving you the necessary tools to excel in the world of empirical finance research.\nThe objectives of this chapter are to:\nBy the end of this chapter, you will have a solid grasp of Python‚Äôs basic syntax, empowering you to use it as a versatile tool for finance-related tasks. Remember, the key to success in learning any programming language is practice. As you work through this tutorial, be sure to experiment with the examples provided and try writing your own code to reinforce your understanding.\nI am purposefully leaving out of this chapter more advanced topics such as object-oriented programming, testing, and logging. These topics are important, but they are not necessary to get started with Python, so will come later in the book.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#introduction",
    "href": "python/python-basics/index.html#introduction",
    "title": "2¬† Python Syntax",
    "section": "",
    "text": "Provide a gentle introduction to the basic syntax of Python, allowing you to read and understand Python code.\nEnable you to write simple programs that will serve as building blocks for more advanced applications.\nEquip you with the knowledge and confidence to further explore advanced topics in Python and its applications in finance.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#data-types",
    "href": "python/python-basics/index.html#data-types",
    "title": "2¬† Python Syntax",
    "section": "2.2 Data types",
    "text": "2.2 Data types\nThe Python language offers many built-in fundamental data types. These data types serve as the building blocks for working with different kinds of data, which is critical in many applications. The basic data types you should be familiar with are presented in Table¬†2.1.\n\n\n\nTable¬†2.1: Main data types in Python\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nExample\n\n\n\n\nInteger\nint\nIntegers represent whole positive and negative numbers. They are used for counting, indexing, and various arithmetic operations.\n1\n\n\nFloat-Point Number\nfloat\nFloats represent real numbers with decimals. They are used for working with financial data that require precision, such as interest rates, stock prices, and percentages.\n1.0\n\n\nComplex\ncomplex\nComplex numbers consist of real and imaginary parts, represented as a + bj. While less commonly used in finance, they may be relevant in specific advanced applications, such as signal processing or quantitative finance.\n1.0 + 2.0j\n\n\nBoolean\nbool\nBooleans represent the truth values of True and False. They are used in conditional statements, comparisons, and other logical operations.\nTrue\n\n\nText String\nstr\nStrings are sequences of characters used for storing and manipulating text data, such as stock symbols, company names, or descriptions.\n\"Hello\"\n\n\nBytes\nbytes\nBytes are sequences of integers in the range of 0-255, often used for representing binary data or encoding text. Bytes may be used when working with binary file formats or network communication.\nb\"Hello\"\n\n\nNone\nNone\nNone is a special data type representing the absence of a value or a null value. It is used to signify that a variable has not been assigned a value or that a function returns no value.\nNone\n\n\n\n\n\n\n\n2.2.1 Literals\nA literal is a notation for representing a fixed value in source code. For example, 42 is a literal for the integer value of forty-two. The following are examples of literals in Python. Each code block contains code and is followed by the output of the code.\n\n2.2.1.1 int\nint literals are written as positive and negative whole numbers.\n\n42\n\n42\n\n\n\n-99\n\n-99\n\n\nThey can also include underscores to make them more readable.\n\n1_000_000\n\n1000000\n\n\n\n\n2.2.1.2 float\nfloat literals are written as decimal numbers.\n\n2.25\n\n2.25\n\n\nThey can be written in scientific notation by using e to indicate the exponent.\n\n2.25e8\n\n225000000.0\n\n\nTo define a whole number literal as a float instead of an int, you can append a decimal point to the number.\n\n2.0\n\n2.0\n\n\n\n\n2.2.1.3 complex\nComplex numbers consist of a real part and an imaginary part, represented as a + bj.\n\n2.3 + 4.5j\n\n(2.3+4.5j)\n\n\n\n\n2.2.1.4 None\nNone is a special data type that represents the absence of a value or a null value. It is used to signify that a variable has not been assigned a value or that a function returns no value.\n\nNone\n\n\n\n2.2.1.5 bool\nbool is a data type that represents the truth values of True and False. They are used in conditional statements, comparisons, and other logical operations.\n\nTrue\n\nTrue\n\n\n\n\n\n2.2.2 str\nStrings are sequences of characters. Strings literals are written by enclosing a sequence of characters in single or double quotes. Note that double quotes are preferred by the ruff formatter, which is used in this book, but most Python environments will use single quotes by default when displaying strings.\n\n\"USD\"\n\n'USD'\n\n\nStrings are sequences of Unicode characters, which means they can represent any character in any language, including emojis.\n\n\"Bitcoin  üöÄ\"\n\n'Bitcoin  üöÄ'\n\n\nString literals can span multiple lines by enclosing them in triple quotes or triple double quotes. This is useful for writing multiline strings.\n\n# Multiline strings\n\n\"\"\"GAFA is a group of companies:\n\n- Google\n- Apple\n- Facebook\n- Amazon\n\n\"\"\"\n\n'GAFA is a group of companies:\\n\\n- Google\\n- Apple\\n- Facebook\\n- Amazon\\n\\n'\n\n\nMultiline strings, or any strings with special characters, can be displayed using the print function.\n\nprint(\n    \"\"\"GAFA is a group of companies:\n\n- Google\n- Apple\n- Facebook\n- Amazon\n\n\"\"\"\n)\n\nGAFA is a group of companies:\n\n- Google\n- Apple\n- Facebook\n- Amazon\n\n\n\n\n\n\n2.2.3 bytes\nbytes are sequences of integers in the range of 0-255. They are often used for representing binary data or encoding text. Bytes literals are written by prepending a string literal with b.\n\nb\"Hello\"\n\nb'Hello'\n\n\n\n\n\n\n\n\nCautionBytes vs strings\n\n\n\nBytes can be confused with strings, but they are not the same. Strings are sequences of Unicode characters, while bytes are sequences of integers in the range of 0-255. Bytes are often used for representing binary data or encoding text. In most cases, you will be working with strings, but you may encounter bytes when working with binary file formats or network communication.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#variables",
    "href": "python/python-basics/index.html#variables",
    "title": "2¬† Python Syntax",
    "section": "2.3 Variables",
    "text": "2.3 Variables\nA variable in Python is a named location in the computer‚Äôs memory that holds a value. It serves as a container for data, allowing you to reference and manipulate the data stored within it. Variables are created by assigning a value to a name using the assignment operator (=). They can store data of various types, such as integers, floats, strings, or even more complex data structures like lists.\nUnderstanding the concept of variables and their naming conventions will help you write clean, readable, and maintainable code. An overview of variable naming rules in Python is presented in Table¬†2.2, and Table¬†2.3 presents some examples of valid and invalid variable names.\n\n\n\nTable¬†2.2: Variable naming rules\n\n\n\n\n\n\n\n\n\nRule\nDescription\n\n\n\n\nCan contain letters, numbers, and underscores\nVariable names can include any combination of letters (both uppercase and lowercase), numbers, and underscores (_). Python variable names support Unicode characters, enabling you to use non-English characters in your variable names. However, they must follow the other rules mentioned below.\n\n\nCannot start with a number\nAlthough variable names can contain numbers, they must not begin with a number. For example, 1_stock is an invalid variable name, whereas stock_1 is valid.\n\n\nCannot be a reserved word\nPython has a set of reserved words (e.g., if, else, while) that have special meanings within the language. You should not use these words as variable names.\n\n\n\n\n\n\n\n\n\nTable¬†2.3: Variable naming examples\n\n\n\n\n\nValid\nInvalid\n\n\n\n\nticker\n1ceo\n\n\nfirm_size\n@price\n\n\ntotal_sum_2023\nclass\n\n\n_tmp_buffer\nfor\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCase-sensitive\n\n\n\nPython is case-sensitive, so ret and RET are two different variables.\n\n\nBeyond the rules mentioned above, there are also some conventions that you should follow when naming variables. These conventions are not enforced by Python, but they are widely adopted by the Python community. Table¬†2.4 summarizes the most common conventions.\n\n\n\nTable¬†2.4: Variable naming conventions\n\n\n\n\n\n\n\n\n\nConvention\nDescription\n\n\n\n\nUse lowercase letters and underscores for variable names\nTo enhance code readability, use lowercase letters for variable names and separate words with underscores. For example, market_cap is a recommended variable name, whereas MarketCap or marketCap are not. This naming convention is known as snake case.\n\n\nUse uppercase letters for constants\nConstants are values that do not change throughout the program. Use uppercase letters and separate words with underscores to differentiate them from regular variables. For example, INFLATION_TARGET is a suitable constant name. Note that Python does not support constants like other languages, so this is just a convention, but Python won‚Äôt stop you from changing the value of a constant.\n\n\n\n\n\n\nBy adhering to these guidelines, you will improve your coding style and ensure that your code is easier to understand, maintain, and collaborate on with your peers.\n\n\n\n\n\n\nTipReserved keywords\n\n\n\nReserved keywords cannot be used as variable names. You can check the complete list of reserved keywords by running the following command in the Python console:\n\nhelp(\"keywords\")\n\n\nHere is a list of the Python keywords.  Enter any keyword to get more help.\n\nFalse               class               from                or\nNone                continue            global              pass\nTrue                def                 if                  raise\nand                 del                 import              return\nas                  elif                in                  try\nassert              else                is                  while\nasync               except              lambda              with\nawait               finally             nonlocal            yield\nbreak               for                 not                 \n\n\n\nNote that some reserved keywords may be confusing when thinking about finance problems. For example, return, yield, raise, global, class, and lambda are all reserved keywords, so you cannot use them as variable names. Most modern IDEs, such as Visual Studio Code, will highlight reserved keywords in a different color to help you avoid using them as variable names.\n\n\n\n2.3.1 Declaring variables\nA simple way to think about variables is to consider them labels that you can use to refer to values. For example, you can create a variable x and assign it a value of 42 using the assignment operator (=). You can then use the variable x to refer to the value 42 in your code.\n\nx = 42\nx\n\n42\n\n\n\n\n\n\n\n\nNoteThe walrus operator\n\n\n\nIn the previous example, we added x to the last line of the code to display the value of x. This is necessary in the interactive window and in Jupyer Notebooks, as they automatically display the result of the last line of the code. However, the assignment operator (=) does not return a value, so the value of x is not displayed without that last line.\n\nx = 42\n\nIntroduced in Python 3.8, the := operator, also known as the walrus operator, allows you to assign a value to a variable and return that value in a single expression. For example, you can use the walrus operator to assign a value of 10 to a variable z and use that variable in the same expression, assigning the result to y.\n\ny = (z := 10) * 2\n\nNote, however, that the walrus operator cannot be used to assign a value to a variable without using it in an expression. For example, the following code will raise an error.\n\nx := 42\n\n\n  Cell In[19], line 1\n    x := 42\n      ^\nSyntaxError: invalid syntax\n\n\n\n\n\n\nYou can reassign the value of a variable by assigning a new value to it. Once you reassign the value of a variable, the old value is lost. For example, you can reassign the value of x to 32 by running the following code.\n\nx = 32\nx\n\n32\n\n\nYou can perform operations on variables, just like you would on values. For example, you can add 10 to x.\n\nx + 10\n\n42\n\n\nYou can assign the result of an operation to a new variable. For example, you can assign the result of 2 * 10 to a new variable y.\n\ny = 2 * x\ny\n\n64\n\n\n\nz = x + y\nz\n\n96\n\n\nIf you try to use a variable name that is invalid, Python will raise an error. For example, if you try to assign a variable 1ceo, Python will raise an error because variable names cannot start with a number.\n\n1ceo = 2\n\n\n  Cell In[24], line 1\n    1ceo = 2\n    ^\nSyntaxError: invalid decimal literal\n\n\n\n\nYou can, however, use Unicode characters in variable names. For example, you can use accents such as √© in a variable name.\n\ncote_de_cr√©dit = \"AAA\"\n\nA leading underscore in a variable name indicates that the variable is private, which means that it should not be accessed outside of the module or scope in which it is defined. For example, you can use a leading underscore in a variable name to indicate that the variable is private. This is a convention that is widely adopted by the Python community, but it is not enforced by Python.\n\n_hidden = 30_000\n\nAnother convention is to use all caps for constants. For example, you can use all caps to indicate that INFLATION_TARGET is a constant.\n\nINFLATION_TARGET = 0.02\n\nPython will raise an error if you attempt to use a variable that has not been declared. For instance, if you try to use the variable inflation_target instead of INFLATION_TARGET, Python will generate an error. It‚Äôs important to note that Python is case-sensitive, so variables must be referenced with the exact casing as their declaration.\n\ninflation_target\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[28], line 1\n----&gt; 1 inflation_target\n\nNameError: name 'inflation_target' is not defined\n\n\n\n\n\n\n2.3.2 Variable types\nPython is a dynamically typed language, meaning you do not need to specify the variable type when you declare it. Instead, Python will automatically infer the type of a variable based on the value you assign to it. For example, if you assign an integer value to a variable, Python will infer that the variable is an integer. Similarly, if you assign a string value to a variable, Python will infer that the variable is a string. You can use the type() function to check the type of a variable. For example, you can check the type of a by running the following code.\n\na = 3.3\ntype(a)\n\nfloat\n\n\n\nb = 2\ntype(b)\n\nint\n\n\n\nmarket_open = True\ntype(market_open)\n\nbool\n\n\n\ncurrency = \"CAD\"\ntype(currency)\n\nstr\n\n\n\n\n\n\n\n\nTipVariables explorer in Visual Studio Code\n\n\n\nVS Code has a built-in variable explorer that allows you to view the variables in your workspace when using the interactive window or a Jupyer Notebook. You can open the Variables View by clicking on the Variables button in the top toolbar of the editor:\n\n\n\nVariable View button\n\n\nThe Variables View will appear at the bottom of the window, showing the variables in your workspace, along with their values, types, and size for collections. For example, the following screenshot shows the variables in the workspace after running the code in this section:\n\n\n\nVariable View\n\n\n\n\n\n2.3.2.1 Converting between types\nYou can convert a variable from one type to another using the built-in functions float(), int(), str(), and bool(). For example, you can convert the variable x, which is currently an int, to a float by running the following code.\n\nfloat(x)\n\n32.0\n\n\nThe same way, you can convert the variable y, which is currently a float, to an int by running the following code. Note that the int() function will round down the value of y to the nearest integer.\n\nint(a)\n\n3\n\n\nSimilarly, you can convert the variable x to a string by running the following code.\n\nstr(x)\n\n'32'\n\n\nYou can convert a string to an integer or a float if the string contains a valid representation of a number. For example, you can convert the string \"42\" to an integer by running the following code.\n\nint('42')\n\n42\n\n\nHowever, you cannot convert a string that does not contain a valid representation of a number to an integer. For example, you cannot convert the string \"42.5\" to an integer.\n\nint('42.5')\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[37], line 1\n----&gt; 1 int('42.5')\n\nValueError: invalid literal for int() with base 10: '42.5'\n\n\n\n\nWhen converting to a boolean value, most values will be converted to True, except for 0, 0.0, and \"\", which will be converted to False.\n\nbool(0)\n\nFalse\n\n\n\nbool(1)\n\nTrue\n\n\n\nbool(\"\")\n\nFalse\n\n\n\nbool(\"33\")\n\nTrue\n\n\nThe None value is a special type in Python that represents the absence of a value. You can use the None value to initialize a variable without assigning it a value. For example, you can initialize a variable problem to None by running the following code.\n\nproblem = None\ntype(problem)\n\nNoneType",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#comments",
    "href": "python/python-basics/index.html#comments",
    "title": "2¬† Python Syntax",
    "section": "2.4 Comments",
    "text": "2.4 Comments\nComments are an essential part of writing clear, maintainable code. They help explain the purpose, logic, or any specific details of the code that might not be obvious at first glance. However, excessive or unnecessary commenting can clutter your code and make it harder to read. To strike the right balance, consider the guidelines listed in Table¬†2.5 when deciding when to use comments and when to avoid them:\n\n\n\nTable¬†2.5: Guidelines for comments\n\n\n\n\n\n\n\n\n\nGuideline\nDescription\n\n\n\n\nUse comments when the code is complex or non-obvious\nWhen your code involves complex algorithms, calculations, or logic that may be difficult for others (or yourself) to understand at a glance, use comments to explain the reasoning behind the code or to provide additional context.\n\n\nAvoid comments for simple or self-explanatory code\nFor code that is simple, clear, and easy to understand, avoid adding comments. Instead, use descriptive variable and function names that convey the purpose of the code.\n\n\nUse comments to explain the ‚Äòwhy‚Äô, not the ‚Äòhow‚Äô\nGood comments explain the purpose of a piece of code or the reasoning behind a decision. Focus on providing context and insight that isn‚Äôt immediately apparent from reading the code. Avoid repeating what the code is doing, as this can be redundant and clutter the code.\n\n\nAvoid commenting out large blocks of code\nInstead of leaving large blocks of commented-out code in your final version, remove them. It‚Äôs better to use version control systems like Git to keep track of previous versions of your code.\n\n\nKeep comments up-to-date\nEnsure that your comments are always up-to-date with the code they describe. Outdated comments can be confusing and misleading, making it harder to understand the code.\n\n\nUse comments to provide additional information\nUse comments to provide references to external resources, such as links to relevant documentation, papers, or articles. This can be helpful for providing additional context or background information related to the code.\n\n\nUse consistent commenting style\nFollow a consistent commenting style throughout your codebase. This makes it easier for others to read and understand your comments.\n\n\n\n\n\n\n\n2.4.1 Writing comments\nIn Python, comments are created using the # symbol. Any text that follows the # symbol on the same line is ignored by the Python interpreter. Comments can be placed on a separate line or at the end of a line of code.:\n\n# This is a single-line comment\n\nprice = 150  # This is an inline comment\n\nYou can also create multi-line comments by enclosing the text in triple quotes (\"\"\" or '''). Multi-line comments are often used to provide docstrings (documentation strings) for functions and classes. We‚Äôll learn more about functions and classes in a later section. Note that multi-line comments are technically strings, but the Python interpreter ignores them and does not store them in memory because they are not assigned to a variable.\n\n\"\"\"\nThis is a multi-line comment.\nYou can write your comments across multiple lines.\n\"\"\"\n\n'\\nThis is a multi-line comment.\\nYou can write your comments across multiple lines.\\n'\n\n\nComments can occur alongside code to document its purpose or explain the logic.\n\n# Calculate compound interest\nprincipal = 1000  # Principal amount\nrate = 0.05  # Annual interest rate\ntime = 5  # Time in years\n\n# Future value with compound interest formula\nfuture_value = principal * (1 + rate) ** time\n\n# Display the result\nprint(f\"Future value: {future_value:.2f}\")\n\nFuture value: 1276.28\n\n\n\n\n\n\n\n\nTipDon‚Äôt overdo it\n\n\n\nComments are useful for providing additional context or explanation, but they can also be overdone. Avoid adding comments for trivial or self-explanatory code. For example, the code above is simple and clear enough to understand without comments, so adding comments is decreasing readability instead of improving it.\n\n\nComments are usually written in English, but you can use any language as long as the file is UTF-8 encoded. You can also use emojis in comments if you like üòä.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#numbers",
    "href": "python/python-basics/index.html#numbers",
    "title": "2¬† Python Syntax",
    "section": "2.5 Numbers",
    "text": "2.5 Numbers\nPython provides built-in functions and operators to perform mathematical operations on numbers. Some commonly used mathematical functions include abs(), round(), min(), max(), and pow(). Additionally, Python‚Äôs math library offers more advanced functions like trigonometry and logarithms.\n\n\n\n\n\n\nWarningRounding errors\n\n\n\nFloating-point numbers may be subject to rounding errors due to the limitations of their binary representation. Keep this in mind when comparing or performing calculations with floats. Consider using the Decimal data type from Python‚Äôs decimal library to avoid floating-point inaccuracies when dealing with high-precision financial data.\n\n\n\n\n\n\n\n\nNotePerformance\n\n\n\nWhen working with large datasets or performing complex calculations, consider using third-party libraries like NumPy and pandas, which are covered in later chapters, for improved performance and additional functionality.\n\n\n\n2.5.1 Operations\nThe Python language supports many mathematical operations. Table¬†2.6 lists some of the most commonly used operators.\n\n\n\nTable¬†2.6: Basic Arithmetic Operations\n\n\n\n\n\nOperator\nName\nExample\nResult\n\n\n\n\n+\nAddition\n1 + 2\n3\n\n\n-\nSubtraction\n1 - 2\n-1\n\n\n*\nMultiplication\n3 * 4\n12\n\n\n/\nDivision\n1 / 2\n0.5\n\n\n**\nExponentiation\n2 ** 3\n8\n\n\n//\nFloor division\n14 // 3\n4\n\n\n%\nModulo (remainder)\n14 % 3\n2\n\n\n\n\n\n\n\na = 5\nb = 3\n\nprint(f\"Addition: a + b = {a + b}\")\nprint(f\"Subtraction: a - b = {a - b}\")\nprint(f\"Multiplication: a * b = {a * b}\")\nprint(f\"Division: a / b = {a / b}\")\nprint(f\"Exponentiation: a ** b = {a ** b}\")\nprint(f\"Floor Division: a // b = {a // b}\")\nprint(f\"Modulo: a % b = {a % b}\")\n\nAddition: a + b = 8\nSubtraction: a - b = 2\nMultiplication: a * b = 15\nDivision: a / b = 1.6666666666666667\nExponentiation: a ** b = 125\nFloor Division: a // b = 1\nModulo: a % b = 2\n\n\n\n\n\n\n\n\nNotef-strings\n\n\n\nThe previous examples use a special type of strings called f-strings to format the output. f-strings are a convenient way to embed variables and expressions inside strings. They are denoted by the f prefix and curly braces ({}) containing the variable or expression to be evaluated.\nWe cover f-strings in more details in Section 2.7.2.\n\n\n\n\n2.5.2 Common mathematical functions\nTo round numbers, use the round() function. The round() function takes two arguments: the number to be rounded and the number of decimal places to round to. The number is rounded to the nearest integer if the second argument is omitted.\n\nrounded_num = round(5.67, 1)\n\nprint(rounded_num)\nprint(type(rounded_num))\n\n\nrounded_to_int = round(5.67)\n\nprint(rounded_to_int)\nprint(type(rounded_to_int))\n\n5.7\n&lt;class 'float'&gt;\n6\n&lt;class 'int'&gt;\n\n\nSome mathematical functions will require the use of the math module from the Python Standard Library. The standard library is a collection of modules included with every Python installation. You can use the functions and types in these modules by importing them into your code using the import statement.\nFor example, to calculate the square root of a number, you can use the sqrt() function from the math module:\n\n1import math\n\nmath.sqrt(25)\n\n\n1\n\nImports the math module, making its functions available in the current code.\n\n\n\n\n5.0\n\n\nThis is only one of the many functions in the math module. You can view the complete list of functions in the module documentation. The math module also contains constants like pi and e, which you can access using the dot notation.\n\nmath.pi\n\n3.141592653589793\n\n\n\n\n2.5.3 Random numbers\nIt is often useful to generate random numbers for simulations and other applications. Python‚Äôs random module provides functions for generating pseudo-random1 numbers from different distributions.\n\n\n\n\n\n\nImportantPseudo-random number generator\n\n\n\nThe random module uses the Mersenne Twister algorithm to generate pseudo-random numbers. This algorithm is deterministic, meaning that given the same seed value, it will produce the same sequence of numbers every time. This is useful for debugging and testing but not for security purposes. If you need a cryptographically secure random number generator, use the secrets module instead.\n\n\nThe random.seed() function initializes the pseudo-random number generator. If you do not call this function, Python will automatically call it the first time you generate a random number. The random.seed() function takes an optional argument that can be used to set the seed value. This can be useful for debugging and testing, allowing you to generate the same sequence of random numbers every time. If you do not specify a seed, Python will use the system time as the seed value, so you will get a different sequence of random numbers every time.\n\nimport random\n\n1random.seed(42)\n\n\n1\n\nSets the seed value to 42. Why 42? Because it‚Äôs the answer to life, the universe, and everything.\n\n\n\n\nrandom.random() generates a random float between 0 and 1 (exclusive).\n\nrand_num = random.random()\n\nrand_num\n\n0.6394267984578837\n\n\nrandom.randint(a, b) generates a random integer between a and b (inclusive).\n\nrand_int = random.randint(1, 10)\n\nrand_int\n\n1\n\n\nrandom.uniform(a, b) generates a random float between a and b (exclusive).\n\nrand_float = random.uniform(0, 1)\n\nrand_float\n\n0.7415504997598329\n\n\nrandom.normalvariate(mu, sigma) generates a random float from a normal distribution with mean mu and standard deviation sigma.\n\nrand_norm = random.normalvariate(0, 1)\n\nrand_norm\n\n-0.508616386057752\n\n\nThe full list of functions in the random module can be found in the module documentation.\n\n\n2.5.4 Floats and decimals\nBecause of the way computers store numbers, floating-point numbers are not exact. This can lead to unexpected results when performing arithmetic operations on floats.\n\n2.33 + 4.44\n\n6.7700000000000005\n\n\nTo avoid this problem when exact results are needed, use the Decimal type from the decimal module to perform arithmetic operations on decimal numbers. You could import the module using import decimal but this would require you to prefix all the functions and types in the module with decimal. To avoid this, you can directly import the Decimal type from the decimal module using from decimal import Decimal.\n\n1from decimal import Decimal\n\nDecimal(\"2.33\") + Decimal(\"4.44\")\n\n\n1\n\nImports the Decimal type from the decimal module. You can now refer to the Decimal type directly without having to prefix it with decimal.\n\n\n\n\nDecimal('6.77')\n\n\n\n\n\n\n\n\nNoteDecimals vs.¬†floats\n\n\n\nUsing the Decimal type in Python provides precise decimal arithmetic and avoids rounding errors, making it suitable for financial and monetary calculations, while floats offer faster computation and are more memory-efficient but can introduce small inaccuracies due to limited precision and binary representation.\n\n\n\n\n2.5.5 Financial formulas\nMany financial calculations involve performing arithmetic operations on financial data. Here are two examples of common calculations in finance and how they can be implemented in Python.\n\n2.5.5.1 Calculating the present value of a future cash flow\nThe formula for calculating the present value of a future cash flow is: \\[ PV = \\frac{FV_t}{(1 + r)^t}, \\] where \\(FV_t\\) is the future value of the cash flow at time \\(t\\), \\(r\\) is the discount rate, and \\(t\\) is the number of periods.\n\nfuture_value = 1000\ndiscount_rate = 0.05\nperiods = 5\n\npresent_value = future_value / (1 + discount_rate) ** periods\n\npresent_value\n\n783.5261664684588\n\n\n\n\n2.5.5.2 Calculating the future value of an annuity\nThe formula for calculating the future value of an annuity is: \\[ FV = PMT \\frac{(1 + r)^t - 1}{r}, \\] where \\(PMT\\) is the payment, \\(r\\) is the interest rate, and \\(t\\) is the number of periods.\nIt can be written in Python as:\n\npayment = 100\nrate = 0.05\nperiods = 5\n\nfuture_value_annuity = payment * ((1 + rate) ** periods - 1) / rate\n\nfuture_value_annuity\n\n552.5631250000007\n\n\n\n\n\n\n\n\nTipParentheses and operator precedence\n\n\n\nPython, just like mathematics, follows a specific order of operations when evaluating expressions. The complete list of precedence rules can be found in the Python documentation.\nWhen in doubt, use parentheses to make the order of operations explicit.\nFor arithmetic operations, the order of operations is as follows:\n\nExponents\nNegative (-)\nMultiplication and division\nAddition and subtraction",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#sec-defining-functions",
    "href": "python/python-basics/index.html#sec-defining-functions",
    "title": "2¬† Python Syntax",
    "section": "2.6 Defining functions",
    "text": "2.6 Defining functions\nFunctions are blocks of organized and reusable code that perform a specific action. They allow you to encapsulate a set of instructions, making your code modular and easier to maintain. Functions can take input parameters, perform operations on those inputs, and return a result.\nDefining a function in Python involves the following steps:\n\nUse the def keyword: Start by using the def keyword, followed by the function name and parentheses that enclose any input parameters.\nAdd input parameters: Specify any input parameters within the parentheses, separated by commas. These parameters allow you to pass values to the function, which it can then use in its calculations or operations.\nWrite the function body: After the parentheses, add a colon (:) and indent the following lines to create the function body. This block of code contains the instructions that the function will execute when called.\nReturn a result (optional): If your function produces a result, use the return statement to send the result back to the caller. If no return statement is specified, the function will return None by default.\n\n\n\n\n\n\n\nTipBest practices\n\n\n\nWhen defining functions, keep the following best practices in mind:\n\nChoose descriptive function names: Use meaningful names that reflect the purpose of the function, making your code more readable and easier to understand.\nKeep functions small and focused: Each function should have a single responsibility, making it easier to test, debug, and maintain.\n\n\n\nWe can define functions to perform a wide variety of tasks. For example, we can define a function to calculate the present value of a future cash flow:\n\n1def present_value(future_value, discount_rate, periods):\n2    return future_value / (1 + discount_rate) ** periods\n\n\n# Example usage:\nfuture_value = 1000\ndiscount_rate = 0.05\nperiods = 5\n3result = present_value(future_value, discount_rate, periods)\nprint(f\"Present value: {result:.2f}\")\n\n\n1\n\nDefines a function called present_value that takes three input parameters: future_value, discount_rate, and periods.\n\n2\n\nCalculates the present value of a future cash flow using the formula from the previous section and returns the result to the caller. The code in the function body is indented to indicate that it is part of the function.\n\n3\n\nCalls the present_value function with the specified input values and stores the returned value in a variable called result. When the function is called, the input values are passed to the function as arguments in the same order as the parameters were defined. The function body is then executed, and the result is returned to the caller.\n\n\n\n\nPresent value: 783.53\n\n\n\n\n\n\n\n\nNoteIndentation\n\n\n\nIndentation refers to the spaces or tabs used at the beginning of a line to organize code. It helps Python understand the program‚Äôs structure and which lines of code are grouped together.\nThe Python language specification mandates the use of consistent indentation for code readability and proper execution. Indentation is typically achieved using four spaces per level. It plays a crucial role in determining the scope and hierarchy of statements within control structures, such as loops and conditional statements. For example, the statements that are part of a function body must be indented to indicate that they are part of the function. The Python interpreter knows that the function body ends when the indentation level returns to the previous level.\n\n\nWe will learn more about functions in Section 2.11.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#sec-strings",
    "href": "python/python-basics/index.html#sec-strings",
    "title": "2¬† Python Syntax",
    "section": "2.7 Strings",
    "text": "2.7 Strings\nText data is often encountered in finance in the form of stock symbols, company names, descriptions, or financial reports. Understanding how to work with strings is essential for processing and manipulating text data effectively.\nStrings are sequences of characters, and they can be created using single quotes (' '), double quotes (\" \"), or triple quotes (''' ''' or \"\"\" \"\"\") for multi-line strings.\n\n\n\n\n\n\nNoteSpecial characters\n\n\n\nSome characters have special meanings in Python strings. The backslash (\\) is used to escape characters that have special meaning, such as newline (\\n) or tab (\\t). To include a backslash in a string, you need to escape it by adding another backslash before it (\\\\). Alternatively, you can use raw strings by prefixing the strings with r or R, which will treat backslashes as literal characters. For example, these two strings are equivalent:\nstr1 = \"C:\\\\Users\\\\John\"\nstr2 = r\"C:\\Users\\John\"\n\n\n\n2.7.1 String operations\nThe Python language provides many common string operations. Table¬†2.7 lists some of the most commonly used operations.\n\n\n\nTable¬†2.7: Common string operations\n\n\n\n\n\n\n\n\n\n\nOperation\nExample\nDescription\n\n\n\n\nConcatenate strings\nresult = str1 + \" \" + str2\nCombines two or more strings together\n\n\nRepeat strings\nresult = repeat_str * 3\nRepeats a string a specified number of times\n\n\nLength of a string\nlength = len(text)\nGets the length (number of characters) of a string\n\n\nAccess characters in a string\nfirst_char = text[0]\nRetrieves a specific character in a string\n\n\nSlice a string\nslice_text = text[0:12]\nExtracts a part of a string\n\n\nConvert case\nupper_text = text.upper()\nConverts a string to uppercase\n\n\n\nlower_text = text.lower()\nConverts a string to lowercase\n\n\nJoin a list of strings\ntext = \", \".join(companies)\nJoins a list of strings using a delimiter\n\n\nSplit a string\ncompanies = text.split(\", \")\nSplits a string into a list based on a delimiter\n\n\nReplace a substring\nnew_text = text.replace(\"Finance\", \"Python\")\nReplaces a specified substring in a string\n\n\nCheck substring existence\nresult = substring in text\nChecks if a substring exists in a string\n\n\n\n\n\n\nWe can concatenate (combine) two or more strings into a single string using the + operator.\n\nstr1 = \"Hello\"\nstr2 = \"World\"\nresult = str1 + \" \" + str2\nprint(result)\n\nHello World\n\n\nThe * operator repeats a string multiple times.\n\nrepeat_str = \"Python \"\nresult = repeat_str * 3\nprint(result)\n\nPython Python Python \n\n\nThe len() function returns the string‚Äôs length (number of characters).\n\ntext = \"Finance\"\nlength = len(text)\nprint(length)\n\n7\n\n\nSingle characters in a string can be accessed using the index of the character within square brackets ([]). Python uses zero-based indexing, so the first character in a string has index 0, the second character has index 1, and so on. You can also use negative indices to access characters from the end of a string, with the last character having index -1, the second last character having index -2, and so on.\n\ntext = \"Python\"\nfirst_char = text[0]\nlast_char = text[-1]\nprint(first_char)\nprint(last_char)\n\nP\nn\n\n\nExtracting a portion of a string by specifying a start and end index is called slicing. In Python, you can slice a string using the following syntax: text[start:end]. The start index is inclusive, while the end index is exclusive. If the start index is omitted, it defaults to 0. If the end index is omitted, it defaults to the length of the string.\n\ntext = \"empirical finance Python\"\nslice_text = text[0:7]\nprint(slice_text)\n\nempiric\n\n\nThe upper() and lower() methods convert a string to uppercase or lowercase, respectively.\n\ntext = \"Finance\"\nupper_text = text.upper()\nlower_text = text.lower()\nprint(upper_text)\nprint(lower_text)\n\nFINANCE\nfinance\n\n\n\n\n\n\n\n\nNoteMethods vs functions\n\n\n\nA method is similar to a function but associated with a specific object or data type. In this case, upper() and lower() are methods specific to the str (string) data type. When we call the upper method on the text object using the dot notation (text.upper()), Python knows to transform the string stored in the text variable. Methods are particularly useful because they allow us to perform actions or operations specific to the object or data type they belong to, and they improve code readability by making it clear what object the method is being called on.\n\n\nThe join() method joins a list of strings into a single string using a delimiter. The delimiter can be specified as an argument to the join() method. Lists are introduced in the next section.\n\ncompanies = [\"Apple\", \"Microsoft\", \"Google\"]\ntext = \" | \".join(companies)\nprint(text)\n\nApple | Microsoft | Google\n\n\nThe split() method splits a string into a list of substrings based on a delimiter. The delimiter can be specified as an argument to the split() method. If no delimiter is specified, the string is split on whitespace characters.\n\ntext = \"Apple, Microsoft, Google\"\ncompanies = text.split(\", \")\nprint(companies)\n\n['Apple', 'Microsoft', 'Google']\n\n\nThe replace() method replaces a substring in a string with another string. It takes two arguments: the substring to replace and the string to replace it with.\n\ntext = \"Introduction to Finance\"\nnew_text = text.replace(\"Finance\", \"Python\")\nprint(new_text)\n\nIntroduction to Python\n\n\nThe in operator checks if a substring exists in a string. It returns a boolean value, True if the substring exists in the string, and False otherwise.\n\ntext = \"Introduction to Python\"\nsubstring = \"Python\"\nresult = substring in text\nprint(result)\n\nTrue\n\n\nThe Python documentation provides a complete list of string methods that you can refer to for more details.\n\n\n2.7.2 Formatting strings\nYou will often encounter situations where you must present or display data in a formatted, human-readable manner. F-strings are a powerful tool for formatting strings and embedding expressions or variables directly within the string. They provide a concise and easy-to-read way of formatting strings, making them an essential tool for working with text data.\nF-strings, also known as ‚Äúformatted string literals,‚Äù allow you to embed expressions, variables, or even basic arithmetic directly into a string by enclosing them in curly braces {} within the string. The expressions inside the curly braces are evaluated at runtime and then formatted according to the specified format options.\nSome key features of f-strings that are useful include:\n\nExpression Evaluation: You can embed any valid Python expression within the curly braces, including variables, arithmetic operations, or function calls. This feature enables you to generate formatted strings based on your data dynamically.\nFormatting Options: F-strings support various formatting options, such as alignment, width, precision, and thousand separators. These options can be specified within the curly braces after the expression, separated by a colon (:).\nFormat Specifiers: You can use format specifiers to control the display of numbers, such as specifying the number of decimal places, using scientific notation, or adding a percentage sign. Format specifiers are especially useful in finance when working with currency, percentages, or large numbers.\n\nTo create an f-string, prefix the string with an f character, followed by single or double quotes. You can then embed expressions or variables within the string by enclosing them in curly braces ({}). For example, this lets you concatenate strings and variables together in a single statement:\n\nticker = \"AAPL\"\nexchange = \"NASDAQ\"\ncompany_name = \"Apple, Inc.\"\nfull_name = f\"{company_name} ({exchange}:{ticker})\"\nprint(full_name)\n\nApple, Inc. (NASDAQ:AAPL)\n\n\nPython will convert the expression within the curly braces to a string, which can be used to convert numbers to strings.\n\nnum = 42\nnum_str = f\"{num}\"\nprint(num_str)\n\n42\n\n\nPython evaluates the expression within the curly braces at runtime and then formats the string according to the specified format options. For example, you can use the :,.2f format option to display a number with a thousand separator and two decimal places.\n\namount = 12345.6789\nformatted_amount = f\"${amount:,.2f}\"\nprint(formatted_amount) \n\n$12,345.68\n\n\nYou can also use the :.2% format option to display a number as a percentage with two decimal places.\n\nrate = 0.05\nformatted_rate = f\"{rate:.2%}\"\nprint(formatted_rate) \n\n5.00%\n\n\nThe datetime module provides a datetime class to represent dates and times. The datetime class has a now() method that returns the current date and time. You can use the :%Y-%m-%d format option to display the date in YYYY-MM-DD format.\n\nfrom datetime import datetime\n\ncurrent_date = datetime.now()\nformatted_date = f\"{current_date:%Y-%m-%d}\"\nprint(formatted_date)\n\n2025-12-31\n\n\nYou can also use f-strings to align text to the left (&lt;), right (&gt;), or center (^) within a fixed-width column:\n\nticker = \"AAPL\"\nprice = 150.25\nchange = -1.25\n\n1formatted_string = f\"|{ticker:&lt;10}|{price:^10.2f}|{change:&gt;10.2f}|\"\nprint(formatted_string)\n\n\n1\n\nThe :&lt;10 format option aligns the text to the left within a 10-character column. The :^10.2f format option aligns the number to the center within a 10-character column and displays it with two decimal places. The :&gt;10.2f format option aligns the number to the right within a 10-character column and displays it with two decimal places.\n\n\n\n\n|AAPL      |  150.25  |     -1.25|\n\n\nMultiline f-strings work the same way as multiline strings, except that they are prefixed with an f character. You can use multiline f-strings to create formatted strings that span multiple lines.\n\nstock = \"AAPL\"\nprice = 150.25\nchange = -1.25\n\nformatted_string = f\"\"\"\nStock:  \\t{stock}\nPrice:  \\t${price:.2f}\nChange: \\t${change:.2f}\n\"\"\"\nprint(formatted_string)\n\n\nStock:      AAPL\nPrice:      $150.25\nChange:     $-1.25\n\n\n\n\n\n\n\n\n\nNoteAlternative formatting methods\n\n\n\nWhen reading code or answers on websites such as Stack Overflow or receiving suggestions from AI-assisted coding assistant, you may encounter other string formatting methods. Before f-strings, the two primary string formatting methods in Python were %-formatting and str.format().\n%-formatting\nAlso known as printf-style formatting, %-formatting uses the % operator to replace placeholders with values. Inspired by the printf function in C, it has been available since early versions of Python. It is less readable and more error-prone than other methods.\nExample:\nformatted_string = \"%s has a balance of $%.2f\" % (name, balance)\nstr.format()\nThe str.format() method embeds placeholders using curly braces {} and replaces them with the format() method. Introduced in Python 2.6, it offers improved readability and more advanced formatting options compared to %-formatting.\nExample:\nformatted_string = \"{} has a balance of ${:,.2f}\".format(name, balance)\nAdvantages of f-strings\nI recommend using f-strings instead of %-formatting or str.format() for string formatting for the following reasons:\n\nReadability: Concise syntax with expressions and variables embedded directly.\nFlexibility: Supports any valid Python expression within curly braces.\nPerformance: Faster than other methods, evaluated at runtime.\nSimplicity: No need to specify variable order or maintain separate lists.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#sec-collections",
    "href": "python/python-basics/index.html#sec-collections",
    "title": "2¬† Python Syntax",
    "section": "2.8 Collections",
    "text": "2.8 Collections\nSequences and collections are fundamental data structures in Python that allow you to store and manipulate multiple elements in an organized manner. They differ along three dimensions: order, mutability, and indexability. An ordered collection is one where the elements are stored in a particular order, the order of the elements is important, and you can iterate over the elements in that order. A collection is mutable if you can add, remove, or modify elements, after it is created. A collection is indexable if you can refer to its elements by their index (position or key).\nTable¬†2.8 presents the main types of sequences and collections in Python. You are already familiar with the string type, an ordered, immutable, and indexable sequence of characters.\n\n\n\nTable¬†2.8: Sequences and collections in Python\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nExample\n\n\n\n\nList\nlist\nOrdered, mutable, and indexed. Allows duplicate members.\n[1, 2, 3]\n\n\nTuple\ntuple\nOrdered, immutable, and indexed. Allows duplicate members.\n(1, 2, 3)\n\n\nSet\nset\nUnordered, mutable, and unindexed. No duplicate members.\n{1, 2, 3}\n\n\nDictionary\ndict\nUnordered, mutable, and indexed. No duplicate index entries. Elements are indexed according to a key.\n{\"a\": 1, \"b\": 4}\n\n\nString\nstring\nOrdered, immutable, and indexed. Allows duplicate characters.\n\"abc\"\n\n\n\n\n\n\n\n2.8.1 Lists\nLists in Python are ordered collections of items that can hold different data types. They are mutable, meaning that elements can be added, removed, or modified. Lists are versatile and commonly used to store and manipulate sets of related data. The elements within a list are accessed using indexes, which allow for easy retrieval and modification. Lists also support various built-in methods and operations for efficient data manipulation, such as appending, extending, sorting, and slicing.\nA list is created by enclosing a comma-separated sequence of elements within square brackets ([ ]). The elements can be of any data type, including other lists. The following code snippet creates a list of strings and a list of integers.\n\nstocks = [\"AAPL\", \"GOOG\", \"MSFT\"]\nprices = [150.25, 1200.50, 250.00]\n\nYou can access the elements of a list using their index. The index of the first element is 0, the index of the second element is 1, and so on. You can also use negative indexes to access elements from the end of the list. The index of the last element is -1, the index of the second to last element is -2, and so on. The following code snippet illustrates how to access the elements of the stocks and prices lists.\n\nfirst_stock = stocks[0]\nprint(first_stock)\n\nlast_price = prices[-1]\nprint(last_price)\n\nAAPL\n250.0\n\n\nYou can replace the elements of a list by assigning new values to their indexes, add new elements to the list using the append() method, or delete elements from the list using the remove() method.\n\n# Replace an element\nstocks[1] = \"GOOGL\"\nprint(stocks)\n\n# Adding an element to the list\nstocks.append(\"AMZN\")\nprint(stocks)\n\n# Removing an element from the list\nstocks.remove(\"MSFT\")\nprint(stocks)\n\n['AAPL', 'GOOGL', 'MSFT']\n['AAPL', 'GOOGL', 'MSFT', 'AMZN']\n['AAPL', 'GOOGL', 'AMZN']\n\n\nYou can also use the len() function to get the length of a list, and the in operator to check if an element is present in a list.\n\n# Length of the list\nlist_length = len(stocks)\nprint(f\"Length: {list_length}\")\n\n# Checking if an element is in the list\nis_present = \"AAPL\" in stocks\nprint(f\"Is AAPL in the list? {is_present}\")\n\nLength: 3\nIs AAPL in the list? True\n\n\n\n\n2.8.2 Tuples\nTuples are ordered collections of elements that are immutable, meaning they cannot be modified after creation. They are typically used to store related pieces of data as a single entity, and their immutability provides benefits such as ensuring data integrity and enabling safe data sharing across different parts of a program.\n\n\n\n\n\n\nNoteTuples vs lists\n\n\n\nTuples and lists in Python differ in mutability, syntax, and use cases. Tuples are commonly used for fixed data, have a slight performance advantage over lists and can be more memory-efficient. Lists are commonly used for variable data, and provide more flexibility in terms of operations and methods.\n\n\nA tuple is created by enclosing a comma-separated sequence of elements within parentheses (( )). The elements can be of any data type, including other tuples. The following code snippet creates a tuple of integers and a tuple of strings.\n\nmu = 0.1\nsigma = 0.2\ntheta = 0.5\n\nparameters = (mu, sigma, theta)\nprint(parameters)\n\n(0.1, 0.2, 0.5)\n\n\nYou can access the elements of a tuple using their index, find their length using len(), just like with lists.\n\n# Accessing elements in a tuple\nsigma0 = parameters[1]\nprint(sigma0)\n\n# Length of the tuple\ntuple_length = len(parameters)\nprint(f\"Length: {tuple_length}\")\n\n0.2\nLength: 3\n\n\nTuples are immutable, so you cannot add, remove, or replace their elements directly. You can, however, create a new tuple with the modified elements. Also note that you can modify mutable elements within a tuple, such as a list.\n\na = [1, 2, 3]\nb = (\"c\", a, 2)\nprint(f\"Before appending to list a: {b}\")\n\na.append(4)\nprint(f\"After appending to list a: {b}\")\n\nBefore appending to list a: ('c', [1, 2, 3], 2)\nAfter appending to list a: ('c', [1, 2, 3, 4], 2)\n\n\nb still contains the same elements, but the list a within the tuple has been modified.\n\n2.8.2.1 Tuple unpacking\nTuple unpacking is a powerful feature of Python that allows you to assign multiple variables from the elements of a tuple in a single line of code. It is a form of ‚Äúdestructuring assignment‚Äù that provides a concise way to extract the elements of a tuple into individual variables.\nTo perform tuple unpacking, you use a sequence of variables on the left side of an assignment statement, followed by a tuple on the right side. When the assignment is made, each variable on the left side will be assigned the corresponding value from the tuple on the right side.\nHere is an example:\n\n# Create a tuple\nt = (1, 2, 3)\n\n# Unpack the tuple into three variables\na, b, c = t\n\n# Display the values of the variables\nprint(f\"a: {a}, b: {b}, c: {c}\")\n\na: 1, b: 2, c: 3\n\n\nIn this example, the tuple t contains three elements: 1, 2, and 3. When the tuple is unpacked into the variables a, b, and c, each variable gets assigned the corresponding value from the tuple: a gets 1, b gets 2, and c gets 3.\nTuple unpacking can be useful in various situations. For example, when working with functions that return multiple values as a tuple, you can use tuple unpacking to assign the return values to individual variables. Here‚Äôs an example:\n\n# Define a function that returns a tuple\ndef get_top3_stocks():\n    return (\"AAPL\", \"MSFT\", \"AMZN\")\n\n# Unpack the returned tuple into three variables\nstock1, stock2, stock3 = get_top3_stocks()\n\n# Display the values of the variables\nprint(f\"Largest: {stock1}, 1nd: {stock2}, 3rd: {stock3}\")\n\nLargest: AAPL, 1nd: MSFT, 3rd: AMZN\n\n\nNote that the number of variables on the left side of the assignment must match the number of elements in the tuple being unpacked.\n\n\n\n2.8.3 Sets\nSets are unordered collections of unique elements. They are defined using curly braces { } or the set() constructor. Sets do not allow duplicate values and support various operations such as intersection, union, and difference. Sets are commonly used for tasks like removing duplicates from a list, membership testing, and mathematical operations on distinct elements.\n\n# Creating a set\nunique_numbers = {1, 2, 3, 2, 1}\nprint(unique_numbers)\n\n# Adding an element to the set\nunique_numbers.add(4)\nprint(f\"Added 4: {unique_numbers}\")\n\n# Removing an element from the set\nunique_numbers.remove(1)\nprint(f\"Removed 1: {unique_numbers}\")\n\n# Checking if an element is in the set\nis_present = 2 in unique_numbers\nprint(f\"Is 2 in the set? {is_present}\")\n\n# Length of the set\nset_length = len(unique_numbers)\nprint(f\"Length: {set_length}\")\n\n{1, 2, 3}\nAdded 4: {1, 2, 3, 4}\nRemoved 1: {2, 3, 4}\nIs 2 in the set? True\nLength: 3\n\n\nSets support operations such as intersection, union, and difference, which are performed using the &, |, and - operators respectively.\n\nset1 = {1, 2, 3, 4}\nset2 = set([3, 4, 5, 6])\n\n# Intersection\nprint(f\"Intersection: {set1 & set2}\")\n\n# Union\nprint(f\"Union: {set1 | set2}\")\n\n# Difference\nprint(f\"Difference: {set1 - set2}\")\n\nIntersection: {3, 4}\nUnion: {1, 2, 3, 4, 5, 6}\nDifference: {1, 2}\n\n\n\n\n\n\n\n\nNoteSets and data types\n\n\n\nSets can contain elements of different data types, including numbers, strings, and tuples. However, sets only support immutable elements, so you cannot add lists or dictionaries to a set.\n\n\n\n\n2.8.4 Dictionaries\nDictionaries are key-value pairs that provide a way to store and retrieve data using unique keys. They are defined with curly braces { } like sets, but contain pairs of elements called items, where each item is a key-value pair separated by a colon (:).\nDictionaries are unordered and mutable, allowing for efficient data lookup and modification. They are commonly used for mapping and associating values with specific keys, making them useful for tasks like storing settings, organizing data, or building lookup tables.\n\nstock_prices = {\"AAPL\": 150.25, \"GOOGL\": 1200.50, \"MSFT\": 250.00}\nprint(stock_prices)\n\n{'AAPL': 150.25, 'GOOGL': 1200.5, 'MSFT': 250.0}\n\n\nYou access the value for a specific key using square brackets [ ], and modify the value for a key using the assignment operator =. You add new key-value pairs to a dictionary using a new key and assignment operator and remove a key-value pair using the del keyword. The len() function returns the number of key-value pairs in a dictionary.\n\n# Accessing elements in a dictionary\nprice_aapl = stock_prices[\"AAPL\"]\nprint(f\"Price for AAPL: {price_aapl:0.2f}\")\n\n# Modifying an element\nstock_prices[\"GOOGL\"] = 1205.00\nprint(f\"Modified GOOGL: {stock_prices}\")\n\n# Adding a new element to the dictionary\nstock_prices[\"AMZN\"] = 3300.00\nprint(f\"Added AMZN: {stock_prices}\")\n\n# Removing an element from the dictionary\ndel stock_prices[\"MSFT\"]\nprint(f\"Removed MSFT: {stock_prices}\")\n\n# Length of the dictionary\ndict_length = len(stock_prices)\nprint(f\"Length: {dict_length}\")\n\nPrice for AAPL: 150.25\nModified GOOGL: {'AAPL': 150.25, 'GOOGL': 1205.0, 'MSFT': 250.0}\nAdded AMZN: {'AAPL': 150.25, 'GOOGL': 1205.0, 'MSFT': 250.0, 'AMZN': 3300.0}\nRemoved MSFT: {'AAPL': 150.25, 'GOOGL': 1205.0, 'AMZN': 3300.0}\nLength: 3\n\n\nThe collection module from Python‚Äôs standard library provides many other data structures such as defaultdict, OrderedDict, Counter, and deque. You can learn more about these data structures in the Python documentation.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#comparison-operators-and-branching",
    "href": "python/python-basics/index.html#comparison-operators-and-branching",
    "title": "2¬† Python Syntax",
    "section": "2.9 Comparison operators and branching",
    "text": "2.9 Comparison operators and branching\n\n2.9.1 Comparison operators\nPython provides several comparison operators that allow you to compare values and evaluate expressions. Comparison operators can be used with various data types, such as numbers, strings, or even complex data structures, and return a boolean value (True or False). Table¬†2.9 lists the comparison operators available in Python.\n\n\n\nTable¬†2.9: Comparison operators in Python\n\n\n\n\n\nOperator\nName\nExample\nResult\n\n\n\n\n==\nEqual\n1 == 2\nFalse\n\n\n!=\nNot equal\n1 != 2\nTrue\n\n\n&gt;\nGreater than\n1 &gt; 2\nFalse\n\n\n&lt;\nLess than\n1 &lt; 2\nTrue\n\n\n&gt;=\nGreater or equal\n1 &gt;= 2\nFalse\n\n\n&lt;=\nLess or equal\n1 &lt;= 2\nTrue\n\n\nin\nMembership\n1 in [1, 2, 3]\nTrue\n\n\nis\nIdentity comparison\n1 is None\nFalse\n\n\n\n\n\n\nTo create more complex conditions, you can chain multiple comparisons in a single expression using logical operators like and, or, or not. The result of a logical operator is a boolean value (True or False). Table¬†2.10 lists the logical operators available in Python.\n\n\n\nTable¬†2.10: Logical operators in Python\n\n\n\n\n\na\nb\na and b\na or b\nnot a\n\n\n\n\nTrue\nTrue\nTrue\nTrue\nFalse\n\n\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\nFalse\nTrue\nFalse\nTrue\nTrue\n\n\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\nWarning& and | are bitwise operators, not logical operators\n\n\n\nPython also provides bitwise operators that perform bitwise operations on integers. These operators are & (bitwise AND), | (bitwise OR), ^ (bitwise XOR), ~ (bitwise NOT), &lt;&lt; (bitwise left shift), and &gt;&gt; (bitwise right shift). Most casual Python users will not need to use these operators, but they can be confusing for new users due to their similar syntax to logical operators in other programming languages. To add to the confusion, popular Python libraries like NumPy and Pandas overload the bitwise operators to perform logical operations on arrays.\nIt is crucial for beginners to understand the distinction between logical and bitwise operators and to use the appropriate operators (which are usually and, or, or not) based on their intended purpose to ensure the desired logical evaluations are achieved.\n\n\nLonger expressions can be grouped using parentheses to ensure the desired order of operations. For example, a and b or c is equivalent to (a and b) or c, whereas a and (b or c) is different. Python does not offer a built-in exclusive or (XOR) operator, but it can be achieved using a combination of other operators.\n\ndef xor(a, b):\n    return (a and not b) or (not a and b)\n\n\nprint(f\"xor(True, True)) = {xor(True, True)}\")\nprint(f\"xor(True, False)) = {xor(True, False)}\")\nprint(f\"xor(False, True)) = {xor(False, True)}\")\nprint(f\"xor(False, False)) = {xor(False, False)}\")\n\nxor(True, True)) = False\nxor(True, False)) = True\nxor(False, True)) = True\nxor(False, False)) = False\n\n\n\n\n2.9.2 Branching\nBranching allows your code to execute different actions based on specific conditions. The primary branching construct in Python is the if statement, which can be combined with elif (short for ‚Äúelse if‚Äù) and else clauses to create more complex decision-making structures.\nLike other compound statements in Python, the if statement uses indentation to group statements together. The general syntax for an if statement is to start with the if keyword followed by a condition, then a colon (:), and, finally, an indented block of code that will be executed if the condition evaluates to True. The elif and else clauses are optional and can be used to specify additional conditions and code blocks to execute if the initial condition evaluates to False. The elif clause is used to chain multiple conditions together, whereas the else clause is used to specify a default code block to execute if none of the previous conditions evaluate to True.\n\nprice = 150\n\nif price &gt; 100:\n    print(\"The stock price is high.\")\n\nThe stock price is high.\n\n\nIn the previous example, the code block is executed because the price = 150, therefore the condition price &gt; 100 evaluates to True.\nWe can add an else clause to specify a default code block to execute if the condition evaluates to False.\n\nprice = 50\n\nif price &gt; 100:\n    print(\"The stock price is high.\")\nelse:\n    print(\"The stock price is low.\")\n\nThe stock price is low.\n\n\nWe can add an elif clause to specify additional conditions to evaluate if the initial condition evaluates to False. The elif clause can be used multiple times to chain multiple conditions together. The elif clause is optional, but if it is used, it must come before the else clause. The else clause is also optional, but if it is used, it must come last.\nIn all cases, the code block associated with the first condition that evaluates to True will be executed, and the remaining conditions will be skipped. If none of the conditions evaluate to True, then the code block associated with the else clause will be executed. If there is no else clause, then nothing will be executed.\n\nprice = 75\n\nif price &gt; 100:\n    print(\"The stock price is high.\")\nelif price &gt; 50:\n    print(\"The stock price is moderate.\")\nelse:\n    print(\"The stock price is low.\")\n\nThe stock price is moderate.\n\n\nYou can nest if statements inside other if statements to create more complex branching structures. The code block associated with the nested if statement must be indented further than the outer if statement. The nested if statement will only be evaluated if the condition associated with the outer if statement evaluates to True. When reading nested if statements, it is helpful to read from the top down and to keep track of the indentation level to understand which code blocks are associated with which conditions.\n\nprice = 150\nvolume = 1000000\n\nif price &gt; 100:\n    if volume &gt; 500000:\n        print(\"The stock price is high and has high volume.\")\n    else:\n        print(\"The stock price is high but has low volume.\")\nelse:\n    print(\"The stock price is not high.\")\n\nThe stock price is high and has high volume.\n\n\nConditions can be combined using the logical operators and, or, and not to create more complex conditions.\n\nprice = 150\nvolume = 1000000\n\nif price &gt; 100 and volume &gt; 500000:\n    print(\"The stock price is high and has high volume.\")\nelif price &gt; 100 or volume &gt; 500000:\n    print(\"The stock price is high or has high volume.\")\nelse:\n    print(\"The stock price is not high and has low volume.\")\n\nThe stock price is high and has high volume.\n\n\n\n\n2.9.3 Conditional assignment\nPython provides a convenient shorthand for assigning a value to a variable based on a condition. This is known as conditional assignment. The syntax for conditional assignment is variable = value1 if condition else value2. If the condition evaluates to True, the variable is assigned value1; otherwise, it is assigned value2.\n\nprice = 150\n\nmessage = \"The stock price is high.\" if price &gt; 100 else \"The stock price is low.\"\nprint(message)\n\nThe stock price is high.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#typing",
    "href": "python/python-basics/index.html#typing",
    "title": "2¬† Python Syntax",
    "section": "2.10 Typing",
    "text": "2.10 Typing\nPython is a dynamically typed language. This means that you don‚Äôt have to specify the type of a variable when you define it. The Python interpreter will automatically infer the type based on the value assigned to the variable.\nPython also supports optional type annotations, also called type hints, since version 3.5. This allows you to specify the types of variables, function arguments, and return values to improve code readability and catch potential errors early. The Python interpreter will ignore the type annotations and run the code normally. However, you can use external tools like mypy to analyze the code and check for type errors, and modern IDEs like VS Code provide built-in support for type checking.\n\nticker: str = \"AAPL\"\n\n\nstock_prices: list[float] = [150.25, 1200.50, 250.00]\n\n\n# Old version, not needed since Python 3.9\n\nfrom typing import List\n\nstock_prices: List[float] = [150.25, 1200.50, 250.00]\n\n\n# You can also specify function parameters and return types:\n\n\ndef calculate_profit(revenue: float, expenses: float) -&gt; float:\n    return revenue - expenses\n\n\nrevenue = 1000.00\nexpenses = 500.00\nprofit = calculate_profit(revenue, expenses)\n\n\n\n\n\n\n\nTipType hints in Visual Studio Code\n\n\n\nType hints are not required to run Python code, but they can be very useful to improve code readability and catch potential errors early. Modern IDEs like VS Code provide built-in support for type checking that you can enable.\nI find this quite overwhelming, so I prefer to enable type-checking only when needed. However, VS Code still uses type hints to provide useful features like hover info.\n\n\n\nTooltip when hovering over variable.\n\n\n\n\n\nTooltip when hovering over function.\n\n\n\n\nThe typing module provides a set of special types that can be used in type hints. Here are some of the most commonly used ones:\n\nAny: Any type\nOptional: An optional value (can be None)\nCallable: A function\nIterable: An iterable object (e.g., list, tuple, set)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#sec-functions-parameters-return-values",
    "href": "python/python-basics/index.html#sec-functions-parameters-return-values",
    "title": "2¬† Python Syntax",
    "section": "2.11 Functions: parameters and return values",
    "text": "2.11 Functions: parameters and return values\nFunctions help you organize and structure your code by encapsulating specific tasks or calculations. They allow you to define input parameters, perform operations, and return the results, making your code more flexible and maintainable. We have already written simple functions in Section 2.6; we will now look in more detail at how to define parameters and return values.\n\n\n\n\n\n\nNoteType hints in examples\n\n\n\nIn the examples below, I use type hints to indicate the type of the function parameters and return values. Type hints are not required to run Python code, but using them is a good practice as they provide helpful information to other developers (including your future self!) and tools such as linters and type checkers.\n\n\n\n2.11.1 Parameters\nParameters are variables defined within the function signature, enabling you to pass input values to the function when it is called. Parameters can have a default value assigned to them when no value is provided during the function call. Default values can make your functions more flexible and easy to use. Using the *args and **kwargs syntax, you can pass a variable number of positional or keyword arguments to a function, providing greater flexibility for handling different input scenarios.2\n\n\n\n\n\n\nNoteParameter vs.¬†argument\n\n\n\nThe terms function parameter and argument refer to different concepts related to functions.\nFunction parameter: A function parameter is a variable defined in the function‚Äôs definition or signature. It represents a value that the function expects to receive when it is called. Parameters act as placeholders for the actual values that will be passed as arguments when the function is invoked.\nArgument: An argument is the actual value that is passed to a function when it is called. It corresponds to a specific function parameter and provides the actual data or input that the function operates on. Arguments are supplied in the function call, within parentheses, and are passed to the corresponding function parameters based on their position or using keyword arguments.\n\n\n\ndef calculate_roi(investment: float, profit: float) -&gt; float:\n    return (profit / investment) * 100.0\n\n\ninvestment = 2000.00\nprofit = 500.00\nroi = calculate_roi(investment, profit)\nprint(roi)\n\n25.0\n\n\nIn the previous example, variables investment and profit are passed to the function calculate_roi() in the same order as they are defined in the function definition. This is called positional arguments. The names of the variables do not matter, only the order in which they are passed to the function. If we invert the order of the variables, the result will be different.\n\nbad_roi = calculate_roi(profit, investment)\nprint(bad_roi)\n\n400.0\n\n\nPositional arguments can be confusing when the function has many parameters or when the order of the arguments is not obvious. To avoid this, you can use keyword arguments.\n\nroi1 = calculate_roi(investment=2000.00, profit=500.00)\nprint(roi1)\n\nroi2 = calculate_roi(profit=500.00, investment=2000.00)\nprint(roi2)\n\nroi3 = calculate_roi(profit=profit, investment=investment)\nprint(roi3)\n\n25.0\n25.0\n25.0\n\n\nNote that the name of the original variables does not matter, only the name of the parameters in the function definition. In the last example, we use the same names for the variables and the parameters, but the Python interpreter does not care about that. The following code is equivalent to the previous one:\n\nx = 2000.00\ny = 500.00\n\nroi4 = calculate_roi(profit=y, investment=x)\nprint(roi4)\n\n25.0\n\n\nYou can also mix positional and keyword arguments. However, positional arguments must always come before keyword arguments.\n\nroi4 = calculate_roi(investment, profit=profit)\nprint(roi4)\n\n25.0\n\n\nDefault parameters are useful when you want to provide a default value for a parameter, which is used when no value is provided during the function call. This makes your functions more flexible and easy to use, as you can omit parameters that have a default value.\nTo define a default parameter, assign a value to the parameter in the function definition using =. When the function is called, the default value will be used if no value is provided for that parameter. If a value is provided, it will override the default value.\n\ndef calculate_present_value(cashflow: float, discount_rate: float = 0.1) -&gt; float:\n    return cashflow / (1 + discount_rate)\n\n\n# Uses default discount_rate of 10%\npv = calculate_present_value(cashflow=100.00)\nprint(f\"Present Value: {pv}\")\n\n# Uses discount_rate of 5%\npv2 = calculate_present_value(cashflow=100.00, discount_rate=0.05)\nprint(f\"Present Value: {pv2}\")\n\nPresent Value: 90.9090909090909\nPresent Value: 95.23809523809524\n\n\nParameters with default values must come after parameters without default values. Otherwise, the function call will raise a SyntaxError. When you call a function with default parameters, you can omit any parameters that have a default value. However, when you omit a parameter, you must use keyword parameters to specify the values for the parameters that follow it.\n\n\n2.11.2 Passing arguments: peek under the hood\nPython uses a mechanism called ‚Äúpassing arguments by assignment.‚Äù In simple terms, this means that when you pass an argument to a function, a copy of the reference to the object is made and assigned to the function parameter.\nWhen an immutable object (like a number, string, or tuple) is passed as an argument, it is effectively passed by value. Any modifications made to the parameter within the function do not affect the original object outside the function. Changes to the parameter create a new object rather than modifying the original one.\nOn the other hand, when a mutable object (like a list or dictionary) is passed as an argument, it is effectively passed by reference. Any modifications made to the parameter within the function will affect the original object outside the function. This is because both the parameter and the original object refer to the same memory location, so changes are reflected in both.\n\n\n2.11.3 Return values\nFunctions can return a value, multiple values, or no value at all. To return a value, use the return keyword followed by the value or expression you want to return. If a function doesn‚Äôt include a return statement, it will implicitly return None. A function can contain multiple return statements, but the execution of the function will stop as soon as any of them is reached.\nA function can return a single value, such as a number, string, or a more complex data structure. A function can also return multiple values, typically in the form of a tuple. This is useful when you need to return several related results from a single function call. If a function doesn‚Äôt explicitly return a value using the return keyword, it will implicitly return None when it reaches the end of the function body.\n\ndef calculate_mean_and_median(numbers: list[float]) -&gt; tuple[float, float]:\n    mean = sum(numbers) / len(numbers)\n\n    # Sort the numbers in ascending order using the sorted() function\n    sorted_numbers = sorted(numbers)\n    length = len(sorted_numbers)\n\n    if length % 2 == 0:\n        median = (sorted_numbers[length // 2 - 1] + sorted_numbers[length // 2]) / 2\n    else:\n        median = sorted_numbers[length // 2]\n\n    return mean, median\n\n\nprices = [150.25, 1200.50, 250.00]\nmean, median = calculate_mean_and_median(prices)\nprint(f\"Mean: {mean}, Median: {median}\")\n\nMean: 533.5833333333334, Median: 250.0\n\n\n\n\n2.11.4 Scope\nIn Python, the scope of a variable refers to the region of a program where the variable is accessible and can be referenced. The scope determines the visibility and lifetime of a variable, including where it can be accessed and modified.\nWhen using Python functions, there are two main scopes to consider:\n\nLocal scope (function scope): Variables defined within a function have local scope. They are accessible only within the function where they are defined. Local variables are created when the function is called and destroyed when the function execution completes or reaches a return statement. They are not accessible outside the function.\nGlobal scope (module scope): Variables defined outside of any function in the interactive window or in a Python script, have global scope. They are accessible from anywhere within the program, including all functions.\n\nWhen a function is called, it creates a new local scope, which is independent of the global scope. Inside the function, the local scope takes precedence over the global scope. If a variable is referenced within a function, Python first checks the local scope for its existence. If not found, it then searches the global scope.\n\n# Global variable\nmessage = \"Hello\"\nx = 123\n\n\ndef say_hello(m: str):\n    # Local variable\n    message = \"Hello, World!\"\n    print(f\"local message = {message}\")\n\n    # Local variable, copied from the argument\n    print(f\"local m = {m}\")\n\n    # print(f\"global x inside function = {x}\") \n1\n\n    # Local variable\n    x = len(m)\n    print(f\"local x = {x}\")\n\n    return x\n\n\ny = say_hello(message)\n\nprint(f\"global message = {message}\")\nprint(f\"global x after function = {x}\")\nprint(f\"global y = {y}\")\n\n\n1\n\nThis will access the global x if there is no local variable with the same name. In this specific case, it will cause an error because x is actually defined later in the function.\n\n\n\n\nlocal message = Hello, World!\nlocal m = Hello\nlocal x = 5\nglobal message = Hello\nglobal x after function = 123\nglobal y = 5\n\n\nIf you want to modify a global variable from within a function, you can use the global keyword to indicate that the variable being referred to is a global variable rather than creating a new local variable. However, this is generally not recommended, as it can lead to unexpected side effects and make the code difficult to understand and debug.\nIt is important to carefully manage variable scope to avoid naming conflicts and unintended side effects. Understanding the scope of variables helps in organizing and managing data within functions and modules effectively.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#loops",
    "href": "python/python-basics/index.html#loops",
    "title": "2¬† Python Syntax",
    "section": "2.12 Loops",
    "text": "2.12 Loops\nLoops enable you to easily perform repetitive tasks or iterate through data structures, such as sequences and collections. Python provides two primary loop constructs: the for loop and the while loop.\n\n2.12.1 for loops\nFor loops in Python are used to iterate over a sequence (e.g., a list, tuple, or string) or other iterable objects. The loop iterates through each item in the sequence, executing a block of code for each item. The ‚Äòfor‚Äô loop has the following syntax:\nfor item in sequence:\n    # code to execute for each item in the sequence\nAs for function bodies and conditional statements, the code block in a loop must be indented.\n\n\n2.12.2 range() function\nThe built-in range() function in Python is often used in conjunction with for loops to generate a sequence of numbers. This function can be used to create a range of numbers with a specified start, end, and step size. The syntax for the range function is:\nrange(start, stop, step)\nThe ‚Äòstart‚Äô and ‚Äòstep‚Äô arguments are optional, with default values of 0 and 1, respectively. The ‚Äòstop‚Äô argument is required and defines the upper limit of the range (exclusive).\n\n1for i in range(5):\n    print(i)\n\n\n1\n\nThe range(5) function generates a sequence of numbers from 0 to 4 (inclusive) with a step of 1.\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n1for i in range(2, 7):\n    print(i)\n\n\n1\n\nThe range(2, 7) function generates a sequence of numbers from 2 to 6 (inclusive) with a step of 1.\n\n\n\n\n2\n3\n4\n5\n6\n\n\n\n1for i in range(1, 11, 2):\n    print(i)\n\n\n1\n\nThe range(1, 11, 2) function generates a sequence of numbers from 1 to 10 (inclusive) with a step of 2.\n\n\n\n\n1\n3\n5\n7\n9\n\n\nYou can use a negative step size to generate a sequence of numbers in reverse order.\n\n1for i in range(5, 0, -1):\n    print(i)\n\n\n1\n\nThe range(5, 0, -1) function generates a sequence of numbers from 5 to 1 (inclusive) with a step of -1 (decreasing).\n\n\n\n\n5\n4\n3\n2\n1\n\n\nYou can use the range() function to generate a sequence of numbers and iterate through them using a for loop to execute some code for each number in the sequence. In this example, we use the range() function to generate a sequence of numbers from 1 to 5 (inclusive) and calculate the compound interest for each year of an investment.\n\nprincipal = 1000\nrate = 0.05\n\nfor year in range(1, 6):\n    interest = principal * ((1 + rate) ** year - 1)\n    print(f\"Year {year}: Interest = {interest:.2f}\")\n\nYear 1: Interest = 50.00\nYear 2: Interest = 102.50\nYear 3: Interest = 157.63\nYear 4: Interest = 215.51\nYear 5: Interest = 276.28\n\n\n\n\n2.12.3 continue and break statements\nYou can use the continue and break statements to control the flow of a for loop. The continue statement skips the current iteration and continues with the next one. The break statement terminates the loop and transfers execution to the statement immediately following the loop.\n\nfor i in range(10):\n    if i == 3:\n1        continue\n    elif i == 5:\n2        break\n    print(i)\n\n\n1\n\nSkip the rest of the code in the loop and go to the next iteration\n\n2\n\nExit the loop\n\n\n\n\n0\n1\n2\n4\n\n\n\n\n2.12.4 for loops with lists\nYou can also loop over a collection of items using the for loop. For example, you can loop over a list of numbers to calculate the sum of all numbers in the list.\n\ndaily_profit_losses = [1500, 1200, 1800, 2300, 900]\n\ntotal_pl = 0\nfor pl in daily_profit_losses:\n    total_pl += pl\n\nprint(f\"Total P&L for the period: {total_pl}\")\n\nTotal P&L for the period: 7700\n\n\n\n\n\n\n\n\nTipBuilt-in functions\n\n\n\nPython provides several built-in functions that can be used to perform common tasks. For example, the sum() function can be used to calculate the sum of all numbers in a list.\n\ndaily_profit_losses = [1500, 1200, 1800, 2300, 900]\ntotal_pl = sum(daily_profit_losses)\nprint(f\"Total P&L for the period: {total_pl}\")\n\nTotal P&L for the period: 7700\n\n\n\n\nYou can combine two lists of the same length using the zip() built-in function to loop over both lists at the same time.\n\nstock_prices = [150.25, 1200.50, 250.00]\nquantities = [10, 5, 20]\n\ntotal_value = 0\nfor price, quantity in zip(stock_prices, quantities):\n    total_value += price * quantity\n\nprint(f\"Total value of the portfolio: {total_value:.2f}\")\n\nTotal value of the portfolio: 12505.00\n\n\nYou can use the enumerate() function to loop over a list and get the index of each item in the list.\n\ncash_flows = [100, 200, 300, 400, 500]\n\ndiscount_rate = 0.1\n\npresent_values = []\nfor year, cash_flow in enumerate(cash_flows):\n    present_value = cash_flow / (1 + discount_rate) ** year\n    print(f\"Year {year}: Present Value = {present_value:.2f}\")\n\nYear 0: Present Value = 100.00\nYear 1: Present Value = 181.82\nYear 2: Present Value = 247.93\nYear 3: Present Value = 300.53\nYear 4: Present Value = 341.51\n\n\n\n\n\n\n\n\nNoteIterables and iterators\n\n\n\nIn Python, an iterable is an object capable of returning its elements one at a time, such as a list, tuple, or string. An iterator is an object that keeps track of its current position within an iterable and provides a way to access the next element when required.\nMany built-in data types and functions return values in Python are iterables or iterators. For example, the range() function returns an iterator that produces a sequence of numbers. The zip() function returns an iterator that produces tuples containing elements from the input iterables. The enumerate() function returns an iterator that produces tuples containing the index and value of each item in the input iterable.\nThere are many benefits to iterators, such as better memory efficiency and allowing you to work with infinite sequences, such as the sequence of all prime numbers. However, you can‚Äôt print the result of calling an iterator like zip() directly. Instead, you must convert the iterator to a list or another collection using the list() function.\n\nstock_prices = [150.25, 1200.50, 250.00]\nquantities = [10, 5, 20]\n\nzipped = zip(stock_prices, quantities)\n\nprint(f\"zipped: {zipped}\")\nprint(f\"list(zipped): {list(zipped)}\")\n\nzipped: &lt;zip object at 0x124798080&gt;\nlist(zipped): [(150.25, 10), (1200.5, 5), (250.0, 20)]\n\n\n\n\n\n\n\n2.12.5 Nested for loops\nYou can nest loops. The inner loop will be executed one time for each iteration of the outer loop.\nIn this example, we have a list of products, each with a name, per-item profit margin, and a list of quantities sold at different times. The outer loop iterates through each product, while the inner loop iterates through the quantities for each product. The product‚Äôs profit is calculated by multiplying its margin by the quantity sold and adding it to the product_profit variable. The total_profit variable accumulates the profit for all products.\n\nproducts = [\n    {\"name\": \"Product A\", \"margin\": 10, \"quantities\": [5, 10, 15]},\n    {\"name\": \"Product B\", \"margin\": 20, \"quantities\": [2, 4, 6]},\n    {\"name\": \"Product C\", \"margin\": 30, \"quantities\": [1, 3, 5]},\n]\n\ntotal_profit = 0\n\nfor product in products:\n    product_profit = 0\n    for quantity in product[\"quantities\"]:\n        product_profit += product[\"margin\"] * quantity\n    total_profit += product_profit\n    print(f\"Profit for {product['name']}: {product_profit}\")\n\nprint(f\"Total profit: {total_profit}\")\n\nProfit for Product A: 300\nProfit for Product B: 240\nProfit for Product C: 270\nTotal profit: 810\n\n\n\n\n2.12.6 while loops\nWhile loops are used to repeatedly execute a block of code as long as a specified condition is True. The while loop has the following syntax:\nwhile condition:\n    # code to execute while the condition is True\nIn this example, we use a while loop to calculate the number of years it takes for an investment to double at a given interest rate.\n\nprincipal = 1000\nrate = 0.05\nbalance = principal\ntarget = principal * 2\nyears = 0\n\nwhile balance &lt; target:\n    interest = balance * rate\n    balance += interest\n    years += 1\n\nprint(f\"It takes {years} years for the investment to double.\")\n\nIt takes 15 years for the investment to double.\n\n\nYou can use the continue statement to skip the rest of the code in the current iteration and continue with the next one and the break statement to exit a while loop before the condition becomes False.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#list-and-dictionary-comprehensions",
    "href": "python/python-basics/index.html#list-and-dictionary-comprehensions",
    "title": "2¬† Python Syntax",
    "section": "2.13 List and dictionary comprehensions",
    "text": "2.13 List and dictionary comprehensions\nPython supports list and dictionary comprehensions, which allow you to create lists and dictionaries in a concise and efficient manner by transforming or filtering items from another iterable, such as a range, a tuple or another list. Comprehensions can be confusing at first, but they are a powerful tool worth learning.\n\n2.13.1 List comprehensions\nList comprehension syntax consists of square brackets containing an expression followed by a for clause, then zero or more for or if clauses. The expression can be anything, meaning you can put in all kinds of objects in lists.\n\n# This is perfectly valid:\nsquared1 = []\nfor x in range(10):\n    squared1.append(x * x)\n\nprint(squared1)\n\n# This is much shorter!\nsquared2 = [x * x for x in range(10)]\n\nprint(squared2)\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nYou can use list comprehensions to transform items from a list into a new list using complex expressions.\n\n# Calculate the percentage change for a list of stock prices\nstock_prices = [150.25, 1200.50, 250.00, 175.00, 305.75]\npercentage_changes = [\n    (stock_prices[i + 1] - stock_prices[i]) / stock_prices[i] * 100\n    for i in range(len(stock_prices) - 1)\n]\n\nprint(\"Percentage changes:\", percentage_changes)\n\nPercentage changes: [699.0016638935108, -79.17534360683048, -30.0, 74.71428571428571]\n\n\n\n\n2.13.2 Dictionary comprehensions\nSimilar to list comprehensions, dictionary comprehensions use a single line of code to define the structure of the new dictionary.\n\n# Create a dictionary mapping numbers to their squares\nsquares = {i: i**2 for i in range(1, 6)}\n\nprint(squares)\n\n{1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n\n\n\n\n2.13.3 Filtering and transforming\nYou can use conditional statements in list and dictionary comprehensions to filter items from the source iterable.\n\n# Create a dictionary mapping even numbers to their cubes\neven_cubes = {i: i**3 for i in range(1, 6) if i % 2 == 0}\n\nprint(even_cubes)\n\n{2: 8, 4: 64}\n\n\nYou can transform the items in the source iterable before adding them to the new list or dictionary.\n\n# List of stock symbols and prices\nstock_data = [(\"AAPL\", 150.25), (\"GOOG\", 1200.50), (\"MSFT\", 250.00)]\n\n# Create a dictionary mapping lowercase stock symbols to their prices\nstocks = {symbol.lower(): price for symbol, price in stock_data}\n\nprint(stocks)\n\n{'aapl': 150.25, 'goog': 1200.5, 'msft': 250.0}\n\n\n\n\n2.13.4 Nested comprehensions\nYou can nest comprehensions inside other comprehensions to create complex data structures.\n\n# Create a list of (stock, prices) tuples\nstock_data = [\n    (\"AAPL\", [150.25, 150.50, 150.75]),\n    (\"GOOG\", [1200.50, 1201.00]),\n    (\"MSFT\", [250.00, 250.25, 250.50, 250.75]),\n]\n\n1stocks = [(symbol, price) for symbol, prices in stock_data for price in prices]\nprint(stocks)\n\n\n1\n\nThe comprehension is evaluated from left to right, so the prices variable is available in the second for clause.\n\n\n\n\n[('AAPL', 150.25), ('AAPL', 150.5), ('AAPL', 150.75), ('GOOG', 1200.5), ('GOOG', 1201.0), ('MSFT', 250.0), ('MSFT', 250.25), ('MSFT', 250.5), ('MSFT', 250.75)]\n\n\n\n\n\n\n\n\nWarningNested comprehensions vs readability\n\n\n\nNested comprehensions with more than two levels can be difficult to read, so you should avoid them if possible. If you find yourself nesting comprehensions, it‚Äôs probably a good idea to use a regular for loop instead. Remember, code readability is more important than brevity.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#working-with-files-and-the-with-statement",
    "href": "python/python-basics/index.html#working-with-files-and-the-with-statement",
    "title": "2¬† Python Syntax",
    "section": "2.14 Working with files and the with statement",
    "text": "2.14 Working with files and the with statement\nSo far, we‚Äôve focused on working with data in memory. But in practice, you‚Äôll often need to read data from files or save results to disk. Python provides powerful tools for working with files, and understanding them requires introducing an important concept: context managers and the with statement.\n\n2.14.1 The with statement\nWhen you open a file, you acquire a resource that needs to be properly released when you‚Äôre done. If you forget to close a file, you can run into problems: data might not be written to disk, you could run out of file handles, or other programs might not be able to access the file.\nThe with statement solves this problem elegantly. It ensures that cleanup happens automatically, even if an error occurs while you‚Äôre working with the resource. Here‚Äôs the basic pattern:\nwith some_resource as name:\n    # work with the resource\n    ...\n# resource is automatically cleaned up here\nThe with statement works with any object that implements the context manager protocol. These objects define what should happen when entering the with block (acquiring the resource) and when exiting it (releasing the resource). Many built-in Python types support this, including files, database connections, and locks.\n\n\n\n\n\n\nTipWhy use with?\n\n\n\nYou might be tempted to open and close files manually:\nf = open(\"data.txt\")\ncontent = f.read()\nf.close()  # Easy to forget!\nThis works, but it‚Äôs risky. If an error occurs before close() is called, the file might remain open. The with statement guarantees cleanup:\nwith open(\"data.txt\") as f:\n    content = f.read()\n# File is automatically closed, even if an error occurred\nAlways prefer with when working with files and other resources.\n\n\n\n\n2.14.2 Reading and writing files with open()\nThe built-in open() function is Python‚Äôs standard way to work with files. It takes a file path and a mode, and returns a file object that you can read from or write to.\nCommon modes include:\n\n\"r\" - read mode (default)\n\"w\" - write mode (creates a new file or overwrites existing)\n\"a\" - append mode (adds to the end of an existing file)\n\"x\" - exclusive creation (fails if the file already exists)\n\nFor text files, you can read the entire contents at once or process line by line:\n\n# First, let's create a sample file to work with\nsample_data = \"\"\"Date,Ticker,Price\n2024-01-02,AAPL,185.64\n2024-01-02,MSFT,374.58\n2024-01-03,AAPL,184.25\"\"\"\n\nwith open(\"sample_prices.csv\", \"w\") as f:\n    f.write(sample_data)\n\n# Now read it back\nwith open(\"sample_prices.csv\", \"r\") as f:\n    content = f.read()\n\nprint(content)\n\nDate,Ticker,Price\n2024-01-02,AAPL,185.64\n2024-01-02,MSFT,374.58\n2024-01-03,AAPL,184.25\n\n\nFor processing files line by line, you can iterate directly over the file object:\n\nwith open(\"sample_prices.csv\", \"r\") as f:\n    for line in f:\n        print(f\"Line: {line.strip()}\")  # strip() removes the newline character\n\nLine: Date,Ticker,Price\nLine: 2024-01-02,AAPL,185.64\nLine: 2024-01-02,MSFT,374.58\nLine: 2024-01-03,AAPL,184.25\n\n\nThis approach is memory-efficient for large files because it only loads one line at a time.\n\n\n\n\n\n\nWarningFile encoding\n\n\n\nWhen working with text files, be aware of character encoding. The default encoding depends on your system, which can cause problems when sharing files. It‚Äôs good practice to specify the encoding explicitly:\nwith open(\"data.txt\", \"r\", encoding=\"utf-8\") as f:\n    content = f.read()\nUTF-8 is the most common encoding for modern text files and handles international characters well.\n\n\n\n\n2.14.3 Working with paths using pathlib\nPython‚Äôs pathlib module provides a modern, object-oriented approach to working with file paths. Instead of manipulating path strings directly, you work with Path objects that understand the structure of file paths on your operating system.\n\nfrom pathlib import Path\n\n# Create a Path object\ndata_dir = Path(\"data\")\nfile_path = data_dir / \"prices.csv\"  # Use / to join paths\n\nprint(f\"Full path: {file_path}\")\nprint(f\"Parent directory: {file_path.parent}\")\nprint(f\"File name: {file_path.name}\")\nprint(f\"File extension: {file_path.suffix}\")\n\nFull path: data/prices.csv\nParent directory: data\nFile name: prices.csv\nFile extension: .csv\n\n\nThe / operator for joining paths is particularly elegant. It works correctly across different operating systems, so you don‚Äôt need to worry about whether to use forward slashes or backslashes.\nPath objects have convenient methods for common file operations:\n\nfrom pathlib import Path\n\n# Check if a file exists\nsample_file = Path(\"sample_prices.csv\")\nprint(f\"File exists: {sample_file.exists()}\")\n\n# Read text directly (no need for open!)\nif sample_file.exists():\n    content = sample_file.read_text()\n    print(f\"First 50 characters: {content[:50]}...\")\n\nFile exists: True\nFirst 50 characters: Date,Ticker,Price\n2024-01-02,AAPL,185.64\n2024-01-0...\n\n\nFor simple read and write operations, Path methods like read_text() and write_text() are often more concise than using open():\n\nfrom pathlib import Path\n\n# Write text to a file\noutput_file = Path(\"output.txt\")\noutput_file.write_text(\"Hello from pathlib!\")\n\n# Read it back\nprint(output_file.read_text())\n\n# Clean up\noutput_file.unlink()  # Delete the file\n\nHello from pathlib!\n\n\nWhen you need more control, such as reading line by line or appending to a file, Path objects work seamlessly with open():\n\nfrom pathlib import Path\n\nfile_path = Path(\"sample_prices.csv\")\n\nwith file_path.open(\"r\") as f:\n    header = f.readline()\n    print(f\"Header: {header.strip()}\")\n\nHeader: Date,Ticker,Price\n\n\n\n\n\n\n\n\nTipPath vs string paths\n\n\n\nWhile you can use plain strings for file paths, Path objects offer several advantages:\n\nCross-platform compatibility: Path handles the differences between Windows (\\) and Unix (/) automatically\nClear intent: Code using Path clearly signals you‚Äôre working with file paths\nConvenient methods: Methods like exists(), is_dir(), suffix, and stem make common operations easy\nComposability: The / operator makes building paths readable and safe\n\nMost modern Python code uses pathlib for path manipulation. You‚Äôll see it throughout this book when working with files.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#pattern-matching",
    "href": "python/python-basics/index.html#pattern-matching",
    "title": "2¬† Python Syntax",
    "section": "2.15 Pattern matching",
    "text": "2.15 Pattern matching\nPattern matching is a powerful feature introduced in Python 3.10. It allows you to match the structure of data and execute code based on the shape and contents of that data. It is particularly useful for working with complex data structures and can lead to cleaner and more readable code.\n\n\n\n\n\n\nCautionPython 3.10+ only\n\n\n\nPattern matching is a new feature introduced in Python 3.10, released on October 4, 2021. If you‚Äôre using an older version of Python, or your code will be running on a system with an older version of Python, you should avoid using pattern matching.\n\n\nPattern matching is implemented using the match statement, which is similar to a switch-case statement in other languages but with more advanced capabilities. The match statement takes an expression and a series of cases. Each case is a pattern that is matched against the expression in turn. If the pattern matches, the code in that case is executed, otherwise, the next case is checked. The match statement can also have a case with the wildcard pattern _, which will always match. If no pattern matches, a MatchError is raised.\n\ndef process_transaction(transaction: tuple):\n    match transaction:\n        case (\"deposit\", amount):\n            print(f\"Deposit: {amount:.2f}\")\n        case (\"withdraw\", amount):\n            print(f\"Withdraw: {amount:.2f}\")\n        case (\"transfer\", amount, recipient):\n            print(f\"Transfer {amount:.2f} to {recipient}\")\n        case _:\n            print(\"Unknown transaction\")\n\nprocess_transaction((\"deposit\", 1000))\nprocess_transaction((\"burn\", 100.00))\nprocess_transaction((\"transfer\", 500.00, \"John Doe\"))\nprocess_transaction((\"withdraw\", 250.00))\n\nDeposit: 1000.00\nUnknown transaction\nTransfer 500.00 to John Doe\nWithdraw: 250.00",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#additional-resources",
    "href": "python/python-basics/index.html#additional-resources",
    "title": "2¬† Python Syntax",
    "section": "2.16 Additional resources",
    "text": "2.16 Additional resources\n\nPython 3.14 documentation\nLubanovic, Bill. Introducing Python, 2nd Edition, O‚ÄôReilly Media, Inc., 2019",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#footnotes",
    "href": "python/python-basics/index.html#footnotes",
    "title": "2¬† Python Syntax",
    "section": "",
    "text": "A pseudo-random number is a sequence of numbers that appear random but are generated using a deterministic algorithm.‚Ü©Ô∏é\nI do not recommend using variable-length parameters unless you have a specific need for them, as they can make your code more complex and harder to read and understand.‚Ü©Ô∏é",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/oop/index.html",
    "href": "python/oop/index.html",
    "title": "3¬† Object-Oriented Programming Basics",
    "section": "",
    "text": "3.1 Classes and Objects\nObject-Oriented Programming (OOP) is a programming paradigm that organizes code around ‚Äúobjects‚Äù rather than functions and logic. While Python fully supports OOP, you don‚Äôt need to use it for everything you do. In fact, for many data analysis tasks, a procedural or functional approach is simpler and more appropriate.\nThat said, understanding the basics of OOP is valuable for several reasons. First, many of the libraries you‚Äôll use in empirical finance are built using OOP principles, so understanding these concepts will help you use them more effectively. Second, there are certain situations in research code where OOP can make your code cleaner, more organized, and easier to maintain. Finally, OOP provides a way to model real-world entities and relationships in your code, which can be particularly useful when working with financial concepts like trades and limit order books.\nIn this chapter, we‚Äôll cover the fundamentals of OOP in Python, focusing on practical applications relevant to empirical finance. We‚Äôll start with the basic concepts of classes and objects, then discuss when OOP is genuinely useful in research code, and finally introduce Python‚Äôs data classes, which provide a streamlined way to work with structured data.\nAt its core, object-oriented programming is about creating custom data types that bundle together related data and the functions that operate on that data. Let‚Äôs break down the key concepts.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Object-Oriented Programming Basics</span>"
    ]
  },
  {
    "objectID": "python/oop/index.html#classes-and-objects",
    "href": "python/oop/index.html#classes-and-objects",
    "title": "3¬† Object-Oriented Programming Basics",
    "section": "",
    "text": "3.1.1 What is a Class?\nA class is essentially a blueprint or template for creating objects. It defines what data an object will hold (attributes) and what operations can be performed on that data (methods). Think of a class as a cookie cutter and objects as the cookies made from that cutter.\nLet‚Äôs start with a simple example. Suppose you‚Äôre working on a project that involves tracking individual trades. Each trade has certain properties: a ticker symbol, a quantity, a price, and whether it‚Äôs a buy or sell. You could represent each trade as a dictionary:\n\ntrade1 = {\n    \"ticker\": \"AAPL\",\n    \"quantity\": 100,\n    \"price\": 150.50,\n    \"side\": \"buy\"\n}\n\ntrade2 = {\n    \"ticker\": \"MSFT\",\n    \"quantity\": 50,\n    \"price\": 280.25,\n    \"side\": \"sell\"\n}\n\nThis works, but it has some limitations. There‚Äôs no guarantee that every trade dictionary has the same keys. You might accidentally misspell a key, or forget to include one. And if you want to calculate the total value of a trade, you need to write that logic separately.\nA class provides a better solution. Here‚Äôs how we might define a Trade class:\n\nclass Trade:\n1    def __init__(self, ticker: str, quantity: int, price: float, side: str):\n2        self.ticker = ticker\n        self.quantity = quantity\n        self.price = price\n        self.side = side\n\n3    def value(self) -&gt; float:\n        \"\"\"Calculate the total value of the trade.\"\"\"\n        return self.quantity * self.price\n\n4    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation of the trade.\"\"\"\n        return f\"Trade({self.ticker}, {self.quantity}, ${self.price}, {self.side})\"\n\n\n1\n\nWe define the class with class Trade:. By convention, class names use CamelCase. The __init__ method is a special method called a constructor. It runs automatically when you create a new object from the class. The self parameter refers to the instance being created.\n\n2\n\nInside __init__, we set attributes on the object using self.attribute_name. These become the object‚Äôs data.\n\n3\n\nThe value method is a regular method that calculates the trade‚Äôs total value. Like all methods, it takes self as its first parameter.\n\n4\n\nThe __repr__ method is another special method that defines how the object should be displayed. Methods that start and end with double underscores are called ‚Äúdunder‚Äù (double underscore) methods or magic methods.\n\n\n\n\nNow we can create trades as objects:\n\ntrade1 = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade2 = Trade(\"MSFT\", 50, 280.25, \"sell\")\n\nprint(trade1)\nprint(f\"Trade value: ${trade1.value():.2f}\")\n\nTrade(AAPL, 100, $150.5, buy)\nTrade value: $15050.00\n\n\nThis is cleaner and more robust. Every Trade object is guaranteed to have the required attributes, and the logic for calculating value is bundled with the data.\n\n\n3.1.2 Attributes and Methods\nLet‚Äôs clarify some terminology:\n\nAttributes are variables that belong to an object. In our example, ticker, quantity, price, and side are attributes.\nMethods are functions that belong to a class. They operate on the object‚Äôs data. In our example, value() is a method.\nInstance refers to a specific object created from a class. trade1 and trade2 are instances of the Trade class.\n\nYou access attributes and call methods using dot notation:\n\nprint(trade1.ticker)      # Accessing an attribute\nprint(trade1.value())     # Calling a method\n\nAAPL\n15050.0\n\n\n\n\n3.1.3 Adding More Functionality\nLet‚Äôs expand our Trade class to include more useful functionality. Suppose we want to compare trades and calculate profit and loss:\n\nclass Trade:\n    def __init__(self, ticker: str, quantity: int, price: float, side: str):\n        self.ticker = ticker\n        self.quantity = quantity\n        self.price = price\n        self.side = side\n\n    def value(self) -&gt; float:\n        \"\"\"Calculate the total value of the trade.\"\"\"\n        return self.quantity * self.price\n\n1    def pnl(self, current_price: float) -&gt; float:\n        \"\"\"Calculate profit/loss relative to a current price.\"\"\"\n        if self.side == \"buy\":\n            return self.quantity * (current_price - self.price)\n        else:  # sell\n            return self.quantity * (self.price - current_price)\n\n    def __repr__(self) -&gt; str:\n        return f\"Trade({self.ticker}, {self.quantity}, ${self.price:.2f}, {self.side})\"\n\n2    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Check if two trades are equal.\"\"\"\n        if not isinstance(other, Trade):\n            return NotImplemented\n        return (self.ticker == other.ticker and\n                self.quantity == other.quantity and\n                self.price == other.price and\n                self.side == other.side)\n\n\n1\n\nThe pnl method calculates the profit or loss based on a current market price. The logic differs for buys (profit when price goes up) and sells (profit when price goes down).\n\n2\n\nThe __eq__ method defines what it means for two Trade objects to be equal. Here, two trades are equal if all their attributes match.\n\n\n\n\nThe __eq__ method is one of several special comparison methods Python supports. Others include __ne__ (not equal, !=), __lt__ (less than, &lt;), __le__ (less than or equal, &lt;=), __gt__ (greater than, &gt;), and __ge__ (greater than or equal, &gt;=). For a complete list of these ‚Äúrich comparison‚Äù methods and other special methods, see the Python documentation on basic customization.\nNow we can do more with our trades:\n\ntrade = Trade(\"AAPL\", 100, 150.50, \"buy\")\nprint(f\"Trade value: ${trade.value():.2f}\")\n\n# Calculate P&L at a current price\ncurrent_price = 160.00\npnl = trade.pnl(current_price)\nprint(f\"P&L at ${current_price:.2f}: ${pnl:.2f}\")\n\n# Test equality\ntrade2 = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade3 = Trade(\"MSFT\", 50, 280.25, \"sell\")\nprint(f\"trade == trade2: {trade == trade2}\")  # Same attributes\nprint(f\"trade == trade3: {trade == trade3}\")  # Different attributes\n\nTrade value: $15050.00\nP&L at $160.00: $950.00\ntrade == trade2: True\ntrade == trade3: False\n\n\n\n\n3.1.4 A More Complex Example: Portfolio Class\nLet‚Äôs build a more sophisticated example: a Portfolio class that manages a collection of trades. This demonstrates how objects can contain other objects:\n\nclass Portfolio:\n    def __init__(self, name):\n        self.name = name\n        self.trades = []\n\n    def add_trade(self, trade):\n        \"\"\"Add a trade to the portfolio.\"\"\"\n        self.trades.append(trade)\n\n    def total_value(self):\n        \"\"\"Calculate the total value of all trades.\"\"\"\n        return sum(trade.value() for trade in self.trades)\n\n    def positions(self):\n        \"\"\"Calculate net position for each ticker.\"\"\"\n        positions = {}\n        for trade in self.trades:\n            if trade.ticker not in positions:\n                positions[trade.ticker] = 0\n\n            if trade.side == \"buy\":\n                positions[trade.ticker] += trade.quantity\n            else:  # sell\n                positions[trade.ticker] -= trade.quantity\n\n        return positions\n\n    def __repr__(self):\n        return f\"Portfolio('{self.name}', {len(self.trades)} trades)\"\n\n    def summary(self):\n        \"\"\"Print a summary of the portfolio.\"\"\"\n        print(f\"Portfolio: {self.name}\")\n        print(f\"Total trades: {len(self.trades)}\")\n        print(f\"Total value: ${self.total_value():.2f}\")\n        print(\"\\nPositions:\")\n        for ticker, quantity in self.positions().items():\n            print(f\"  {ticker}: {quantity} shares\")\n\nNow we can use our Portfolio class:\n\n# Create a portfolio\nportfolio = Portfolio(\"My Research Portfolio\")\n\n# Add some trades\nportfolio.add_trade(Trade(\"AAPL\", 100, 150.50, \"buy\"))\nportfolio.add_trade(Trade(\"AAPL\", 50, 155.00, \"buy\"))\nportfolio.add_trade(Trade(\"MSFT\", 75, 280.25, \"buy\"))\nportfolio.add_trade(Trade(\"AAPL\", 25, 152.00, \"sell\"))\n\n# View summary\nportfolio.summary()\n\nPortfolio: My Research Portfolio\nTotal trades: 4\nTotal value: $47618.75\n\nPositions:\n  AAPL: 125 shares\n  MSFT: 75 shares\n\n\nThis example shows how OOP allows you to build up layers of abstraction. A Portfolio is a collection of Trade objects, and both have methods that make sense for their level of abstraction.\n\n\n\n\n\n\nTipWhen to Use Classes vs.¬†Functions\n\n\n\nDon‚Äôt create a class just to group functions together. If your class only has one or two methods and no meaningful state (attributes), it should probably just be a function. Classes are most useful when you need to maintain state across multiple operations.\n\n\n\n\n3.1.5 String Representations: __repr__ vs.¬†__str__\nPython provides two different methods for converting objects to strings: __repr__ and __str__. Understanding the difference between them helps you write more useful classes.\n\n__repr__ is meant to produce an unambiguous representation of the object, primarily for developers and debugging. Ideally, it should look like a valid Python expression that could recreate the object.\n__str__ is meant to produce a readable, user-friendly string. It‚Äôs what gets displayed when you use print() on an object.\n\nIf you only implement one, implement __repr__. Python will use it as a fallback for __str__ if __str__ isn‚Äôt defined. Here‚Äôs an example showing both:\n\nclass Trade:\n    def __init__(self, ticker: str, quantity: int, price: float, side: str):\n        self.ticker = ticker\n        self.quantity = quantity\n        self.price = price\n        self.side = side\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Unambiguous representation for developers.\"\"\"\n        return f\"Trade({self.ticker!r}, {self.quantity}, {self.price}, {self.side!r})\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"User-friendly representation.\"\"\"\n        action = \"Buy\" if self.side == \"buy\" else \"Sell\"\n        return f\"{action} {self.quantity} shares of {self.ticker} @ ${self.price:.2f}\"\n\ntrade = Trade(\"AAPL\", 100, 150.50, \"buy\")\n\n# __str__ is used by print()\nprint(trade)\n\n# __repr__ is used in the REPL and for debugging\nprint(repr(trade))\n\nBuy 100 shares of AAPL @ $150.50\nTrade('AAPL', 100, 150.5, 'buy')\n\n\n\n\n3.1.6 Rich Display in Jupyter and Quarto\nJupyter notebooks (and Quarto, a publishing system for creating documents from notebooks and other sources) support special methods for rich display. These methods allow your objects to render as HTML, Markdown, or LaTeX instead of plain text:\n\n_repr_html_() returns HTML that will be rendered in the notebook\n_repr_markdown_() returns Markdown text\n_repr_latex_() returns LaTeX for mathematical notation\n\nHere‚Äôs a simple example:\n\nclass Trade:\n    def __init__(self, ticker: str, quantity: int, price: float, side: str):\n        self.ticker = ticker\n        self.quantity = quantity\n        self.price = price\n        self.side = side\n\n    def __repr__(self) -&gt; str:\n        return f\"Trade({self.ticker!r}, {self.quantity}, {self.price}, {self.side!r})\"\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"Rich HTML display for Jupyter/Quarto.\"\"\"\n        color = \"green\" if self.side == \"buy\" else \"red\"\n        return f\"\"\"\n        &lt;div style=\"border: 1px solid #ccc; padding: 10px; border-radius: 5px; width: fit-content;\"&gt;\n            &lt;strong&gt;{self.ticker}&lt;/strong&gt;&lt;br&gt;\n            &lt;span style=\"color: {color};\"&gt;{self.side.upper()}&lt;/span&gt;\n            {self.quantity} shares @ ${self.price:.2f}\n        &lt;/div&gt;\n        \"\"\"\n\n    def _repr_latex_(self) -&gt; str:\n        \"\"\"Rich LaTeX display for PDF output.\"\"\"\n        action = \"Buy\" if self.side == \"buy\" else \"Sell\"\n        return (\n            rf\"\\textbf{{{self.ticker}}}: \"\n            rf\"{action} {self.quantity} shares @ \\${self.price:.2f}\"\n        )\n\ntrade = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade  # In Jupyter/Quarto, this displays as formatted HTML or LaTeX\n\n\n        \n            AAPL\n            BUY\n            100 shares @ $150.50\n        \n        \n\n\nMany of the data analysis libraries you‚Äôll use later in this course, such as pandas, use these methods to display data frames as nicely formatted tables.\n\n\n3.1.7 Class Variables vs.¬†Instance Variables\nSo far, we‚Äôve been working with instance variables‚Äîattributes that are unique to each object. Python also supports class variables, which are shared by all instances of a class:\n\nclass Trade:\n    # Class variable\n    commission_rate = 0.001  # 0.1% commission\n\n    def __init__(self, ticker, quantity, price, side):\n        # Instance variables\n        self.ticker = ticker\n        self.quantity = quantity\n        self.price = price\n        self.side = side\n\n    def value(self):\n        \"\"\"Calculate the total value of the trade.\"\"\"\n        return self.quantity * self.price\n\n    def net_value(self):\n        \"\"\"Calculate value after commission.\"\"\"\n        gross_value = self.value()\n        commission = gross_value * Trade.commission_rate\n        return gross_value - commission\n\n    def __repr__(self):\n        return f\"Trade({self.ticker}, {self.quantity}, ${self.price:.2f}, {self.side})\"\n\n# All trades share the same commission rate\ntrade1 = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade2 = Trade(\"MSFT\", 50, 280.25, \"sell\")\n\nprint(f\"Trade 1 net value: ${trade1.net_value():.2f}\")\nprint(f\"Trade 2 net value: ${trade2.net_value():.2f}\")\n\n# Changing the class variable affects all instances\nTrade.commission_rate = 0.002\nprint(f\"Trade 1 net value (new rate): ${trade1.net_value():.2f}\")\n\nTrade 1 net value: $15034.95\nTrade 2 net value: $13998.49\nTrade 1 net value (new rate): $15019.90\n\n\nClass variables are useful for values that should be consistent across all instances, like constants, default settings, or shared configuration.\n\n\n3.1.8 Property Decorators\nSometimes you want to compute a value on-the-fly rather than storing it as an attribute. Python‚Äôs @property decorator makes this look like a simple attribute access:\n\nclass Trade:\n    def __init__(self, ticker, quantity, price, side):\n        self.ticker = ticker\n        self.quantity = quantity\n        self.price = price\n        self.side = side\n\n    @property\n    def value(self):\n        \"\"\"Calculate the total value of the trade.\"\"\"\n        return self.quantity * self.price\n\n    @property\n    def is_buy(self):\n        \"\"\"Check if this is a buy trade.\"\"\"\n        return self.side == \"buy\"\n\n    def __repr__(self):\n        return f\"Trade({self.ticker}, {self.quantity}, ${self.price:.2f}, {self.side})\"\n\ntrade = Trade(\"AAPL\", 100, 150.50, \"buy\")\n\n# No parentheses needed - looks like an attribute\nprint(f\"Trade value: ${trade.value:.2f}\")\nprint(f\"Is buy: {trade.is_buy}\")\n\nTrade value: $15050.00\nIs buy: True\n\n\nThe advantage of using @property is that it allows you to start with a simple attribute and later change it to a computed value without changing how the class is used. It also makes the code more readable when the value is conceptually an attribute rather than an action.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Object-Oriented Programming Basics</span>"
    ]
  },
  {
    "objectID": "python/oop/index.html#when-oop-is-useful-in-research-code",
    "href": "python/oop/index.html#when-oop-is-useful-in-research-code",
    "title": "3¬† Object-Oriented Programming Basics",
    "section": "3.2 When OOP is Useful in Research Code",
    "text": "3.2 When OOP is Useful in Research Code\nNow that you understand the basics of classes and objects, an important question remains: when should you actually use OOP in your research code?\nThe truth is, many data analysis tasks in empirical finance don‚Äôt require OOP. A straightforward script that loads data, performs some analysis, and generates output can be perfectly fine without defining any classes. In fact, overusing OOP can make simple tasks more complicated than they need to be.\nHowever, there are several scenarios where OOP becomes genuinely useful in research code.\n\n3.2.1 Scenario 1: Managing Complex State\nIf you find yourself passing around many related variables to multiple functions, a class might be appropriate. Consider a simulation configuration that needs to be loaded, validated, and used across multiple functions:\nWithout OOP:\n\nimport json\nfrom pathlib import Path\n\ndef load_simulation_config(filepath):\n    with open(filepath) as f:\n        data = json.load(f)\n    return {\n        \"name\": data[\"name\"],\n        \"n_simulations\": data[\"n_simulations\"],\n        \"initial_value\": data[\"initial_value\"],\n        \"drift\": data[\"drift\"],\n        \"volatility\": data[\"volatility\"],\n        \"results\": None\n    }\n\ndef validate_config(config):\n    if config[\"n_simulations\"] &lt;= 0:\n        raise ValueError(\"n_simulations must be positive\")\n    if config[\"initial_value\"] &lt;= 0:\n        raise ValueError(\"initial_value must be positive\")\n    if config[\"volatility\"] &lt; 0:\n        raise ValueError(\"volatility must be non-negative\")\n    return config\n\ndef run_simulation(config):\n    if config[\"n_simulations\"] &lt;= 0:\n        raise ValueError(\"Invalid config\")\n    # Simulation logic would go here...\n    config[\"results\"] = {\"mean\": 105.2, \"std\": 12.3}\n    return config\n\n# Usage - must remember the correct sequence of function calls\nconfig = load_simulation_config(\"sim_config.json\")\nconfig = validate_config(config)\nconfig = run_simulation(config)\nprint(config[\"results\"])\n\nWith OOP:\n\nimport json\nfrom pathlib import Path\n1import pprint\n\nclass SimulationConfig:\n    def __init__(\n        self,\n        name: str,\n        n_simulations: int,\n        initial_value: float,\n        drift: float,\n        volatility: float,\n    ):\n        self.name = name\n        self.n_simulations = n_simulations\n        self.initial_value = initial_value\n        self.drift = drift\n        self.volatility = volatility\n        self.results: dict | None = None\n\n        # Validation happens automatically on creation\n        self._validate()\n\n    def _validate(self) -&gt; None:\n        \"\"\"Validate the configuration parameters.\"\"\"\n        if self.n_simulations &lt;= 0:\n            raise ValueError(\"n_simulations must be positive\")\n        if self.initial_value &lt;= 0:\n            raise ValueError(\"initial_value must be positive\")\n        if self.volatility &lt; 0:\n            raise ValueError(\"volatility must be non-negative\")\n\n2    @classmethod\n    def from_json(cls, filepath: str | Path) -&gt; \"SimulationConfig\":\n        \"\"\"Create a SimulationConfig from a JSON file.\"\"\"\n        with open(filepath) as f:\n            data = json.load(f)\n        return cls(\n            name=data[\"name\"],\n            n_simulations=data[\"n_simulations\"],\n            initial_value=data[\"initial_value\"],\n            drift=data[\"drift\"],\n            volatility=data[\"volatility\"],\n        )\n\n    def run(self) -&gt; \"SimulationConfig\":\n        \"\"\"Run the simulation.\"\"\"\n        # Simulation logic would go here...\n        self.results = {\"mean\": 105.2, \"std\": 12.3}\n        return self\n\n    def __repr__(self) -&gt; str:\n        status = \"completed\" if self.results else \"not run\"\n        return f\"SimulationConfig({self.name!r}, {self.n_simulations} sims, {status})\"\n\n\n1\n\nThe pprint (pretty print) module from Python‚Äôs standard library formats complex data structures like dictionaries and lists in a more readable way, with proper indentation and line breaks. This is especially useful when displaying nested structures or long lists.\n\n2\n\nThe @classmethod decorator creates a class method‚Äîa method that receives the class itself (conventionally named cls) as its first argument instead of an instance. Class methods are often used as alternative constructors, like from_json() here. In contrast, a @staticmethod doesn‚Äôt receive any implicit first argument and behaves like a regular function that happens to live inside a class.\n\n\n\n\nNow we can use the class:\n\n# Load from file using the class method\nconfig = SimulationConfig.from_json(\"sim_config.json\")\n\n# Or create directly\nconfig = SimulationConfig(\n    name=\"Test Simulation\",\n    n_simulations=1000,\n    initial_value=100.0,\n    drift=0.05,\n    volatility=0.2,\n)\n\n# Run and display results\nconfig.run()\npprint.pprint(config.results)  # Pretty print the results dictionary\n\n\n\n{'mean': 105.2, 'std': 12.3}\n\n\nThe OOP version is cleaner because the state is bundled together, and you don‚Äôt need to pass around a configuration dictionary. The simulation object maintains its own state, making the code more organized and less error-prone.\nBeyond reducing errors, the class also makes your code more clearly defined. With a dictionary, nothing prevents you from accessing a misspelled key like config[\"n_simulaitons\"]‚Äîyou‚Äôll only discover the typo at runtime. With a class, your editor (like VS Code) can immediately flag config.n_simulaitons as an error because it knows exactly which attributes SimulationConfig has. This kind of immediate feedback makes development faster and catches bugs before you even run the code.\n\n\n3.2.2 Scenario 2: Multiple Related Variants\nIf you need to implement several variants of a similar concept, OOP with inheritance can reduce code duplication. For example, different return calculation methods:\n\nimport math\n\nclass Returns:\n    \"\"\"Base class for return calculations.\"\"\"\n\n    def __init__(self, prices: list[float]):\n        self.prices = prices\n\n    def calculate(self) -&gt; list[float]:\n        raise NotImplementedError(\"Subclasses must implement calculate()\")\n\nclass SimpleReturns(Returns):\n    \"\"\"Calculate simple returns: (P_t / P_{t-1}) - 1\"\"\"\n\n    def calculate(self) -&gt; list[float]:\n        return [\n            (self.prices[i] / self.prices[i - 1]) - 1\n            for i in range(1, len(self.prices))\n        ]\n\nclass LogReturns(Returns):\n    \"\"\"Calculate log returns: log(P_t / P_{t-1})\"\"\"\n\n    def calculate(self) -&gt; list[float]:\n        return [\n            math.log(self.prices[i] / self.prices[i - 1])\n            for i in range(1, len(self.prices))\n        ]\n\nclass ExcessReturns(Returns):\n    \"\"\"Calculate excess returns over risk-free rate.\"\"\"\n\n    def __init__(self, prices: list[float], risk_free_rate: float):\n        super().__init__(prices)\n        self.risk_free_rate = risk_free_rate\n\n    def calculate(self) -&gt; list[float]:\n        simple_returns = [\n            (self.prices[i] / self.prices[i - 1]) - 1\n            for i in range(1, len(self.prices))\n        ]\n        return [r - self.risk_free_rate for r in simple_returns]\n\n# Usage\nprices = [100.0, 102.0, 101.0, 105.0, 108.0]\n\nsimple = SimpleReturns(prices)\nprint(\"Simple returns:\", [f\"{r:.4f}\" for r in simple.calculate()])\n\nlog_ret = LogReturns(prices)\nprint(\"Log returns:\", [f\"{r:.4f}\" for r in log_ret.calculate()])\n\nexcess = ExcessReturns(prices, risk_free_rate=0.001)\nprint(\"Excess returns:\", [f\"{r:.4f}\" for r in excess.calculate()])\n\nSimple returns: ['0.0200', '-0.0098', '0.0396', '0.0286']\nLog returns: ['0.0198', '-0.0099', '0.0388', '0.0282']\nExcess returns: ['0.0190', '-0.0108', '0.0386', '0.0276']\n\n\nThis pattern is useful when you want to ensure different variants share a common interface or when you want to write code that works with any of the variants.\n\n\n\n\n\n\nWarningDon‚Äôt Overuse Inheritance\n\n\n\nInheritance can create tight coupling between classes and make code harder to understand. Often, composition (having one class use another as an attribute) is a better choice. Only use inheritance when you have a genuine ‚Äúis-a‚Äù relationship and need to substitute one type for another.\nFor cases where you want classes to share a common interface without inheritance, Python 3.8+ offers Protocols (from the typing module). A Protocol defines what methods and attributes a class should have, without requiring the class to explicitly inherit from anything. This is sometimes called ‚Äústructural subtyping‚Äù or ‚Äúduck typing with type hints.‚Äù\n\n\n\n\n3.2.3 Scenario 3: Encapsulating Complex Data Structures\nWhen working with complex data structures that need validation or computed properties, classes provide a clean way to manage this complexity:\n\nclass EventStudyWindow:\n    \"\"\"Represents an event study window with validation.\"\"\"\n\n    def __init__(self, event_date, estimation_start, estimation_end,\n                 event_start, event_end):\n        self.event_date = event_date\n        self.estimation_start = estimation_start\n        self.estimation_end = estimation_end\n        self.event_start = event_start\n        self.event_end = event_end\n\n        # Validate the window\n        self._validate()\n\n    def _validate(self):\n        \"\"\"Validate that the window makes sense.\"\"\"\n        if self.estimation_end &gt;= self.event_date:\n            raise ValueError(\"Estimation window must end before event date\")\n\n        if self.event_start &gt; self.event_date:\n            raise ValueError(\"Event window start must be at or before event date\")\n\n        if self.event_end &lt; self.event_date:\n            raise ValueError(\"Event window end must be at or after event date\")\n\n    @property\n    def estimation_length(self):\n        \"\"\"Length of the estimation window in days.\"\"\"\n        return (self.estimation_end - self.estimation_start).days\n\n    @property\n    def event_length(self):\n        \"\"\"Length of the event window in days.\"\"\"\n        return (self.event_end - self.event_start).days\n\n    def __repr__(self):\n        return (f\"EventStudyWindow(event={self.event_date}, \"\n                f\"estimation={self.estimation_length} days, \"\n                f\"event_window={self.event_length} days)\")\n\n# Usage\nfrom datetime import date, timedelta\n\nevent_date = date(2024, 6, 15)\nwindow = EventStudyWindow(\n    event_date=event_date,\n    estimation_start=event_date - timedelta(days=260),\n    estimation_end=event_date - timedelta(days=10),\n    event_start=event_date - timedelta(days=1),\n    event_end=event_date + timedelta(days=1)\n)\n\nprint(window)\nprint(f\"Estimation period: {window.estimation_length} days\")\nprint(f\"Event window: {window.event_length} days\")\n\nEventStudyWindow(event=2024-06-15, estimation=250 days, event_window=2 days)\nEstimation period: 250 days\nEvent window: 2 days\n\n\nThe class encapsulates both the data and the logic for validation and computation, making it easier to work with event study windows correctly.\n\n\n3.2.4 Scenario 4: Building Reusable Components\nIf you‚Äôre building functionality that will be reused across multiple projects, classes provide a clean interface:\n\nimport statistics\nimport random\n\nclass RollingWindow:\n    \"\"\"Calculate rolling window statistics.\"\"\"\n\n    def __init__(self, data: list[float], window_size: int):\n        self.data = data\n        self.window_size = window_size\n\n        if len(self.data) &lt; window_size:\n            raise ValueError(\"Data must be longer than window size\")\n\n    def mean(self) -&gt; list[float]:\n        \"\"\"Calculate rolling mean.\"\"\"\n        return [\n            statistics.mean(self.data[i : i + self.window_size])\n            for i in range(len(self) )\n        ]\n\n    def std(self) -&gt; list[float]:\n        \"\"\"Calculate rolling standard deviation.\"\"\"\n        return [\n            statistics.stdev(self.data[i : i + self.window_size])\n            for i in range(len(self))\n        ]\n\n    def sharpe(self, risk_free_rate: float = 0) -&gt; list[float]:\n        \"\"\"Calculate rolling Sharpe ratio.\"\"\"\n        means = self.mean()\n        stds = self.std()\n        return [(m - risk_free_rate) / s for m, s in zip(means, stds)]\n\n    def __len__(self) -&gt; int:\n        return len(self.data) - self.window_size + 1\n\n    def __repr__(self) -&gt; str:\n        return f\"RollingWindow(data_length={len(self.data)}, window={self.window_size})\"\n\n# Generate some sample returns\nrandom.seed(42)\nreturns = [random.gauss(0.001, 0.02) for _ in range(100)]\nrolling = RollingWindow(returns, window_size=10)\n\nprint(f\"Rolling windows: {len(rolling)}\")\nprint(f\"Mean rolling mean: {statistics.mean(rolling.mean()):.4f}\")\nprint(f\"Mean rolling Sharpe: {statistics.mean(rolling.sharpe()):.4f}\")\n\nRolling windows: 91\nMean rolling mean: 0.0019\nMean rolling Sharpe: 0.1290\n\n\n\n\n3.2.5 When to Avoid OOP\nJust as important as knowing when to use OOP is knowing when not to use it. Avoid OOP when:\n\nYou‚Äôre doing one-off analysis: If you‚Äôre exploring data or doing a quick calculation, a simple script is fine.\nYour code is primarily a sequence of transformations: Data pipelines that transform data step-by-step are often clearer as functions rather than classes.\nYou‚Äôre wrapping a single function: Don‚Äôt create a class with only one method. Just use a function.\nIt makes the code more complex: If OOP is making your code harder to understand, you‚Äôre probably not in a situation where it helps.\n\nRemember: the goal is clarity and maintainability, not using OOP for its own sake.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Object-Oriented Programming Basics</span>"
    ]
  },
  {
    "objectID": "python/oop/index.html#data-classes",
    "href": "python/oop/index.html#data-classes",
    "title": "3¬† Object-Oriented Programming Basics",
    "section": "3.3 Data Classes",
    "text": "3.3 Data Classes\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video provides a good introduction to data classes.\n\n\n\nPython 3.7 introduced data classes, which provide a streamlined way to create classes that are primarily used to store data. They automatically generate common methods like __init__, __repr__, and __eq__, reducing boilerplate code significantly.\n\n3.3.1 Basic Data Classes\nLet‚Äôs revisit our Trade class, but this time using a data class:\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass Trade:\n    ticker: str\n    quantity: int\n    price: float\n    side: str\n\n    @property\n    def value(self) -&gt; float:\n        \"\"\"Calculate the total value of the trade.\"\"\"\n        return self.quantity * self.price\n\n# Create trades\ntrade1 = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade2 = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade3 = Trade(\"MSFT\", 50, 280.25, \"sell\")\n\nprint(trade1)\nprint(f\"Value: ${trade1.value:.2f}\") \nprint(f\"trade1 == trade2: {trade1 == trade2}\")\nprint(f\"trade1 == trade3: {trade1 == trade3}\")\n\nTrade(ticker='AAPL', quantity=100, price=150.5, side='buy')\nValue: $15050.00\ntrade1 == trade2: True\ntrade1 == trade3: False\n\n\nWith just the @dataclass decorator and type annotations, we get:\n\nAn __init__ method that accepts all the attributes\nA __repr__ method that shows a useful string representation\nAn __eq__ method that compares instances by their attributes\n\nThis is much less code than writing these methods manually, and it‚Äôs less error-prone.\n\n\n3.3.2 Default Values\nData classes make it easy to specify default values:\n\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass Trade:\n    ticker: str\n    quantity: int\n    price: float\n    side: str = \"buy\"  # default value\n    commission: float = 0.0\n    notes: Optional[str] = None\n\n    def value(self):\n        \"\"\"Calculate the total value of the trade.\"\"\"\n        return self.quantity * self.price\n\n    def net_value(self):\n        \"\"\"Calculate value after commission.\"\"\"\n        return self.value() - self.commission\n\n# Use defaults\ntrade1 = Trade(\"AAPL\", 100, 150.50)\nprint(trade1)\n\n# Override defaults\ntrade2 = Trade(\"MSFT\", 50, 280.25, side=\"sell\", commission=14.00)\nprint(trade2)\nprint(f\"Net value: ${trade2.net_value():.2f}\")\n\nTrade(ticker='AAPL', quantity=100, price=150.5, side='buy', commission=0.0, notes=None)\nTrade(ticker='MSFT', quantity=50, price=280.25, side='sell', commission=14.0, notes=None)\nNet value: $13998.50\n\n\n\n\n3.3.3 Immutable Data Classes\nYou can make a data class immutable by setting frozen=True. This means that once created, the attributes cannot be changed:\n\nfrom dataclasses import dataclass\n\n@dataclass(frozen=True)\nclass Trade:\n    ticker: str\n    quantity: int\n    price: float\n    side: str\n\n    def value(self):\n        return self.quantity * self.price\n\ntrade = Trade(\"AAPL\", 100, 150.50, \"buy\")\nprint(trade)\n\n# This would raise an error:\n# trade.price = 160.00  # FrozenInstanceError\n\nTrade(ticker='AAPL', quantity=100, price=150.5, side='buy')\n\n\nImmutable data classes are useful when you want to ensure that data doesn‚Äôt change unexpectedly, or when you need to use instances as dictionary keys or in sets.\n\n\n3.3.4 Data Classes vs.¬†Regular Classes\nWhen should you use a data class instead of a regular class?\nUse data classes when:\n\nYour class is primarily for storing data\nYou want automatic generation of common methods\nYou want type hints for all attributes\nYou need value-based equality (comparing by content, not identity)\n\nUse regular classes when:\n\nYou need more control over initialization\nThe class has complex behavior with little data\nYou need inheritance from non-dataclass parents\n\n\n\n3.3.5 Data Classes and Type Checking\nData classes work particularly well with static type checkers. The type annotations are not just documentation‚Äîthey can be validated by tools like ty (a fast type checker from Astral, the creators of uv and ruff) or directly in VS Code.\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass Trade:\n    ticker: str\n    quantity: int\n    price: float\n    side: str\n\n# These work fine\ntrade1 = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade2 = Trade(ticker=\"MSFT\", quantity=50, price=280.25, side=\"sell\")\n\n# Runtime Python won't stop these, but type checkers will flag them:\n# trade3 = Trade(ticker=\"AAPL\", quantity=\"100\", price=150.50, side=\"buy\")  # wrong type\n# trade4 = Trade(\"AAPL\", 100, 150.50)  # missing argument\n\nThe real advantage is that VS Code (with the Python or Pylance extension) can highlight these errors as you type, before you even save the file. This immediate feedback helps catch bugs early and makes development faster.\n\n\n\n\n\n\nTipPydantic for Data Validation\n\n\n\nIf you need runtime data validation (not just static type checking), consider Pydantic. It‚Äôs a third-party library that offers functionality similar to dataclasses but validates data types at runtime, converts values to the correct types when possible, and provides detailed error messages when validation fails. Pydantic is particularly useful when working with external data sources like JSON files or API responses.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Object-Oriented Programming Basics</span>"
    ]
  },
  {
    "objectID": "python/code-quality/index.html",
    "href": "python/code-quality/index.html",
    "title": "4¬† Code Quality and Documentation",
    "section": "",
    "text": "4.1 Code Organization and Readability\nWriting code is like writing prose for a dual audience: computers that execute it and humans who read, maintain, and extend it. While any code that runs correctly serves its immediate purpose, the real measure of quality lies in how easily others (including your future self) can understand, trust, and build upon your work.\nIn empirical research, where reproducibility and transparency are paramount, code quality takes on additional importance. A subtle bug in your data processing pipeline can invalidate months of work. Poorly documented functions can make it impossible for reviewers to verify your methodology. Code that works but cannot be understood becomes a liability rather than an asset.\nThis chapter covers the practical tools and techniques for writing clean, clear, readable, reproducible, and reliable code. We will explore how to organize your code for readability, document it effectively, catch errors before they cause problems, and maintain consistent style across your projects. These practices are not about perfectionism‚Äîthey are about making your research more reliable, your collaboration more effective, and your future work easier.\nThe foundation of code quality is organization. Well-organized code reveals its structure and intent at a glance, making it easier to navigate, debug, and modify. This section covers the principles that make code readable and the practical techniques for achieving them.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Code Quality and Documentation</span>"
    ]
  },
  {
    "objectID": "python/code-quality/index.html#code-organization-and-readability",
    "href": "python/code-quality/index.html#code-organization-and-readability",
    "title": "4¬† Code Quality and Documentation",
    "section": "",
    "text": "4.1.1 The Principle of Least Surprise\nGood code should behave the way readers expect. This means following established conventions, using descriptive names, and structuring your logic in clear, predictable ways. When you need to deviate from conventions, document why.\nConsider two approaches to calculating portfolio returns:\n# Unclear: cryptic names and unexpected structure\ndef calc(d, w):\n    r = []\n    for i in range(len(d)):\n        r.append(sum([d[i][j] * w[j] for j in range(len(w))]))\n    return r\n\n# Clear: descriptive names and explicit structure\ndef calculate_portfolio_returns(asset_returns, weights):\n    \"\"\"Calculate portfolio returns given asset returns and weights.\"\"\"\n    portfolio_returns = []\n    for period_returns in asset_returns:\n        period_portfolio_return = sum(\n            asset_return * weight\n            for asset_return, weight in zip(period_returns, weights)\n        )\n        portfolio_returns.append(period_portfolio_return)\n    return portfolio_returns\nThe second version is longer, but its intent is immediately clear. The function name describes what it does, parameter names indicate what inputs are expected, and the logic is explicit rather than compressed.\n\n\n4.1.2 Project Directory Structure\nA well-organized directory structure makes projects easier to navigate and understand. For empirical research projects, we recommend the following layout:\nmy-project/\n1‚îú‚îÄ‚îÄ conf/\n‚îÇ   ‚îî‚îÄ‚îÄ config.yaml\n2‚îú‚îÄ‚îÄ data/\n3‚îÇ   ‚îú‚îÄ‚îÄ raw/\n4‚îÇ   ‚îú‚îÄ‚îÄ clean/\n5‚îÇ   ‚îî‚îÄ‚îÄ results/\n6‚îú‚îÄ‚îÄ notebooks/\n7‚îú‚îÄ‚îÄ notes/\n8‚îú‚îÄ‚îÄ paper/\n9‚îú‚îÄ‚îÄ slides/\n10‚îú‚îÄ‚îÄ src/my_project/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n11‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py\n12‚îÇ   ‚îî‚îÄ‚îÄ utils/\n13‚îú‚îÄ‚îÄ tests/\n14‚îú‚îÄ‚îÄ .gitignore\n15‚îú‚îÄ‚îÄ pyproject.toml\n16‚îî‚îÄ‚îÄ README.md\n\n1\n\nConfiguration files for your analysis pipeline\n\n2\n\nAll data files, organized by processing stage\n\n3\n\nOriginal unprocessed data (never modify these files)\n\n4\n\nProcessed and cleaned datasets\n\n5\n\nAnalysis outputs like regression results\n\n6\n\nJupyter notebooks for exploration and prototyping\n\n7\n\nResearch notes and documentation\n\n8\n\nPaper manuscript (Quarto or LaTeX)\n\n9\n\nPresentation slides\n\n10\n\nMain Python package with your reusable code\n\n11\n\nMain analysis pipeline script\n\n12\n\nUtility functions and helpers\n\n13\n\nUnit tests for your code\n\n14\n\nFiles to exclude from version control\n\n15\n\nProject dependencies and metadata\n\n16\n\nProject overview and setup instructions\n\n\nThis structure separates concerns clearly: raw data stays pristine, processed data is reproducible, and code is organized into reusable modules. The src/ directory pattern keeps your package importable while maintaining a clean project root.\nWhen working with version control, we usually want to keep data and results in the different location. We discuss this in Chapter 8.\n\n\n4.1.3 Function Design\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video covers function design. Note that this is an external resource that may present concepts differently than those covered here.\n\n\n\nFunctions should do one thing well. A function that does multiple unrelated tasks is harder to test, harder to reuse, and harder to understand. Consider this example:\n# Poor: one function doing too much\ndef analyze_data(filepath):\n    # Read data\n    with open(filepath) as f:\n        lines = f.readlines()\n\n    # Parse and clean data\n    values = []\n    for line in lines:\n        parts = line.strip().split(',')\n        if len(parts) &gt;= 2 and parts[1]:\n            values.append(float(parts[1]))\n\n    # Calculate statistics\n    mean = sum(values) / len(values)\n    squared_diffs = [(x - mean) ** 2 for x in values]\n    std = (sum(squared_diffs) / len(values)) ** 0.5\n\n    # Save results\n    with open('stats.txt', 'w') as f:\n        f.write(f'Mean: {mean}\\nStd: {std}')\n\n    return values\n\n# Better: separate concerns into focused functions\ndef read_data_file(filepath):\n    \"\"\"Read lines from a data file.\"\"\"\n    with open(filepath) as f:\n        return f.readlines()\n\ndef parse_values(lines, column=1):\n    \"\"\"Extract numeric values from CSV lines.\"\"\"\n    values = []\n    for line in lines:\n        parts = line.strip().split(',')\n        if len(parts) &gt; column and parts[column]:\n            values.append(float(parts[column]))\n    return values\n\ndef calculate_mean(values):\n    \"\"\"Calculate the arithmetic mean of a list of numbers.\"\"\"\n    if not values:\n        raise ValueError(\"Cannot calculate mean of empty list\")\n    return sum(values) / len(values)\n\ndef calculate_std(values):\n    \"\"\"Calculate the standard deviation of a list of numbers.\"\"\"\n    if len(values) &lt; 2:\n        raise ValueError(\"Need at least 2 values for standard deviation\")\n    mean = calculate_mean(values)\n    squared_diffs = [(x - mean) ** 2 for x in values]\n    return (sum(squared_diffs) / len(values)) ** 0.5\n\ndef save_statistics(stats, output_path):\n    \"\"\"Save statistics dictionary to a text file.\"\"\"\n    with open(output_path, 'w') as f:\n        for key, value in stats.items():\n            f.write(f'{key}: {value}\\n')\nThe refactored version is more verbose, but each function is now:\n\nTestable: You can verify each step independently\nReusable: Functions can be used in other analyses\nReadable: Each function has a clear, single purpose\nMaintainable: Changes to one step don‚Äôt affect others\n\n\n\n4.1.4 Managing Complexity with Abstraction\nAs your analysis grows more sophisticated, you will build up layers of abstraction. Lower-level functions handle details; higher-level functions orchestrate workflow:\n# Low-level: handle specific calculations\ndef calculate_mean(values):\n    \"\"\"Calculate arithmetic mean.\"\"\"\n    return sum(values) / len(values)\n\ndef calculate_variance(values):\n    \"\"\"Calculate population variance.\"\"\"\n    mean = calculate_mean(values)\n    squared_diffs = [(x - mean) ** 2 for x in values]\n    return sum(squared_diffs) / len(values)\n\ndef calculate_covariance(x_values, y_values):\n    \"\"\"Calculate covariance between two lists of values.\"\"\"\n    if len(x_values) != len(y_values):\n        raise ValueError(\"Lists must have same length\")\n    x_mean = calculate_mean(x_values)\n    y_mean = calculate_mean(y_values)\n    products = [(x - x_mean) * (y - y_mean) for x, y in zip(x_values, y_values)]\n    return sum(products) / len(x_values)\n\n# Mid-level: combine calculations\ndef calculate_descriptive_stats(values):\n    \"\"\"Calculate common descriptive statistics.\"\"\"\n    mean = calculate_mean(values)\n    variance = calculate_variance(values)\n    std = variance ** 0.5\n    return {'mean': mean, 'variance': variance, 'std': std}\n\n# High-level: orchestrate entire analysis\ndef analyze_dataset(filepath, column=1):\n    \"\"\"\n    Perform comprehensive analysis of a data file.\n\n    Reads data, calculates descriptive statistics, and\n    returns a complete summary.\n    \"\"\"\n    lines = read_data_file(filepath)\n    values = parse_values(lines, column)\n    stats = calculate_descriptive_stats(values)\n    stats['n'] = len(values)\n    stats['min'] = min(values)\n    stats['max'] = max(values)\n    return stats\n\n\n4.1.5 Code Layout and Readability\nPython‚Äôs readability comes partly from its use of whitespace. Use it deliberately:\n# Cramped and hard to parse\ndef process_records(data,filters=None,transform=True):\n    if filters is not None:data=[x for x in data if all(f(x) for f in filters)]\n    if transform:data=[{'id':x['id'],'value':x['amount']*100} for x in data]\n    return data\n\n# Readable with proper spacing\ndef process_records(data, filters=None, transform=True):\n    \"\"\"Process records with optional filtering and transformation.\"\"\"\n    if filters is not None:\n        data = [x for x in data if all(f(x) for f in filters)]\n\n    if transform:\n        data = [\n            {'id': x['id'], 'value': x['amount'] * 100}\n            for x in data\n        ]\n\n    return data\nGuidelines for spacing:\n\nUse blank lines to separate logical sections\nAdd spaces around operators (=, +, ==, etc.)\nAvoid spaces immediately inside parentheses or brackets\nGroup related items visually\n\nWe discuss how this can be automated in Section 4.4.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Code Quality and Documentation</span>"
    ]
  },
  {
    "objectID": "python/code-quality/index.html#docstrings-and-documentation-standards",
    "href": "python/code-quality/index.html#docstrings-and-documentation-standards",
    "title": "4¬† Code Quality and Documentation",
    "section": "4.2 Docstrings and Documentation Standards",
    "text": "4.2 Docstrings and Documentation Standards\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video covers code documentation. Note that this is an external resource that may present concepts differently than those covered here.\n\n\n\nDocumentation bridges the gap between what your code does and what users (including future you) need to know to use it correctly. In Python, this documentation primarily takes the form of docstrings‚Äîstring literals that appear as the first statement in a module, class, or function.\n\n4.2.1 Why Docstrings Matter\nUnlike comments, which explain how code works, docstrings explain what code does and how to use it. They serve multiple purposes:\n\nIDE integration: Modern editors display docstrings as tooltips and in autocomplete\nGenerated documentation: Tools like Sphinx can extract docstrings to create HTML documentation\nInteractive help: The help() function displays docstrings in the Python REPL\nCode review: Reviewers can understand intent without reading implementation\nAI assistance: AI coding assistants use docstrings to understand your code and provide better suggestions\n\nConsider the difference:\n# Without docstring\ndef clip_values(values, lower, upper):\n    return [max(lower, min(upper, x)) for x in values]\n\n# With docstring\ndef clip_values(values, lower, upper):\n    \"\"\"\n    Limit values to fall within specified bounds.\n\n    Parameters\n    ----------\n    values : list of float\n        Data values to clip\n    lower : float\n        Lower bound (values below this are set to lower)\n    upper : float\n        Upper bound (values above this are set to upper)\n\n    Returns\n    -------\n    list of float\n        Clipped values with the same length as input\n\n    Examples\n    --------\n    &gt;&gt;&gt; clip_values([1, 5, 10, 15], lower=3, upper=12)\n    [3, 5, 10, 12]\n\n    Notes\n    -----\n    This function is useful for reducing the impact of outliers while\n    retaining more information than simple outlier removal.\n    \"\"\"\n    return [max(lower, min(upper, x)) for x in values]\nWhen you call help(clip_values) or hover over the function in VS Code, you will see the formatted docstring with all the information needed to use the function correctly.\nThere are multiple standard docstring styles, but the most relevant for research projects are the NumPy and Google styles. Both provide clear structure for documenting parameters, return values, and examples. AI assistants can help generate well-formatted docstrings, but you should always review the descriptions to ensure they accurately reflect what your code does.\n\n\n4.2.2 NumPy Documentation Style\nThe NumPy documentation style is the standard in the scientific Python community. It is more verbose than some alternatives, but its structured format makes it ideal for technical and scientific code.\nThe basic structure includes:\n\nOne-line summary: Brief description of what the function does\nExtended description (optional): Additional context and details\nParameters: Each parameter with its type and description\nReturns: What the function returns and its type\nExamples (optional): Usage examples with expected output\nNotes (optional): Additional information, warnings, or references\n\nHere‚Äôs a complete example:\ndef calculate_rolling_mean(values, window, min_periods=None):\n    \"\"\"\n    Calculate rolling mean over a sliding window.\n\n    Computes the arithmetic mean over a rolling window of specified size.\n    The function handles edge cases at the beginning of the series.\n\n    Parameters\n    ----------\n    values : list of float\n        Sequence of numeric values\n    window : int\n        Number of values to include in each rolling window\n    min_periods : int, optional\n        Minimum number of observations required to calculate mean.\n        If None, defaults to window size. Windows with fewer observations\n        will return None.\n\n    Returns\n    -------\n    list of float or None\n        Rolling mean values. Returns None for positions where there\n        are fewer than min_periods observations.\n\n    Raises\n    ------\n    ValueError\n        If window is less than 1\n    ValueError\n        If min_periods is greater than window\n\n    Examples\n    --------\n    &gt;&gt;&gt; values = [1.0, 2.0, 3.0, 4.0, 5.0]\n    &gt;&gt;&gt; calculate_rolling_mean(values, window=3)\n    [None, None, 2.0, 3.0, 4.0]\n    &gt;&gt;&gt; calculate_rolling_mean(values, window=3, min_periods=1)\n    [1.0, 1.5, 2.0, 3.0, 4.0]\n\n    Notes\n    -----\n    The rolling mean at position i is calculated as the average of values\n    from position (i - window + 1) to i, inclusive.\n\n    See Also\n    --------\n    calculate_mean : Calculate mean of entire sequence\n    calculate_rolling_std : Calculate rolling standard deviation\n    \"\"\"\n    if window &lt; 1:\n        raise ValueError(\"window must be at least 1\")\n\n    if min_periods is None:\n        min_periods = window\n\n    if min_periods &gt; window:\n        raise ValueError(\"min_periods cannot exceed window\")\n\n    result = []\n    for i in range(len(values)):\n        start = max(0, i - window + 1)\n        window_values = values[start:i + 1]\n        if len(window_values) &gt;= min_periods:\n            result.append(sum(window_values) / len(window_values))\n        else:\n            result.append(None)\n\n    return result\n\n\n4.2.3 Google Documentation Style\nGoogle style is an alternative that is more compact while still providing structure. It uses indented sections rather than underlined headers:\ndef normalize_values(values, method='zscore'):\n    \"\"\"Normalize a list of values using the specified method.\n\n    Transforms values to have comparable scales, which is useful for\n    combining variables measured in different units.\n\n    Args:\n        values (list of float): Numeric values to normalize\n        method (str): Normalization method. Use 'zscore' for zero mean\n            and unit variance, 'minmax' for scaling to [0, 1] range.\n\n    Returns:\n        list of float: Normalized values\n\n    Raises:\n        ValueError: If values has zero standard deviation (for zscore)\n        ValueError: If all values are equal (for minmax)\n        ValueError: If method is not recognized\n\n    Example:\n        &gt;&gt;&gt; data = [10, 20, 30, 40, 50]\n        &gt;&gt;&gt; normalize_values(data, method='minmax')\n        [0.0, 0.25, 0.5, 0.75, 1.0]\n    \"\"\"\n    if method == 'zscore':\n        mean = sum(values) / len(values)\n        squared_diffs = [(x - mean) ** 2 for x in values]\n        std = (sum(squared_diffs) / len(values)) ** 0.5\n        if std == 0:\n            raise ValueError(\"Cannot zscore normalize: zero standard deviation\")\n        return [(x - mean) / std for x in values]\n\n    elif method == 'minmax':\n        min_val, max_val = min(values), max(values)\n        if min_val == max_val:\n            raise ValueError(\"Cannot minmax normalize: all values are equal\")\n        return [(x - min_val) / (max_val - min_val) for x in values]\n\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\nFor this course, we recommend using NumPy style for longer, more complex functions and Google style for simpler utilities. The key is to be consistent within a project. The docstring standards also define conventions for module-level and class-level docstrings, which follow similar patterns.\n\n\n\n\n\n\nTipDocumentation and Research Transparency\n\n\n\nIn empirical research, good documentation is not just helpful‚Äîit is essential for reproducibility. When writing up your analysis, you should be able to point reviewers to specific, well-documented functions that implement your methodology. This makes peer review more effective and helps establish trust in your results.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Code Quality and Documentation</span>"
    ]
  },
  {
    "objectID": "python/code-quality/index.html#type-hints-and-static-typing",
    "href": "python/code-quality/index.html#type-hints-and-static-typing",
    "title": "4¬† Code Quality and Documentation",
    "section": "4.3 Type Hints and Static Typing",
    "text": "4.3 Type Hints and Static Typing\nPython is a dynamically typed language, meaning you do not need to declare variable types. However, Python 3.5+ supports optional type hints that document expected types without changing runtime behavior. Type hints improve code quality by:\n\nMaking function interfaces explicit and self-documenting\nEnabling static analysis tools to catch type errors before runtime\nImproving IDE autocomplete and error detection\nServing as machine-checked documentation\nProviding additional context to AI coding assistants\n\n\n4.3.1 Basic Type Hints\nType hints specify the expected type of variables, parameters, and return values:\ndef calculate_return(initial_price: float, final_price: float) -&gt; float:\n    \"\"\"Calculate simple return between two prices.\"\"\"\n    return (final_price - initial_price) / initial_price\n\n\ndef read_config(filepath: str) -&gt; dict[str, str]:\n    \"\"\"Read configuration from a file.\"\"\"\n    config = {}\n    with open(filepath) as f:\n        for line in f:\n            key, value = line.strip().split('=')\n            config[key] = value\n    return config\nThe syntax parameter: type indicates the expected type, and -&gt; type indicates the return type.\n\n\n4.3.2 Common Type Hints\nHere are the type hints you will use most frequently:\ndef calculate_weighted_sum(\n    values: list[float],\n    weights: list[float]\n) -&gt; float:\n    \"\"\"Calculate weighted sum of values.\"\"\"\n    return sum(v * w for v, w in zip(values, weights))\n\n\ndef process_records(\n    records: list[dict[str, str]],\n    key: str = 'id'\n) -&gt; dict[str, dict[str, str]]:\n    \"\"\"Index records by a key field.\"\"\"\n    return {record[key]: record for record in records}\n\n\ndef calculate_statistics(\n    values: list[float]\n) -&gt; tuple[float, float, float]:\n    \"\"\"Calculate mean, min, and max.\"\"\"\n    mean = sum(values) / len(values)\n    return mean, min(values), max(values)\n\n\n\n\n\n\nNoteLegacy Type Hint Syntax\n\n\n\nWhen type hints were first introduced in Python 3.5, you had to import special types from the typing module like List, Dict, Tuple, and Union. Starting with Python 3.9+, you can use the built-in types directly: list[str] instead of List[str], dict[str, int] instead of Dict[str, int], and int | float instead of Union[int, float]. You will still see the old style in many code examples and libraries, but for new code, prefer the modern syntax.\n\n\n\n\n4.3.3 Optional and Union Types\nUse | None when a parameter might be None:\ndef calculate_mean(values: list[float], default: float | None = None) -&gt; float:\n    \"\"\"\n    Calculate mean of values, with optional default for empty lists.\n\n    Parameters\n    ----------\n    values : list of float\n        Values to average\n    default : float, optional\n        Value to return if list is empty. If None, raises ValueError.\n    \"\"\"\n    if not values:\n        if default is None:\n            raise ValueError(\"Cannot calculate mean of empty list\")\n        return default\n    return sum(values) / len(values)\nUse | (pipe) when a parameter can be one of several types:\ndef format_value(value: int | float | str) -&gt; str:\n    \"\"\"Format a value as a string with appropriate formatting.\"\"\"\n    if isinstance(value, float):\n        return f\"{value:.2f}\"\n    return str(value)\n\n\n4.3.4 Type Checking with ty\nType hints become even more valuable when combined with static type checkers. We recommend ty, a fast type checker from Astral (the same company behind uv and ruff). Run it with:\nuvx ty check your_script.py\nThe type checker will detect type inconsistencies:\ndef calculate_return(initial_price: float, final_price: float) -&gt; float:\n    return (final_price - initial_price) / initial_price\n\n# This will trigger a type error\nresult = calculate_return(\"100\", \"110\")  # Error: expected float, got str\nType hints do not affect runtime behavior; Python will not enforce types unless you use a type checker. This means type hints are documentation and analysis tools, not runtime constraints. You can gradually add type hints to your codebase without breaking existing code.\nFor practical use, VS Code can highlight type errors the same way Word highlights typos, which is very useful to catch bugs early on.\nFor research code, focus type hints where they add the most value: public function interfaces that others will use, complex data transformations where types clarify expected structures, and critical calculations where you want to make assumptions explicit. You do not need to type hint every variable in every function‚Äîuse judgment about where type information improves clarity.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Code Quality and Documentation</span>"
    ]
  },
  {
    "objectID": "python/code-quality/index.html#sec-ruff",
    "href": "python/code-quality/index.html#sec-ruff",
    "title": "4¬† Code Quality and Documentation",
    "section": "4.4 Code Style and Linting with ruff",
    "text": "4.4 Code Style and Linting with ruff\nConsistent code style makes collaboration easier and reduces cognitive load when reading code. Rather than debating style choices, the Python community has converged on automated tools that enforce consistent formatting. This section introduces ruff, the modern standard for Python code quality. Ruff is made by Astral, the same company behind uv and ty.\n\n4.4.1 Why Automated Formatting Matters\nManual formatting is time-consuming and leads to inconsistency. Different developers have different preferences for spacing, line breaks, and indentation. These differences create noise in version control, make code reviews harder, and waste mental energy on decisions that don‚Äôt affect functionality.\nAutomated formatters solve this by making formatting decisions for you. While you might not agree with every choice, the consistency and time savings far outweigh any aesthetic preferences. Additionally, when you get used to a specific style, it increases readability‚Äîyour eyes learn to scan consistently formatted code more quickly.\n\n\n4.4.2 ruff: The All-in-One Linter\nRuff is an extremely fast Python linter and code formatter written in Rust. It replaces multiple tools (flake8, isort, pyupgrade, and more) with a single, consistent interface. Ruff can:\n\nCheck for common errors and bugs\nEnforce code style guidelines\nSort and organize imports\nSuggest modernizations and improvements\nAutomatically fix many issues\n\nThe easiest way to use ruff is through the VS Code extension. Install the ‚ÄúRuff‚Äù extension from the VS Code marketplace, and VS Code can be configured to automatically format and fix your code each time you save. This makes code quality effortless‚Äîjust write your code and save.\nYou can also run ruff from the command line. Check your code with:\nuvx ruff check your_script.py\nRuff will identify issues:\n# example.py\nimport json\nimport os  # Unused import\n\ndef load_data(filepath):\n    with open(filepath) as f:\n        data = json.load(f)\n    return data\n\n# Unused variable\nresult = load_data(\"data.json\")\nRunning uvx ruff check example.py produces:\nexample.py:2:8: F401 [*] `os` imported but unused\nexample.py:10:1: F841 [*] Local variable `result` is assigned to but never used\nFound 2 errors.\n[*] 2 potentially fixable with the `--fix` option.\nAuto-fix issues:\nuvx ruff check --fix example.py\nRuff will automatically remove the unused import and variable.\n\n\n4.4.3 Configuring ruff\nConfigure ruff using a pyproject.toml file in your project root:\n[tool.ruff]\n# Set maximum line length (default is 88)\nline-length = 88\n\n# Target Python version\ntarget-version = \"py311\"\n\n[tool.ruff.lint]\n# Enable specific rule sets\nselect = [\n    \"E\",    # pycodestyle errors\n    \"F\",    # Pyflakes\n    \"I\",    # isort (import sorting)\n    \"B\",    # flake8-bugbear (common bugs)\n    \"SIM\",  # flake8-simplify\n    \"UP\",   # pyupgrade (modernize syntax)\n]\n\n# Disable specific rules if needed\nignore = [\n    \"E501\",  # Line too long (handled by formatter)\n]\n\n# Allow auto-fixing for these rule types\nfixable = [\"ALL\"]\n\n[tool.ruff.lint.per-file-ignores]\n# Allow unused imports in __init__.py files\n\"__init__.py\" = [\"F401\"]\n\n# Relaxed rules for test files\n\"tests/**/*.py\" = [\"S101\"]  # Allow assert statements\nThis configuration enables helpful checks while avoiding overly strict rules that might interfere with research workflows.\n\n\n4.4.4 Common ruff Rules\nSome particularly useful ruff rules:\nImport Organization (I)\nThis rule organizes imports in alphabetical order and grouping them in three groups: standard library imports, third-party imports, and imports from the current project. It will also automatically remove imports that are not used in the code.\nBefore ruff:\nimport json\nimport pandas as pd\nimport sys\nimport os\nfrom myproject.utils import helper\nimport csv\nAfter ruff with isort rules:\nimport csv\nimport json\nimport os\nimport sys\n\nimport pandas as pd\n\nfrom myproject.utils import helper\nBug Detection (B)\nRuff catches bugs such as the mutable default argument bug. When you use a mutable object like a list as a default argument, Python creates that object once when the function is defined‚Äînot each time the function is called. This means all calls share the same list, causing unexpected behavior where the list grows across calls.\nBefore (buggy):\ndef collect_values(value, results=[]):  # B006: Mutable default argument\n    results.append(value)\n    return results\nAfter (fixed):\ndef collect_values(value, results=None):\n    if results is None:\n        results = []\n    results.append(value)\n    return results\nCode Simplification (SIM)\nBefore:\nif condition:\n    return True\nelse:\n    return False\nAfter:\nreturn condition\n\n\n4.4.5 Formatting Code with ruff\nIn addition to linting, ruff includes a powerful code formatter. Ruff‚Äôs formatter is opinionated‚Äîit makes formatting decisions for you, eliminating debates about style. The philosophy is simple: let the tool handle formatting so you can focus on the code itself.\nFormat your code:\nuvx ruff format your_script.py\nBefore formatting:\ndef calculate_mean(values,skip_none=True):\n    if skip_none:values=[v for v in values if v is not None]\n    return sum(values)/len(values)\nAfter formatting:\ndef calculate_mean(values, skip_none=True):\n    if skip_none:\n        values = [v for v in values if v is not None]\n    return sum(values) / len(values)\nYou can use both linting and formatting together:\nuvx ruff format your_script.py && uvx ruff check --fix your_script.py\nThis gives you a single, fast tool to automatically improve your code quality.\n\n\n\n\n\n\nTipCode Quality in Jupyter Notebooks\n\n\n\nRuff can also format Jupyter notebooks:\nuvx ruff format analysis.ipynb\nuvx ruff check analysis.ipynb\nConfigure per-cell ignores for exploratory code while maintaining standards for final analysis code.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Code Quality and Documentation</span>"
    ]
  },
  {
    "objectID": "python/code-quality/index.html#summary-and-practical-guidelines",
    "href": "python/code-quality/index.html#summary-and-practical-guidelines",
    "title": "4¬† Code Quality and Documentation",
    "section": "4.5 Summary and Practical Guidelines",
    "text": "4.5 Summary and Practical Guidelines\nCode quality is not about perfectionism‚Äîit is about making your research more reliable, your collaboration more effective, and your future work easier. The practices covered in this chapter form the foundation of professional Python development:\n\nOrganization and readability: Structure code to reveal intent clearly\nDocumentation: Write docstrings that explain what code does and how to use it\nType hints: Make data types explicit to catch errors early\nAutomated formatting: Use ruff to maintain consistent style\n\nFor research projects, we recommend:\n\nStart simple: Begin with basic ruff configuration, add rules gradually\nFormat early: Run ruff format regularly, not just before commits\nFix what matters: Use ruff check --fix to auto-fix safe issues\nTeam consistency: Ensure all collaborators use the same tools and configuration\n\nThese practices require some initial investment, but they pay dividends throughout your research career. Code that is well-organized, well-documented, and consistently formatted is easier to debug, easier to extend, and easier to share with collaborators and reviewers.\nAs you develop your empirical finance projects, make these practices habitual. Configure your tools once, integrate them into your workflow, and let automation handle the details. This frees you to focus on what matters: designing sound research, implementing correct methodology, and drawing valid conclusions from your data.\nThe practices in this chapter work best alongside testing, which we will cover in the next chapter. While code quality ensures your code is readable and well-documented, testing ensures it is correct. Together, they form the foundation of reliable research software.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Code Quality and Documentation</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html",
    "href": "python/testing/index.html",
    "title": "5¬† Error Handling and Testing",
    "section": "",
    "text": "5.1 Exceptions and Error Handling\nIn empirical research, the quality and reliability of your code directly impacts the quality of your results. A small bug in your data processing pipeline or statistical calculation can invalidate months of work. Yet many researchers write code without systematic error handling or testing. The result? Papers retracted due to coding errors, results that can‚Äôt be replicated, and countless hours spent debugging problems that could have been caught early.\nThis chapter introduces two complementary practices that will make your research code more robust: error handling and testing. Error handling is about writing code that fails gracefully and provides useful information when something goes wrong. Testing is about systematically verifying that your code does what you think it does. Together, these practices form the foundation of reliable, reproducible research.\nThink of error handling as defensive driving for your code. You anticipate what might go wrong and plan for it. Testing, on the other hand, is like having a checklist before takeoff. You verify that everything works as expected before committing to your results. Both practices require a small upfront investment that pays enormous dividends in time saved and confidence gained.\nWhen something goes wrong in a Python program, the interpreter raises an exception. An exception is Python‚Äôs way of signaling that an error has occurred. If you‚Äôve written any Python code, you‚Äôve likely encountered exceptions: TypeError, ValueError, KeyError, FileNotFoundError, and so on. By default, an unhandled exception stops your program and prints a traceback showing where the error occurred.\nWhile this default behavior is useful during development, it‚Äôs often not what you want in production code or long-running research scripts. What if a file is missing, but you can use a default dataset instead? What if one stock in your analysis has corrupted data, but you want to continue processing the others? What if a network request fails, but you can retry it? This is where error handling comes in.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html#exceptions-and-error-handling",
    "href": "python/testing/index.html#exceptions-and-error-handling",
    "title": "5¬† Error Handling and Testing",
    "section": "",
    "text": "5.1.1 Understanding Exceptions\nLet‚Äôs start with a simple example. Suppose you‚Äôre calculating portfolio returns and need to divide returns by portfolio values:\ndef calculate_return_percentage(return_dollars, portfolio_value):\n    return (return_dollars / portfolio_value) * 100\n\n# This works fine\nprint(calculate_return_percentage(1000, 50000))  # 2.0\n\n# But this crashes\nprint(calculate_return_percentage(1000, 0))  # ZeroDivisionError!\nWhen you try to divide by zero, Python raises a ZeroDivisionError. The program stops, and you see a traceback. While this is informative, it‚Äôs not helpful if you‚Äôre processing thousands of portfolios and one happens to have zero value.\n\n\n5.1.2 The try-except Block\nPython‚Äôs try-except block allows you to handle exceptions gracefully. The basic structure is:\ntry:\n1    result = risky_operation()\n2except SomeException:\n3    result = default_value\n\n1\n\nCode that might raise an exception goes in the try block.\n\n2\n\nSpecify which exception type(s) to catch.\n\n3\n\nHandle the error and provide a fallback.\n\n\nLet‚Äôs apply this to our portfolio example:\ndef calculate_return_percentage(return_dollars, portfolio_value):\n    try:\n        return (return_dollars / portfolio_value) * 100\n    except ZeroDivisionError:\n        # Portfolio has zero value, return None or a special value\n        return None\n\n# Now this doesn't crash\nprint(calculate_return_percentage(1000, 50000))  # 2.0\nprint(calculate_return_percentage(1000, 0))      # None\nThis is better, but we‚Äôve lost information. We know the calculation failed, but we don‚Äôt know which portfolio or why. In research code, you almost always want to preserve this information:\ndef calculate_return_percentage(return_dollars, portfolio_value):\n    try:\n        return (return_dollars / portfolio_value) * 100\n    except ZeroDivisionError:\n        print(f\"Warning: Cannot calculate return for portfolio with zero value\")\n        return None\nWhile print() statements are commonly used to log errors and warnings, there are better ways to handle logging in production code. We introduce proper logging techniques in Chapter 6.\n\n\n\n\n\n\nTipWhen to Catch Exceptions\n\n\n\nA common mistake is catching exceptions too broadly or too often. Don‚Äôt catch exceptions just because you can. Catch them when you have a specific, sensible way to handle the error. If you can‚Äôt do anything useful with the exception, it‚Äôs often better to let it propagate and fail fast rather than hiding the problem.\n\n\n\n\n5.1.3 Catching Multiple Exceptions\nOften, several different things can go wrong, and you want to handle them differently:\ndef load_stock_data(filename):\n    try:\n        with open(filename, 'r') as f:\n            lines = f.readlines()\n        if len(lines) == 0:\n            raise ValueError(\"File is empty\")\n        # Parse header and validate columns\n        header = lines[0].strip().split(',')\n        required_columns = ['date', 'close', 'volume']\n        missing = set(required_columns) - set(header)\n        if missing:\n            raise ValueError(f\"Missing required columns: {missing}\")\n        return lines\n    except FileNotFoundError:\n        print(f\"Error: File '{filename}' not found\")\n        return None\n    except ValueError as e:\n        print(f\"Error: Invalid data format - {e}\")\n        return None\nYou can also catch multiple exceptions in a single except block if you want to handle them the same way:\ndef load_data(filename):\n    try:\n        with open(filename, 'r') as f:\n            return f.read()\n    except (FileNotFoundError, PermissionError) as e:\n        print(f\"Error loading '{filename}': {e}\")\n        return None\n\n\n5.1.4 The else and finally Clauses\nThe try-except block can include two additional clauses: else and finally.\nThe else clause runs if no exception was raised:\ndef process_file(filename):\n    try:\n        with open(filename, 'r') as f:\n            lines = f.readlines()\n    except FileNotFoundError:\n        print(f\"File not found: {filename}\")\n        return None\n    else:\n        # This runs only if no exception occurred\n        print(f\"Successfully loaded {len(lines)} lines\")\n        return lines\nThe finally clause always runs, whether an exception occurred or not. This is useful for cleanup:\ndef analyze_large_dataset(filename):\n    file_handle = None\n    try:\n        file_handle = open(filename, 'r')\n        data = process(file_handle)\n        return data\n1    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return None\n2    finally:\n        if file_handle:\n            file_handle.close()\n\n1\n\nThe except block handles errors.\n\n2\n\nThe finally block always runs, ensuring the file is closed even if an error occurs or the function returns early.\n\n\n\n\n\n\n\n\nNoteContext Managers vs.¬†finally\n\n\n\nFor file handling and similar resources, Python‚Äôs context managers (the with statement) are usually cleaner than finally:\ndef analyze_large_dataset(filename):\n    try:\n        with open(filename, 'r') as file_handle:\n            data = process(file_handle)\n            return data\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return None\nThe context manager automatically closes the file, even if an exception occurs.\n\n\n\n\n5.1.5 Raising Exceptions\nSometimes you need to signal an error in your own code. Use the raise statement:\ndef calculate_sharpe_ratio(returns, risk_free_rate):\n    \"\"\"Calculate Sharpe ratio.\n\n    Parameters\n    ----------\n    returns : array-like\n        Series of returns\n    risk_free_rate : float\n        Risk-free rate\n\n    Raises\n    ------\n    ValueError\n        If returns is empty or risk_free_rate is negative\n    \"\"\"\n    if len(returns) == 0:\n        raise ValueError(\"Returns array cannot be empty\")\n\n    if risk_free_rate &lt; 0:\n        raise ValueError(\"Risk-free rate cannot be negative\")\n\n    excess_returns = returns - risk_free_rate\n    return excess_returns.mean() / excess_returns.std()\nThis is much better than returning a special value like -999 or None and hoping the caller checks for it. An exception forces the caller to explicitly handle the error.\n\n\n5.1.6 Creating Custom Exceptions\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video covers similar topics to this section.\n\n\n\nFor complex projects, you might want to define your own exception types. In Python, all exceptions are classes that inherit from the built-in Exception class (see Chapter 3 for more on classes and inheritance). This makes it easier to catch specific errors:\nclass DataQualityError(Exception):\n    \"\"\"Raised when data fails quality checks.\"\"\"\n    pass\n\nclass InsufficientDataError(Exception):\n    \"\"\"Raised when there's not enough data for analysis.\"\"\"\n    pass\n\ndef calculate_rolling_beta(stock_returns, market_returns, window=60):\n    \"\"\"Calculate rolling beta with data quality checks.\"\"\"\n    if len(stock_returns) &lt; window:\n        raise InsufficientDataError(\n            f\"Need at least {window} observations, got {len(stock_returns)}\"\n        )\n\n    # Check for too many missing values\n    missing_pct = stock_returns.isna().sum() / len(stock_returns)\n    if missing_pct &gt; 0.1:\n        raise DataQualityError(\n            f\"Too many missing values: {missing_pct:.1%}\"\n        )\n\n    # Calculate beta...\nNow calling code can handle different errors appropriately:\ntry:\n    beta = calculate_rolling_beta(stock_returns, market_returns)\nexcept InsufficientDataError as e:\n    print(f\"Skipping stock: {e}\")\n    beta = None\nexcept DataQualityError as e:\n    print(f\"Data quality issue: {e}\")\n    beta = None\n\n\n\n\n\n\nWarningDon‚Äôt Swallow Exceptions\n\n\n\nA common antipattern is the bare except: clause that catches everything:\n# BAD: This hides all errors, including bugs in your code\ntry:\n    result = complex_calculation()\nexcept:\n    result = None\nThis will catch not just the errors you expect, but also bugs in your code, keyboard interrupts, and system errors. Always catch specific exceptions, or at least use except Exception: which won‚Äôt catch system-exiting exceptions.\n\n\n\n\n5.1.7 Error Handling in Data Pipelines\nIn empirical research, you often process many items (stocks, firms, countries) where some might fail. Here‚Äôs a pattern for handling this gracefully:\ndef process_stock(ticker, start_date, end_date):\n    \"\"\"Process a single stock, raising exceptions on failure.\"\"\"\n    # This function doesn't handle exceptions - it lets them propagate\n    data = download_data(ticker, start_date, end_date)\n    returns = calculate_returns(data)\n    return calculate_statistics(returns)\n\ndef process_all_stocks(tickers, start_date, end_date):\n    \"\"\"Process multiple stocks, collecting both successes and failures.\"\"\"\n    results = {}\n    errors = {}\n\n    for ticker in tickers:\n        try:\n            results[ticker] = process_stock(ticker, start_date, end_date)\n        except Exception as e:\n            # Log the error but continue processing\n            errors[ticker] = str(e)\n            print(f\"Error processing {ticker}: {e}\")\n\n    print(f\"\\nProcessed {len(results)} stocks successfully\")\n    print(f\"Failed to process {len(errors)} stocks\")\n\n    return results, errors\n\n# Usage\ntickers = ['AAPL', 'MSFT', 'INVALID_TICKER', 'GOOGL']\nresults, errors = process_all_stocks(tickers, '2020-01-01', '2023-12-31')\nThis pattern separates the logic (in process_stock) from the error handling (in process_all_stocks). The individual function can be tested in isolation, while the batch function handles partial failures gracefully.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html#unit-testing-with-pytest",
    "href": "python/testing/index.html#unit-testing-with-pytest",
    "title": "5¬† Error Handling and Testing",
    "section": "5.2 Unit Testing with pytest",
    "text": "5.2 Unit Testing with pytest\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video covers similar topics to this section.\n\n\n\nError handling helps your code fail gracefully when things go wrong. Testing helps ensure things don‚Äôt go wrong in the first place. A test is simply code that verifies other code works correctly. You write a test that calls your function with known inputs and checks that it produces the expected output.\n\n5.2.1 Why Test?\nYou might think: ‚ÄúI‚Äôll just run my code and check the results. Why write separate tests?‚Äù Here‚Äôs why testing matters:\n\nConfidence in changes: When you modify code, tests verify you didn‚Äôt break anything.\nDocumentation: Tests show how your code is meant to be used.\nBetter design: Code that‚Äôs easy to test is usually better designed.\nCatch bugs early: Tests find problems before they affect your results.\nReproducibility: Tests verify your code produces consistent results.\nValidation of AI-generated code: Tests provide an additional layer of verification when using AI coding assistants.\n\nIn research, there‚Äôs an additional benefit: tests help you understand your methods. Writing tests forces you to think clearly about what your code should do, edge cases, and assumptions. This deeper understanding often reveals problems in your research design.\n\n\n\n\n\n\nTipAI Coding Assistants and Testing\n\n\n\nAI coding assistants are particularly good at writing tests, making it easier to build a comprehensive test suite. However, always review AI-generated tests carefully. AI may miss important edge cases or make incorrect assumptions about expected behavior. Use AI-generated tests as a starting point, then add your own tests for edge cases and domain-specific scenarios that the AI might overlook.\n\n\n\n\n5.2.2 Getting Started with pytest\npytest is Python‚Äôs most popular testing framework. It‚Äôs simple to use but powerful enough for complex projects. The easiest way to run pytest is using uvx, which runs the tool without requiring explicit installation:\nuvx pytest test_math.py\nIf you‚Äôre working on a project and want pytest available as a development dependency, add it to your project‚Äôs dev group:\nuv add --dev pytest\nThen run it with:\nuv run pytest\nA pytest test is just a function whose name starts with test_. Here‚Äôs the simplest possible test:\ndef test_simple():\n    assert 1 + 1 == 2\nSave this in a file called test_math.py and run:\nuvx pytest test_math.py\nYou‚Äôll see output indicating the test passed. The assert statement is the heart of testing. If the expression after assert is True, the test passes. If it‚Äôs False, the test fails.\n\n\n5.2.3 Testing a Real Function\nLet‚Äôs test a function that calculates simple returns:\n# finance_utils.py\ndef calculate_simple_returns(prices):\n    \"\"\"Calculate simple returns from a price series.\n\n    Parameters\n    ----------\n    prices : list\n        Series of prices\n\n    Returns\n    -------\n    list\n        Simple returns (length is len(prices) - 1)\n    \"\"\"\n    returns = []\n    for i in range(1, len(prices)):\n        ret = (prices[i] - prices[i-1]) / prices[i-1]\n        returns.append(ret)\n    return returns\nNow write tests:\n# test_finance_utils.py\nfrom finance_utils import calculate_simple_returns\n\ndef test_simple_returns_basic():\n    \"\"\"Test simple returns calculation with known values.\"\"\"\n    prices = [100, 110, 121]\n    returns = calculate_simple_returns(prices)\n\n    # Expected: (110-100)/100 = 0.10, (121-110)/110 = 0.10\n    assert abs(returns[0] - 0.10) &lt; 1e-10\n    assert abs(returns[1] - 0.10) &lt; 1e-10\n\ndef test_simple_returns_length():\n    \"\"\"Test that output length is correct.\"\"\"\n    prices = [100, 110, 121, 133.1]\n    returns = calculate_simple_returns(prices)\n    assert len(returns) == len(prices) - 1\n\ndef test_simple_returns_constant_prices():\n    \"\"\"Test with constant prices (zero returns).\"\"\"\n    prices = [100, 100, 100]\n    returns = calculate_simple_returns(prices)\n    assert returns == [0, 0]\nRun the tests:\nuvx pytest test_finance_utils.py\nEach test function checks a different aspect of the behavior. This is much more thorough than running the function once and eyeballing the output.\n\n\n\n\n\n\nTipTest One Thing Per Test\n\n\n\nEach test should verify one specific behavior. This makes failures easier to diagnose. When a test fails, you want to immediately know what‚Äôs wrong, not spend time figuring out which of five assertions in the test failed.\n\n\n\n\n5.2.4 Understanding Test Output\nWhen a test fails, pytest provides detailed information:\ndef test_log_returns_incorrect():\n    \"\"\"This test will fail to demonstrate pytest output.\"\"\"\n    prices = [100, 110]\n    returns = calculate_log_returns(prices)\n    assert returns[0] == 0.1  # This is wrong - log(1.1) ‚âà 0.0953\nRunning this produces:\ntest_finance_utils.py::test_log_returns_incorrect FAILED\n\n================================== FAILURES ===================================\n________________________ test_log_returns_incorrect __________________________\n\n    def test_log_returns_incorrect():\n        prices = [100, 110]\n        returns = calculate_log_returns(prices)\n&gt;       assert returns[0] == 0.1\nE       assert 0.09531017980432493 == 0.1\n\ntest_finance_utils.py:8: AssertionError\nThe output shows exactly which assertion failed and what the actual value was. This makes debugging straightforward.\n\n\n5.2.5 Testing with Fixtures\nOften, you need the same data for multiple tests. pytest fixtures let you set up reusable test data:\nimport pytest\n\n@pytest.fixture\ndef sample_prices():\n    \"\"\"Create sample price data for testing.\"\"\"\n    return [100, 105, 103, 108, 112, 110, 115]\n\ndef test_returns_are_correct(sample_prices):\n    \"\"\"Test returns calculation using fixture.\"\"\"\n    returns = calculate_simple_returns(sample_prices)\n    # First return: (105-100)/100 = 0.05\n    assert abs(returns[0] - 0.05) &lt; 1e-10\n\ndef test_data_has_correct_length(sample_prices):\n    \"\"\"Test using the same fixture.\"\"\"\n    assert len(sample_prices) == 7\nThe @pytest.fixture decorator marks a function as a fixture. When you include the fixture name as a test function parameter, pytest automatically calls the fixture and passes its return value to your test.\nFixtures can also handle setup and teardown:\nimport pytest\nimport tempfile\nimport os\n\n@pytest.fixture\ndef temp_data_file():\n    \"\"\"Create a temporary file with test data.\"\"\"\n1    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv') as f:\n        f.write('date,close\\n')\n        f.write('2020-01-01,100\\n')\n        f.write('2020-01-02,110\\n')\n        temp_path = f.name\n\n2    yield temp_path\n\n3    os.unlink(temp_path)\n\n4def test_load_data_from_file(temp_data_file):\n    \"\"\"Test loading data from a CSV file.\"\"\"\n    with open(temp_data_file) as f:\n        lines = f.readlines()\n    assert len(lines) == 3  # Header + 2 data rows\n\n1\n\nSetup: Create temporary file with test data.\n\n2\n\nYield: Provide the file path to the test.\n\n3\n\nTeardown: Clean up the file after the test completes.\n\n4\n\nThe fixture name as a parameter tells pytest to inject the fixture‚Äôs return value.\n\n\nThe yield statement separates setup from teardown. Everything before yield runs before the test, and everything after runs after the test (even if the test fails).\n\n\n5.2.6 Parametrized Tests\nWhen you want to test the same function with multiple inputs, use parametrization:\nimport pytest\n\n@pytest.mark.parametrize(\"prices,expected_length\", [\n    ([100, 110], 1),\n    ([100, 110, 121], 2),\n    ([100, 110, 121, 133], 3),\n    ([100], 0),  # Edge case: single price\n])\ndef test_returns_length_parametrized(prices, expected_length):\n    \"\"\"Test that returns have correct length for various inputs.\"\"\"\n    returns = calculate_simple_returns(prices)\n    assert len(returns) == expected_length\nThis creates four separate tests, one for each parameter set. This is cleaner than writing four separate test functions and makes the pattern clear.\nYou can parametrize multiple arguments:\n@pytest.mark.parametrize(\"initial_price,final_price,expected_return\", [\n    (100, 110, 0.10),   # 10% price increase\n    (100, 90, -0.10),   # 10% price decrease\n    (100, 100, 0),      # No change\n    (50, 100, 1.0),     # 100% increase\n])\ndef test_simple_return_calculation(initial_price, final_price, expected_return):\n    \"\"\"Test simple return calculation with various price changes.\"\"\"\n    returns = calculate_simple_returns([initial_price, final_price])\n    assert abs(returns[0] - expected_return) &lt; 1e-10\n\n\n5.2.7 Testing for Exceptions\nSometimes you want to verify that your code raises an exception in certain situations:\ndef calculate_mean_return(returns):\n    \"\"\"Calculate mean return.\"\"\"\n    if len(returns) == 0:\n        raise ValueError(\"Returns list cannot be empty\")\n\n    return sum(returns) / len(returns)\n\ndef test_mean_return_empty_raises():\n    \"\"\"Test that empty returns raise ValueError.\"\"\"\n    with pytest.raises(ValueError, match=\"cannot be empty\"):\n        calculate_mean_return([])\n\ndef test_mean_return_with_data():\n    \"\"\"Test normal mean return calculation.\"\"\"\n    returns = [0.01, 0.02, -0.01, 0.03]\n    mean = calculate_mean_return(returns)\n    assert abs(mean - 0.0125) &lt; 1e-10\nThe pytest.raises() context manager asserts that the code block raises the specified exception. The match parameter checks that the exception message matches a pattern (using regular expressions).\n\n\n\n\n\n\nNoteTesting with NumPy and pandas\n\n\n\nTesting functions that work with NumPy arrays or pandas DataFrames sometimes requires special handling, such as using np.allclose() for floating-point array comparisons or pd.testing.assert_frame_equal() for DataFrame comparisons.\n\n\n\n\n5.2.8 Organizing Tests\nAs your project grows, organize tests to mirror your code structure:\nmy_research_project/\n‚îú‚îÄ‚îÄ finance_utils/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ returns.py\n‚îÇ   ‚îú‚îÄ‚îÄ risk.py\n‚îÇ   ‚îî‚îÄ‚îÄ portfolio.py\n‚îî‚îÄ‚îÄ tests/\n    ‚îú‚îÄ‚îÄ __init__.py\n    ‚îú‚îÄ‚îÄ test_returns.py\n    ‚îú‚îÄ‚îÄ test_risk.py\n    ‚îî‚îÄ‚îÄ test_portfolio.py\nRun all tests with:\nuvx pytest tests/\nOr if pytest is installed in your project:\nuv run pytest tests/\nRun specific tests:\nuvx pytest tests/test_returns.py\nuvx pytest tests/test_returns.py::test_simple_returns_basic\n\n\n\n\n\n\nNoteConfiguration with pytest.ini\n\n\n\nCreate a pytest.ini file in your project root to configure pytest:\n[tool.pytest.ini_options]\ntestpaths = tests\npython_files = test_*.py\npython_functions = test_*\naddopts = -v --strict-markers\nThis specifies where to find tests and how to run them.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html#test-driven-development-concepts",
    "href": "python/testing/index.html#test-driven-development-concepts",
    "title": "5¬† Error Handling and Testing",
    "section": "5.3 Test-Driven Development Concepts",
    "text": "5.3 Test-Driven Development Concepts\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video by ArjanCodes covers similar topics to this section.\n\n\n\nTest-Driven Development (TDD) is a development approach where you write tests before writing the code they test. This might seem backwards, but it has significant benefits, especially in research.\n\n5.3.1 The TDD Cycle\nTDD follows a simple cycle:\n\nRed: Write tests that fail (because the code doesn‚Äôt exist yet), including edge cases\nGreen: Write just enough code to make all the tests pass\nRefactor: Improve the code while keeping tests passing\n\nLet‚Äôs walk through an example. Suppose you need to calculate the maximum drawdown of a price series.\nStep 1: Write failing tests, including edge cases\n# test_risk.py\nfrom risk import calculate_max_drawdown\n\ndef test_max_drawdown_simple():\n    \"\"\"Test max drawdown with simple price series.\"\"\"\n    prices = [100, 110, 105, 115, 90, 95]\n    # Peak is 115, trough is 90, drawdown is (90-115)/115 ‚âà -0.217\n    assert abs(calculate_max_drawdown(prices) - (-0.217)) &lt; 0.001\n\ndef test_max_drawdown_no_drawdown():\n    \"\"\"Test with monotonically increasing prices (no drawdown).\"\"\"\n    prices = [100, 110, 120, 130]\n    assert calculate_max_drawdown(prices) == 0\n\ndef test_max_drawdown_single_price():\n    \"\"\"Test with single price.\"\"\"\n    prices = [100]\n    assert calculate_max_drawdown(prices) == 0\nRun these tests. They will all fail because calculate_max_drawdown doesn‚Äôt exist yet.\nStep 2: Write minimal code to pass\n# risk.py\ndef calculate_max_drawdown(prices):\n    \"\"\"Calculate maximum drawdown from a price series.\"\"\"\n    if len(prices) &lt;= 1:\n        return 0\n\n    max_drawdown = 0\n    peak = prices[0]\n\n    for price in prices:\n        if price &gt; peak:\n            peak = price\n        drawdown = (price - peak) / peak\n        if drawdown &lt; max_drawdown:\n            max_drawdown = drawdown\n\n    return max_drawdown\nRun the tests again. They should all pass.\nStep 3: Refactor if needed\nThe code is clean and handles all edge cases. We can now move on, or improve our function to make it more efficient.\n\n\n5.3.2 Benefits of TDD for Research\nTDD might feel slow at first, but it pays off:\n\nClarifies thinking: Writing the test first forces you to specify exactly what you want the function to do.\nPrevents scope creep: You implement only what‚Äôs needed to pass tests.\nDocuments intent: Tests show how the function should behave.\nEnables refactoring: You can improve code with confidence because tests verify behavior doesn‚Äôt change.\n\nIn research, TDD is particularly valuable when implementing statistical methods or financial calculations. Write tests based on the formulas in the paper, then implement the method. The tests verify you‚Äôve implemented the method correctly.\n\n\n\n\n\n\nTipTDD for Complex Calculations\n\n\n\nWhen implementing a complex statistical method from a paper:\n\nCreate tests using examples from the paper (if provided)\nCreate tests using results from R or Stata implementations\nCreate tests using simple cases you can verify by hand\nThen implement your Python version\n\nThis approach catches mistakes early and gives you confidence in your implementation.\n\n\n\n\n5.3.3 When Not to Use TDD\nTDD isn‚Äôt always the right approach:\n\nExploratory analysis: When you don‚Äôt know what you‚Äôre looking for, write code first, then add tests\nPrototypes: If you‚Äôre just trying something to see if it works, TDD adds overhead\nSimple scripts: For one-off analyses, informal testing might be enough\n\nBut for any code you‚Äôll reuse or that‚Äôs critical to your results, testing (whether test-first or test-after) is essential.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html#testing-floating-point-calculations",
    "href": "python/testing/index.html#testing-floating-point-calculations",
    "title": "5¬† Error Handling and Testing",
    "section": "5.4 Testing Floating-Point Calculations",
    "text": "5.4 Testing Floating-Point Calculations\nFinancial calculations often involve floating-point arithmetic, which has quirks:\ndef test_floating_point_comparison():\n    \"\"\"Demonstrate floating-point comparison issues.\"\"\"\n    # This might fail due to floating-point precision\n    result = 0.1 + 0.2\n    # Don't do this:\n    # assert result == 0.3  # Might fail!\n\n    # Do this instead:\n    assert abs(result - 0.3) &lt; 1e-10\n    # Or use pytest's approximate comparison:\n    assert result == pytest.approx(0.3)\nAlways use tolerance-based comparisons for floating-point numbers. The pytest.approx() function is particularly nice because it chooses sensible default tolerances:\ndef test_returns_calculation():\n    \"\"\"Test returns calculation with approximate comparison.\"\"\"\n    prices = [100, 105, 110.25]\n    returns = calculate_simple_returns(prices)\n    expected = [0.05, 0.05]\n\n    assert returns == pytest.approx(expected, rel=1e-6)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html#best-practices-summary",
    "href": "python/testing/index.html#best-practices-summary",
    "title": "5¬† Error Handling and Testing",
    "section": "5.5 Best Practices Summary",
    "text": "5.5 Best Practices Summary\nLet‚Äôs consolidate what we‚Äôve learned into actionable practices:\nError Handling:\n\nCatch specific exceptions, not broad ones\nProvide informative error messages\nDon‚Äôt hide errors unless you can handle them meaningfully\nUse custom exceptions for domain-specific errors\nValidate inputs early and explicitly\n\nTesting:\n\nWrite tests for any code you‚Äôll reuse\nTest edge cases, not just happy paths\nOne assertion per test when possible\nUse fixtures for reusable test data\nUse parametrization to test multiple scenarios\nRun tests frequently during development\n\nGeneral:\n\nMake functions testable (pure functions with clear inputs/outputs)\nValidate assumptions with assertions\nDocument expected behavior\nUse type hints to catch errors early\nReview your own code before considering it done\n\n\n\n\n\n\n\nNoteTesting in Research Workflows\n\n\n\nIn empirical research, you often have a mix of:\n\nLibrary code: Functions you‚Äôll reuse across projects (test thoroughly)\nAnalysis scripts: One-off analyses (test key calculations)\nExploratory code: Trying things out (informal testing is fine)\n\nFocus your testing effort on library code and anything that affects your paper‚Äôs results. A bug in a chart‚Äôs formatting is annoying; a bug in your returns calculation invalidates your research.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html#conclusion",
    "href": "python/testing/index.html#conclusion",
    "title": "5¬† Error Handling and Testing",
    "section": "5.6 Conclusion",
    "text": "5.6 Conclusion\nError handling and testing might feel like overhead when you start a project, but they‚Äôre investments that pay enormous dividends. Code that handles errors gracefully is more robust and maintainable. Code with tests is easier to modify, debug, and trust.\nIn empirical research, where your code directly impacts your results and conclusions, this isn‚Äôt just about software engineering best practices‚Äîit‚Äôs about research integrity. A well-tested analysis pipeline gives you confidence in your results. Good error handling helps you identify data quality issues and edge cases. Together, they make your research more reproducible and reliable.\nStart small. Add error handling to functions that interact with external data. Write tests for your key calculations. As these practices become habits, you‚Äôll find yourself writing better code, spending less time debugging, and having more confidence in your results.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/logging/index.html",
    "href": "python/logging/index.html",
    "title": "6¬† Logging and Configuration",
    "section": "",
    "text": "6.1 Why Logging Matters\nWhen you run a research pipeline‚Äîdownloading data, cleaning it, estimating models, and generating output‚Äîthings will go wrong. Files will be missing, APIs will fail, and edge cases will surface. Without proper logging, you‚Äôre left guessing what happened and when. This chapter covers Python‚Äôs built-in logging module and introduces Hydra, a powerful framework for managing complex research configurations.\nGood logging practices are essential for reproducible research. When you run an analysis months later or share code with collaborators, logs provide a record of what the code did, what warnings occurred, and where things failed. Combined with proper configuration management, you can recreate any run exactly as it happened.\nMany researchers start by sprinkling print() statements throughout their code:\nThis approach has several problems:\nA proper logging system offers:",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Logging and Configuration</span>"
    ]
  },
  {
    "objectID": "python/logging/index.html#why-logging-matters",
    "href": "python/logging/index.html#why-logging-matters",
    "title": "6¬† Logging and Configuration",
    "section": "",
    "text": "print(\"Starting data download...\")\nprint(f\"Downloaded {len(df)} rows\")\nprint(\"WARNING: Missing values detected\")\nprint(\"Error: API rate limit exceeded\")\n\n\nNo severity levels: You can‚Äôt distinguish informational messages from warnings or errors\nNo timestamps: You don‚Äôt know when events occurred\nNo control: You can‚Äôt easily turn messages on or off or redirect them to files\nNo context: You don‚Äôt know which module or function produced the message\nCluttered output: Everything goes to the same place, making it hard to find important messages\n\n\n\nSeverity levels: DEBUG, INFO, WARNING, ERROR, CRITICAL‚Äîso you can filter by importance\nTimestamps: Know exactly when each event occurred\nSource information: See which module and function generated each message\nFlexible output: Send logs to console, files, or external services\nConfiguration: Control logging behavior without changing code",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Logging and Configuration</span>"
    ]
  },
  {
    "objectID": "python/logging/index.html#pythons-logging-module",
    "href": "python/logging/index.html#pythons-logging-module",
    "title": "6¬† Logging and Configuration",
    "section": "6.2 Python‚Äôs logging Module",
    "text": "6.2 Python‚Äôs logging Module\nPython‚Äôs standard library includes a powerful logging module. While it has a learning curve, understanding its core concepts pays off in any serious project.\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video provides a good overview of Python logging.\n\n\n\n\n6.2.1 Basic Usage\nThe simplest way to use logging:\n\nimport logging\n\n# Configure basic logging\nlogging.basicConfig(level=logging.INFO)\n\n# Create a logger for this module\nlogger = logging.getLogger(__name__)\n\n# Log messages at different levels\nlogger.debug(\"Detailed information for debugging\")\nlogger.info(\"General information about program execution\")\nlogger.warning(\"Something unexpected happened, but program continues\")\nlogger.error(\"A serious problem occurred\")\nlogger.critical(\"Program may not be able to continue\")\n\nINFO:__main__:General information about program execution\nWARNING:__main__:Something unexpected happened, but program continues\nERROR:__main__:A serious problem occurred\nCRITICAL:__main__:Program may not be able to continue\n\n\n\n\n6.2.2 Understanding Log Levels\nLog levels form a hierarchy. When you set a level, you see messages at that level and above:\n\n\n\nLevel\nNumeric Value\nWhen to Use\n\n\n\n\nDEBUG\n10\nDetailed diagnostic information\n\n\nINFO\n20\nConfirmation that things work as expected\n\n\nWARNING\n30\nSomething unexpected but not necessarily wrong\n\n\nERROR\n40\nA serious problem; some functionality failed\n\n\nCRITICAL\n50\nA very serious error; program may crash\n\n\n\n\nimport logging\n\n# Only show WARNING and above\n1logging.basicConfig(level=logging.WARNING, force=True)\nlogger = logging.getLogger(\"level_demo\")\n\nlogger.debug(\"This won't appear\")\nlogger.info(\"This won't appear either\")\nlogger.warning(\"This will appear\")\nlogger.error(\"This will definitely appear\")\n\n\n1\n\nThe force=True parameter is needed here because we already called basicConfig() earlier in this chapter. By default, basicConfig() only configures logging once‚Äîsubsequent calls are ignored. Using force=True removes any existing handlers and reconfigures logging with the new settings.\n\n\n\n\nWARNING:level_demo:This will appear\nERROR:level_demo:This will definitely appear\n\n\nUsing log levels provides two key advantages:\n\nRoute messages to different outputs: You can direct messages of different levels to different destinations‚Äîfor example, send INFO messages to a file while only showing WARNING and above on the console.\nControl verbosity at runtime: You can leave all log messages in your code but choose at runtime which levels to display. This means you can include detailed DEBUG messages during development that won‚Äôt clutter your output in production unless you need them.\n\n\n\n6.2.3 Configuring Log Format\nThe default format is minimal. For research workflows, you typically want more information:\n\nimport logging\n\n# Configure with a custom format\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    force=True\n)\n\nlogger = logging.getLogger(\"format_demo\")\nlogger.info(\"Now you can see when this happened\")\n\n2026-01-03 10:48:27 - format_demo - INFO - Now you can see when this happened\n\n\nCommon format fields:\n\n%(asctime)s: Human-readable timestamp\n%(name)s: Logger name (usually module name)\n%(levelname)s: DEBUG, INFO, WARNING, etc.\n%(message)s: The actual log message\n%(filename)s: Source file name\n%(lineno)d: Line number in source file\n%(funcName)s: Function name\n\n\n\n6.2.4 Logging to Files\nFor long-running research pipelines, you want logs saved to files:\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('research_pipeline.log'),\n        logging.StreamHandler()  # Also print to console\n    ]\n)\n\nlogger = logging.getLogger(__name__)\nlogger.info(\"This goes to both the file and console\")\n\n\n6.2.5 Logging Exceptions\nWhen catching exceptions, use logger.exception() to automatically include the traceback:\n\nimport logging\n\nlogging.basicConfig(level=logging.INFO, force=True)\nlogger = logging.getLogger(\"exception_demo\")\n\ndef risky_calculation(x):\n    return 1 / x\n\ntry:\n    result = risky_calculation(0)\nexcept ZeroDivisionError:\n    logger.exception(\"Calculation failed\")\n    # The traceback is automatically included\n\nERROR:exception_demo:Calculation failed\nTraceback (most recent call last):\n  File \"/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_77347/1524267329.py\", line 10, in &lt;module&gt;\n    result = risky_calculation(0)\n  File \"/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_77347/1524267329.py\", line 7, in risky_calculation\n    return 1 / x\n           ~~^~~\nZeroDivisionError: division by zero\n\n\nIncluding the full traceback is a tradeoff: it provides valuable debugging information, but the multi-line output can break the structure of log files, making them harder to parse or query programmatically. For production systems where logs are processed automatically, you might prefer logging just the exception message and using logger.error() instead.\n\n\n6.2.6 Module-Level Loggers\nThe recommended pattern is to create a logger at the top of each module:\n# In portfolio_analysis.py\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef calculate_returns(prices):\n    logger.info(f\"Calculating returns for {len(prices)} observations\")\n    returns = prices.pct_change().dropna()\n\n    if returns.isna().any().any():\n        logger.warning(\"NaN values detected in returns\")\n\n    logger.debug(f\"Returns shape: {returns.shape}\")\n    return returns\nThe __name__ variable becomes the module‚Äôs fully qualified name (e.g., myproject.portfolio_analysis), which helps you trace where messages came from.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Logging and Configuration</span>"
    ]
  },
  {
    "objectID": "python/logging/index.html#logging-best-practices",
    "href": "python/logging/index.html#logging-best-practices",
    "title": "6¬† Logging and Configuration",
    "section": "6.3 Logging Best Practices",
    "text": "6.3 Logging Best Practices\nHere are key practices to follow when implementing logging in your research projects.\nUse appropriate levels. Choose log levels thoughtfully:\n# DEBUG: Detailed diagnostic info, usually only for debugging\nlogger.debug(f\"Processing row {i}: values = {row}\")\n\n# INFO: Key milestones and confirmations\nlogger.info(f\"Loaded {len(df)} rows from {filename}\")\n\n# WARNING: Unexpected but handled situations\nlogger.warning(f\"Missing data for {ticker}, using interpolation\")\n\n# ERROR: Something failed, but program can continue\nlogger.error(f\"Failed to download {ticker}: {e}\")\n\n# CRITICAL: Serious failure, program may need to stop\nlogger.critical(\"Database connection lost, cannot continue\")\nInclude context in messages. Log messages should be self-explanatory:\n# Bad: Not enough context\nlogger.info(\"Processing file\")\nlogger.warning(\"Missing values\")\n\n# Good: Clear context\nlogger.info(f\"Processing file: {filepath}\")\nlogger.warning(f\"Missing values in column '{col}': {count} rows affected\")\nDon‚Äôt log sensitive information. Be careful not to log passwords, API keys, or sensitive data:\n# Bad: Logs the API key\nlogger.info(f\"Connecting with API key: {api_key}\")\n\n# Good: Masks sensitive information\nlogger.info(f\"Connecting with API key: {api_key[:4]}...\")\nUse structured logging for complex data. For data that might be parsed later, consider structured formats:\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Log structured data\nmetrics = {\n    'ticker': 'AAPL',\n    'sharpe_ratio': 1.45,\n    'max_drawdown': -0.15,\n    'n_observations': 252\n}\nlogger.info(f\"Performance metrics: {json.dumps(metrics)}\")\nConfigure logging once at entry point. Configure logging at your application‚Äôs entry point, not in library modules. Include a timestamp in the log filename so that each run generates a new file:\n# In main.py or run_analysis.py\nimport logging\nfrom datetime import datetime\nfrom my_research import run_pipeline\n\ndef setup_logging():\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(f'analysis_{timestamp}.log'),\n            logging.StreamHandler()\n        ]\n    )\n\nif __name__ == \"__main__\":\n    setup_logging()\n    run_pipeline()\nLibrary modules should only create loggers, not configure them:\n# In my_research/analysis.py\nimport logging\n\nlogger = logging.getLogger(__name__)  # Just create the logger\n\ndef run_pipeline():\n    logger.info(\"Starting pipeline\")\n    # ...",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Logging and Configuration</span>"
    ]
  },
  {
    "objectID": "python/logging/index.html#configuration-management-with-hydra",
    "href": "python/logging/index.html#configuration-management-with-hydra",
    "title": "6¬† Logging and Configuration",
    "section": "6.4 Configuration Management with Hydra",
    "text": "6.4 Configuration Management with Hydra\nAs research projects grow, managing configuration becomes a challenge. You might have:\n\nDifferent data sources (local files, APIs, databases)\nMultiple model specifications to compare\nVarious output formats and destinations\nDevelopment vs.¬†production settings\n\nHardcoding these in Python leads to messy code and makes it hard to reproduce specific runs. YAML configuration files help, but you end up writing boilerplate code to load and validate them.\nHydra is a framework developed by Facebook Research (Yadan 2019) that elegantly solves these problems. It provides:\n\nHierarchical configuration: Compose configs from multiple sources\nCommand-line overrides: Change any parameter without editing files\nAutomatic working directories: Each run gets its own output directory\nMulti-run support: Sweep over parameter combinations\n\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video provides a good overview of Hydra for managing project configurations.\n\n\n\n\n6.4.1 Installing Hydra\nuv add hydra-core\n\n\n6.4.2 Basic Hydra Application\nHere‚Äôs a minimal Hydra application:\n# my_analysis.py\nimport hydra\nfrom omegaconf import DictConfig\n\n@hydra.main(version_base=None, config_path=\"conf\", config_name=\"config\")\ndef main(cfg: DictConfig) -&gt; None:\n    print(f\"Processing data from: {cfg.data.source}\")\n    print(f\"Output directory: {cfg.output.dir}\")\n    print(f\"Model: {cfg.model.name}\")\n\nif __name__ == \"__main__\":\n    main()\nWith a configuration file:\n# conf/config.yaml\ndata:\n  source: \"data/returns.parquet\"\n  start_date: \"2020-01-01\"\n  end_date: \"2023-12-31\"\n\nmodel:\n  name: \"ols\"\n  robust_se: true\n\noutput:\n  dir: \"results\"\n  format: \"parquet\"\nRun it:\npython my_analysis.py\nOverride parameters from command line:\npython my_analysis.py data.start_date=2022-01-01 model.name=fama_macbeth\n\n\n6.4.3 Configuration Composition\nHydra‚Äôs power comes from composing configurations. Organize your configs into groups:\nconf/\n‚îú‚îÄ‚îÄ config.yaml          # Main config with defaults\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ crsp.yaml       # CRSP data settings\n‚îÇ   ‚îú‚îÄ‚îÄ compustat.yaml  # Compustat settings\n‚îÇ   ‚îî‚îÄ‚îÄ local.yaml      # Local file settings\n‚îú‚îÄ‚îÄ model/\n‚îÇ   ‚îú‚îÄ‚îÄ ols.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ fama_macbeth.yaml\n‚îÇ   ‚îî‚îÄ‚îÄ panel.yaml\n‚îî‚îÄ‚îÄ output/\n    ‚îú‚îÄ‚îÄ paper.yaml      # Publication-ready output\n    ‚îî‚îÄ‚îÄ debug.yaml      # Quick debug output\nThe main config selects defaults:\n# conf/config.yaml\ndefaults:\n  - data: crsp\n  - model: ols\n  - output: paper\n\nexperiment_name: \"baseline\"\nEach group config defines its settings:\n# conf/data/crsp.yaml\nsource: \"wrds\"\ndatabase: \"crsp\"\ntable: \"msf\"\nstart_date: \"1990-01-01\"\nend_date: \"2023-12-31\"\n# conf/model/fama_macbeth.yaml\nname: \"fama_macbeth\"\nrobust_se: true\nlags: 5\nSwitch configurations easily:\n# Use Compustat data with Fama-MacBeth model\npython my_analysis.py data=compustat model=fama_macbeth\n\n# Quick debug run\npython my_analysis.py output=debug data.end_date=2020-01-31\n\n\n6.4.4 Automatic Output Directories\nHydra automatically creates a unique output directory for each run:\noutputs/\n‚îî‚îÄ‚îÄ 2024-01-15/\n    ‚îî‚îÄ‚îÄ 14-30-22/\n        ‚îú‚îÄ‚îÄ .hydra/\n        ‚îÇ   ‚îú‚îÄ‚îÄ config.yaml      # Full resolved config\n        ‚îÇ   ‚îú‚îÄ‚îÄ hydra.yaml       # Hydra settings\n        ‚îÇ   ‚îî‚îÄ‚îÄ overrides.yaml   # Command-line overrides\n        ‚îú‚îÄ‚îÄ my_analysis.log      # Automatic logging\n        ‚îî‚îÄ‚îÄ results/             # Your output files\nThis makes every run reproducible‚Äîyou can see exactly what configuration was used.\n\n\n6.4.5 Using Hydra for Research Pipelines\nHere‚Äôs a more complete example for an empirical finance pipeline:\n# run_analysis.py\nimport logging\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\nimport pandas as pd\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_data(cfg: DictConfig) -&gt; pd.DataFrame:\n    \"\"\"Load data according to configuration.\"\"\"\n    logger.info(f\"Loading data from {cfg.data.source}\")\n\n    if cfg.data.source == \"local\":\n        df = pd.read_parquet(cfg.data.path)\n    elif cfg.data.source == \"wrds\":\n        # WRDS loading logic\n        pass\n\n    # Apply date filters\n    df = df[(df['date'] &gt;= cfg.data.start_date) &\n            (df['date'] &lt;= cfg.data.end_date)]\n\n    logger.info(f\"Loaded {len(df)} observations\")\n    return df\n\n\ndef run_model(df: pd.DataFrame, cfg: DictConfig) -&gt; dict:\n    \"\"\"Run the specified model.\"\"\"\n    logger.info(f\"Running {cfg.model.name} model\")\n\n    # Model logic here\n    results = {\"coefficients\": {}, \"stats\": {}}\n\n    return results\n\n\ndef save_results(results: dict, cfg: DictConfig) -&gt; None:\n    \"\"\"Save results according to configuration.\"\"\"\n    output_dir = Path(cfg.output.dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save based on format\n    if cfg.output.format == \"parquet\":\n        # Save as parquet\n        pass\n    elif cfg.output.format == \"latex\":\n        # Generate LaTeX tables\n        pass\n\n    logger.info(f\"Results saved to {output_dir}\")\n\n\n@hydra.main(version_base=None, config_path=\"conf\", config_name=\"config\")\ndef main(cfg: DictConfig) -&gt; None:\n    # Log the full configuration\n    logger.info(\"Configuration:\\n\" + OmegaConf.to_yaml(cfg))\n\n    # Run pipeline\n    df = load_data(cfg)\n    results = run_model(df, cfg)\n    save_results(results, cfg)\n\n    logger.info(\"Pipeline completed successfully\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n6.4.6 Multi-Run for Parameter Sweeps\nHydra can automatically run your code with multiple parameter combinations:\n# Run with multiple date ranges\npython run_analysis.py -m data.start_date=2010-01-01,2015-01-01,2020-01-01\n\n# Sweep over models\npython run_analysis.py -m model=ols,fama_macbeth,panel\nEach combination gets its own output directory with full configuration tracking.\nThis feature is particularly useful for testing the sensitivity of your analysis to empirical choices. For example, you can run your analysis with multiple winsorization levels, different sample periods, or alternative variable definitions to ensure your results are robust to these choices.\n\n\n6.4.7 Hydra with Logging\nHydra automatically configures Python‚Äôs logging module. Your log messages go to both the console and a file in the output directory:\nimport logging\nimport hydra\nfrom omegaconf import DictConfig\n\nlogger = logging.getLogger(__name__)\n\n@hydra.main(version_base=None, config_path=\"conf\", config_name=\"config\")\ndef main(cfg: DictConfig) -&gt; None:\n    logger.info(\"Starting analysis\")  # Automatically logged to file\n    logger.debug(\"Debug info\")  # Also captured\n\n    # Your code here\nYou can customize logging in conf/hydra/job_logging.yaml or in your main config.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Logging and Configuration</span>"
    ]
  },
  {
    "objectID": "python/logging/index.html#summary",
    "href": "python/logging/index.html#summary",
    "title": "6¬† Logging and Configuration",
    "section": "6.5 Summary",
    "text": "6.5 Summary\nProper logging and configuration management are essential for reproducible research:\n\nUse Python‚Äôs logging module instead of print statements for production code\nChoose appropriate log levels to distinguish routine information from warnings and errors\nInclude context in log messages so they‚Äôre meaningful when read later\nConfigure logging at the entry point, not in library modules\nUse Hydra for configuration management in complex research pipelines\nLeverage Hydra‚Äôs automatic output directories to make every run reproducible\n\nThese practices might seem like overhead for small scripts, but they pay dividends as projects grow. When you need to debug a failed run from last month or share code with collaborators, good logging and configuration management make the difference between hours of frustration and quickly finding the answer.\n\n\n\n\nYadan, Omry. 2019. ‚ÄúHydra - a Framework for Elegantly Configuring Complex Applications.‚Äù Github. https://github.com/facebookresearch/hydra.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Logging and Configuration</span>"
    ]
  },
  {
    "objectID": "python/tools/index.html",
    "href": "python/tools/index.html",
    "title": "7¬† Development Environment and Tools",
    "section": "",
    "text": "7.1 Fonts for Coding\nThis chapter covers setting up Visual Studio Code (VS Code) for Python data science work. A well-configured development environment makes coding more pleasant and efficient. We‚Äôll cover fonts, themes, extensions for formatting and linting, data exploration tools, and productivity enhancements.\nBefore opening VS Code, choose a good programming font. The right font makes code more readable and reduces eye strain during long sessions. Programming fonts have specific characteristics:\nLigatures are combined symbols that appear when certain characters follow each other. For example, -&gt; becomes a proper arrow ‚Üí, != becomes a not-equal symbol ‚â†, and &gt;= becomes a greater-than-or-equal symbol ‚â•. The underlying characters don‚Äôt change‚Äîyour code still contains -&gt;. Ligatures simply render more elegantly on screen. Whether to use ligatures is a personal choice; some developers love them, others find them distracting.\nHere are some recommended programming fonts:\nAll of these fonts are also available as Nerd Fonts versions. Nerd Fonts are patched versions that include additional icons and symbols. While they don‚Äôt add anything when coding in an editor, they make many terminal-based tools look nicer by enabling icons for file types, git status, and other visual indicators.\nPick the font that feels right to you, install it on your system, and we‚Äôll configure it in VS Code below.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Development Environment and Tools</span>"
    ]
  },
  {
    "objectID": "python/tools/index.html#fonts-for-coding",
    "href": "python/tools/index.html#fonts-for-coding",
    "title": "7¬† Development Environment and Tools",
    "section": "",
    "text": "Monospace: Every character has the same width, essential for code alignment\nDistinguishable characters: Clear differences between similar characters like 1, l, I and 0, O\nLigatures (optional): Special combined glyphs for common character sequences\n\n\n\n\nFiraCode: A popular programming font with excellent readability and comprehensive ligature support. It‚Äôs the font used throughout this book.\nJetBrains Mono: Designed specifically for developers by JetBrains (makers of PyCharm), featuring increased letter height for better readability and distinctive character shapes.\nMonaspace: A family of programming fonts published by GitHub with several variants optimized for different use cases.\nVictor Mono: Particularly nice-looking in terminal applications, with a distinctive style that some developers prefer.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Development Environment and Tools</span>"
    ]
  },
  {
    "objectID": "python/tools/index.html#vs-code-setup",
    "href": "python/tools/index.html#vs-code-setup",
    "title": "7¬† Development Environment and Tools",
    "section": "7.2 VS Code Setup",
    "text": "7.2 VS Code Setup\nIn this section, I present how I configure VS Code for research and data science work. These are my personal preferences‚Äîfeel free to adapt them to your own workflow.\n\n7.2.1 Theme and colors\nA good color theme makes your editor pleasant to look at during long coding sessions. My favorite is Catppuccin, which offers four variants: one light and three dark.\nTo install:\n\nOpen the Extensions panel (Ctrl+Shift+X or Cmd+Shift+X on Mac)\nSearch for ‚ÄúCatppuccin for VSCode‚Äù\nInstall the extension\nChoose your preferred variant (I use Mocha, the darkest)\n\nAlso install Catppuccin Icons for VS Code to get matching file icons in the explorer.\n\n\n\n\n\n\nTipConsistent theming\n\n\n\nCatppuccin is available for many applications beyond VS Code. If you like the color scheme, check out their website to theme your terminal, browser, and other tools consistently.\n\n\n\n\n7.2.2 Configuring your font\nTo set your chosen font in VS Code:\n\nOpen Settings (Ctrl+, or Cmd+, on Mac)\nSearch for ‚Äúfont‚Äù\nIn ‚ÄúEditor: Font Family‚Äù, enter your font name (e.g., Fira Code)\n\nTo enable ligatures, you need to edit the settings JSON directly:\n\nIn Settings, search for ‚ÄúFont Ligatures‚Äù\nClick ‚ÄúEdit in settings.json‚Äù\nAdd or modify:\n\n{\n    \"editor.fontFamily\": \"Fira Code\",\n    \"editor.fontLigatures\": true\n}\nSet fontLigatures to false if you prefer not to use them.\n\n\n7.2.3 Disabling the minimap\nThe minimap (the code preview on the right side of the editor) takes up space without adding much value for most workflows. To disable it:\n\nOpen Settings\nSearch for ‚Äúminimap‚Äù\nUncheck ‚ÄúEditor: Minimap: Enabled‚Äù\n\n\n\n7.2.4 Adding a ruler\nRuff (which we‚Äôll install next) limits lines to 88 characters by default. Adding a visual ruler helps you see this limit:\n\nOpen Settings and search for ‚Äúrulers‚Äù\nClick ‚ÄúEdit in settings.json‚Äù\nAdd:\n\n{\n    \"editor.rulers\": [88]\n}\nThis displays a vertical line at column 88, making it easy to arrange multiple files side by side.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Development Environment and Tools</span>"
    ]
  },
  {
    "objectID": "python/tools/index.html#extensions",
    "href": "python/tools/index.html#extensions",
    "title": "7¬† Development Environment and Tools",
    "section": "7.3 Extensions",
    "text": "7.3 Extensions\n\n7.3.1 Essential extensions\n\n7.3.1.1 Python extension\nThe Python extension is fundamental for Python development in VS Code. It provides:\n\nIntelliSense (smart code completion)\nLinting and error detection\nDebugging support\nJupyter notebook integration\nTest runner integration\n\nInstall it from the Extensions panel by searching for ‚ÄúPython‚Äù (by Microsoft).\n\n\n7.3.1.2 Ruff for formatting and linting\nRuff is a fast Python linter and formatter. Install the Ruff extension from the Extensions panel.\nTo configure automatic formatting on save, add to your settings.json:\n{\n    \"[python]\": {\n        \"editor.formatOnSave\": true,\n        \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n        \"editor.codeActionsOnSave\": {\n            \"source.organizeImports\": \"explicit\"\n        }\n    }\n}\nThis configuration:\n\nFormats your Python code automatically when you save\nUses Ruff as the default formatter\nOrganizes imports (alphabetically, grouped by standard library, third-party, and local imports)\nRemoves unused imports\n\nTo enable formatting for Jupyter notebooks as well:\n{\n    \"notebook.formatOnSave.enabled\": true\n}\n\n\n\n\n\n\nNoteWhat Ruff does\n\n\n\nRuff doesn‚Äôt change your code‚Äôs behavior‚Äîit reformats it to follow consistent conventions. For example, it fixes spacing, line breaks, and quote styles. It also removes unused imports and organizes your import statements.\n\n\n\n\n7.3.1.3 Markdown\nIf you write Markdown documents (notes, README files, documentation), these extensions are useful:\n\nmarkdownlint: Catches formatting issues and enforces consistent style.\nMarkdown All-in-One: Adds productivity features like keyboard shortcuts, table of contents generation, and list editing.\nMarkdown Preview Mermaid Support: Previews Mermaid diagrams in Markdown files. Mermaid is a standard diagramming format supported by GitHub and many documentation tools.\n\n\n\n\n7.3.2 Data science extensions\n\nRainbow CSV: Colorizes CSV files when you open them in VS Code, making it easier to distinguish columns in raw data files.\nData Wrangler: A powerful extension for exploring and cleaning pandas DataFrames interactively. When you load data with pandas, you can click ‚ÄúView Data‚Äù to open an interactive data explorer, filter and sort data visually, and generate Python code that replicates your data transformations. This extension is particularly valuable for data cleaning and initial data exploration.\n\n\n\n7.3.3 Git extensions\nVS Code has built-in Git support, but these extensions enhance it:\n\nGit Graph: Provides a visual representation of your repository‚Äôs branch structure and commit history. It‚Äôs essential for understanding complex branching scenarios.\nGitLens: By GitKraken, adds inline blame annotations showing who changed each line, detailed commit information, and file and line history. The free version includes many useful features; premium features require a subscription.\n\n\n\n7.3.4 AI coding assistant\nGitHub Copilot provides AI-powered code suggestions as you type. Install the GitHub Copilot extension and sign in with your GitHub account. A free tier is available with some limitations. Students and academics can apply for GitHub Education, which includes Copilot access.\n\n\n\n\n\n\nCautionAI tools require verification\n\n\n\nWhile AI assistants can boost productivity, always verify the code they generate. AI models can produce plausible-looking but incorrect code, especially for domain-specific tasks like financial calculations.\n\n\n\n\n7.3.5 Productivity extensions\n\nTODO Tree: Scans your project for comments containing TODO or FIXME and displays them in a sidebar panel. This helps you track tasks and issues scattered throughout your codebase.\nEven Better TOML: Adds syntax highlighting and validation for TOML files. TOML is the standard format for Python configuration files (pyproject.toml), so this extension is useful for any Python project.\nVim (VSCodeVim): For keyboard-driven editing without touching the mouse. Vim bindings have a steep learning curve but dramatically increase editing speed once mastered. If you‚Äôre new to Vim, also install Learn Vim, which provides an interactive tutorial.\n\n\n\n7.3.6 Dev containers\nThe Dev Containers extension lets you develop inside containerized environments, isolating your project dependencies and keeping your system clean. See the Dev Containers chapter for details on setting this up.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Development Environment and Tools</span>"
    ]
  },
  {
    "objectID": "python/tools/index.html#complete-settings-reference",
    "href": "python/tools/index.html#complete-settings-reference",
    "title": "7¬† Development Environment and Tools",
    "section": "7.4 Complete Settings Reference",
    "text": "7.4 Complete Settings Reference\nHere‚Äôs a complete settings.json incorporating the configurations discussed:\n{\n    \"editor.fontFamily\": \"Fira Code\",\n    \"editor.fontLigatures\": true,\n    \"editor.minimap.enabled\": false,\n    \"editor.rulers\": [88],\n    \"[python]\": {\n        \"editor.formatOnSave\": true,\n        \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n        \"editor.codeActionsOnSave\": {\n            \"source.organizeImports\": \"explicit\"\n        }\n    },\n    \"notebook.formatOnSave.enabled\": true\n}\nYou can access your settings file by opening the Command Palette (Ctrl+Shift+P or Cmd+Shift+P) and selecting ‚ÄúPreferences: Open User Settings (JSON)‚Äù.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Development Environment and Tools</span>"
    ]
  },
  {
    "objectID": "python/git/index.html",
    "href": "python/git/index.html",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "",
    "text": "8.1 What is version control?\nVersion control, also known as source control, is a system that records changes to a file or set of files over time so that you can recall specific versions later. It‚Äôs one of the most important tools in the toolkit of any developer or data scientist. It‚Äôs also very useful for researchers, especially those working with code, but in practice, it is underused in academia. The idea behind version control is quite simple: it allows you to track and manage changes to your projects. Think of it as the ‚Äútrack changes‚Äù feature in Microsoft Word, but for all your files and turbocharged with features that make it easy to collaborate with others.\nImagine you‚Äôre working on a research paper and decide to delete a section. A few days later, you realize that section was crucial. Without version control, you‚Äôd have to rewrite that entire section. With version control, you can simply look at your previous versions, find the one that includes the section you need, and restore it. Most of us have some kind of version control in our lives. For example, when you write a paper, you might save different versions of the document as you work on it. This way, if you make a mistake or delete something important, you can go back to a previous version. However, this approach has limitations, and there are better ways to manage versions of your work.\nIn the context of coding, version control is even more important. As you add new features to your code or fix bugs, it‚Äôs essential to be able to track these changes. If something breaks, you need to know what was changed so you can figure out what went wrong and how to fix it. Additionally, version control systems allow multiple people to work on the same project simultaneously, making collaboration easier and more efficient while keeping a detailed record of who made what changes and when.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#what-is-git",
    "href": "python/git/index.html#what-is-git",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.2 What is Git?",
    "text": "8.2 What is Git?\nGit is the most widely used version control system in the world. It was created in 2005 by Linus Torvalds, the creator of the Linux operating system. Torvalds wanted a version control system that was fast, efficient, and capable of handling small to very large projects with ease. Unlike its predecessors, Git was designed to be decentralized, allowing multiple developers to work on the same project simultaneously without stepping on each other‚Äôs toes. Like Linux, Git is free and distributed under an open-source license.\nAt its core, Git allows users to keep a complete history of their project, noting every change made to every file. This feature is akin to having a detailed logbook that captures the evolution of a project over time. With Git, users can branch off from the main project to experiment or work on new features without disrupting the core project. Later, these branches can be merged back into the main project seamlessly. This ability to branch and merge is particularly powerful, preventing conflicts and maintaining the integrity of the original project. Git is also incredibly robust in managing project history, enabling users to revert to previous versions if needed, offering a safety net against errors or unintended consequences of new changes.\n\n\n\n\n\n\nTipNot only for code\n\n\n\nGit is a great tool for version control of any kind of file, especially text files. It turns out that if you mainly use LaTeX or Markdown for writing and presentations, you can use Git to track changes in your documents and collaborate with others. Gone are the days of sending around files with names like paper_v1_final_final_really_final.tex and paper_v1_final_final_really_final_revised.tex!",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#what-is-github",
    "href": "python/git/index.html#what-is-github",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.3 What is GitHub?",
    "text": "8.3 What is GitHub?\nGitHub, launched in 2008 and acquired by Microsoft in 2018, quickly rose to become the de facto online platform for code management and collaboration. While Git is the engine, GitHub can be thought of as the sleek, user-friendly vehicle that houses this engine. It takes the core functionalities of Git and provides a web-based graphical interface that is intuitive and accessible. GitHub‚Äôs rise is not just due to its user-friendly nature but also because it functions like a social network for developers and researchers. Users can host their Git repositories, share their work with others, collaborate on projects, and even contribute to others‚Äô projects.\n\n\n\n\n\n\nNoteNot the only game in town.\n\n\n\nWhile GitHub is the most popular platform for code management and collaboration, it is not the only one. Two other popular platforms are GitLab and Bitbucket. Cloud providers like AWS and Azure also offer Git hosting services. Gitea is an open-source platform that can be self-hosted for free.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#why-use-git-and-github-for-research",
    "href": "python/git/index.html#why-use-git-and-github-for-research",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.4 Why use Git and GitHub for research?",
    "text": "8.4 Why use Git and GitHub for research?\nFor finance researchers, Git and GitHub offer a multitude of benefits. Git is an excellent tool for managing complex research projects. It allows researchers to track changes in their data analysis scripts, models, and even research papers, ensuring a clear audit trail of how the analysis was conducted and conclusions were reached. This level of transparency is crucial not just for personal record-keeping but also for collaborative projects where multiple researchers contribute to a single body of work. In a field where reputation is everything, Git can help researchers maintain a high level of integrity and accountability. The pull request system of GitHub is particularly beneficial for collaborative projects. It enables researchers to propose, discuss, and review changes before they are integrated into the main project. This not only ensures that every change is scrutinized for accuracy and relevance but also fosters a culture of peer review and collective improvement among collaborators as the project progresses. Furthermore, GitHub‚Äôs issue-tracking and project management features help researchers organize their tasks, track bugs, and manage project progress transparently.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#git-workflow",
    "href": "python/git/index.html#git-workflow",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.5 Git workflow",
    "text": "8.5 Git workflow\n\n8.5.1 Understanding the Core Concepts of Git\nGit‚Äôs power lies in its ability to manage and track changes in your projects, and this is achieved through a set of core functionalities. Let‚Äôs demystify these key terms:\n1. Repository (Repo): The heart of any Git project, a repository is like a project folder but with superpowers. It contains all of your project files along with each file‚Äôs revision history. You can have local repositories on your computer and synchronize them with remote repositories on GitHub to share and collaborate.\n2. Staging: Think of staging as a prep area. When you make changes to files, they don‚Äôt automatically get saved into your repository. Instead, you selectively add these changes to the staging area, indicating that you‚Äôve marked these modifications for your next commit.\n3. Commits: Committing is the act of saving your staged changes to the project‚Äôs history. A commit is like a snapshot of your repository at a particular point in time. Each commit has a unique ID and includes a message describing the changes, aiding future you or collaborators in understanding what was modified and why.\n4. Push and Pull: These are the methods by which you interact with a remote repository. When you push, you are sending your committed changes to a remote repo. Conversely, when you pull, you are fetching the latest changes from the remote repo to your local machine.\n5. Branching: Branching allows you to diverge from the main line of development and work independently without affecting the main project, often referred to as the main branch.1 It‚Äôs perfect for developing new features or experimenting.\n6. Merging: After you‚Äôve finished working in a branch, you merge those changes back into the main project. Merging combines the changes in your branch with those in the main branch, creating a single, unified history.\n7. Conflicts: Sometimes, when merging branches, Git encounters conflicts - changes that contradict each other. This can happen when two people make changes to the same file. These conflicts need to be manually resolved before completing the merge process.\nWe will explore these concepts in more detail in the next sections.\n\n\n8.5.2 Creating a repository\nA repository (or repo) is where all the magic happens ‚Äì it‚Äôs where your code, documentation, and all other project-related files reside. To create a new repo, simply log into your GitHub account, click on the + icon in the top right corner, and select New repository.\n\n8.5.2.1 Naming and Describing Your Repository\nChoose a name that succinctly reflects your project. Keep in mind that this name will be part of the URL for your repository, and that it will be used as the default name for the folder when you clone the repository (make a local copy of the repository on your computer). The description field is an opportunity to briefly outline your project‚Äôs objective. This helps others understand the purpose of your repo at a glance.\n\n\n8.5.2.2 Selecting a License\nYou can also define the license for your project. It is not necessary if you don‚Äôt intend on sharing this code publicly, but it is a good practice to include a license. When it comes to research code, transparency and accessibility are key. I recommend opting for a permissive license, like the MIT License. This license allows others to freely use, modify, and distribute your work ‚Äì perfect for fostering open-source collaboration in the research community. GitHub makes it easy to include a license; just select the MIT License from the dropdown menu when creating your repository. Other permissive licenses include the BSD License and the Apache License.\n\n\n8.5.2.3 Adding a .gitignore file\nBefore you start adding files to your repo, consider setting up a .gitignore file. This file tells Git which files or folders to ignore in a project. Typically, you‚Äôll want to exclude certain files from being tracked ‚Äì like temporary files, local configuration files, files containing sensitive information, or large data files. GitHub offers templates for .gitignore files tailored to various programming languages and frameworks, which can be a great starting point. It is available as a dropdown menu option when creating the repository. gitignore.io is another useful resource for generating .gitignore files.\n\n\n8.5.2.4 Adding a README file\nFinally, you‚Äôll want to add a README.md file to your repository. This file is the first thing visitors will see when they visit your repository on GitHub. It‚Äôs an essential component of your project, acting as the introduction and guide. Use the README to explain what your project does, how to set it up, and how to use it. This is important even if your project is not public, as it will help you remember how to use your project in the future and facilitate onboarding new collaborators. This file can be written in plain text or formatted using Markdown, a lightweight markup language that is easy to learn and use. GitHub automatically renders Markdown files, making them easy to read and navigate. You can also include images, links, and code snippets in Markdown files. GitHub offers a handy guide to help you get started with Markdown.\n\n\n\n8.5.3 Cloning a repository\nCloning a repository creates a local copy of the remote repository on your computer. This allows you to work on the project locally and push your changes to the remote repository when you‚Äôre ready to share them with others. To clone a repository, you‚Äôll need the URL of the remote repository. You can find this by clicking on the green Code button on the repository‚Äôs homepage. If you are using GitHub Desktop, you can clone the repository by selecting Open with GitHub Desktop, which will open the repository in GitHub Desktop. You can then select the location where you want to store the repository on your computer and click Clone.\nIf you are not using GitHub Desktop, you can clone the repository using the command line. First, copy the URL of the repository from the repository‚Äôs homepage by clicking on the green Code button, then copying the URL by clicking on the clipboard icon next to the URL. To clone the repository, open the terminal and navigate to the directory where you want to store the repository. Then, run the following command:\ngit clone &lt;url&gt;\nThis will create a new directory with the same name as the repository and download all the files from the remote repository into this directory. You can then open this directory in VS Code and start working on the project. From GitHub Desktop, you can open the repository in VS Code by selecting Open in Visual Studio Code from the Repository menu.\n\n\n8.5.4 Tracking changes\nOnce you have cloned the repository, you can start making changes to the files in the repository. You can create new files, edit existing files, or delete files. You can also move files around or rename them. You can see all your changes in the Source Control tab in VS Code. Files will be listed under Changes with a U if they are new (untracked), a M if they have been modified, or a D if they have been deleted. You can also see the changes you have made to each file by clicking on the file name.\nWhen you create a new file, it will not be tracked by Git until you add it to the staging area. To add a file to the staging area, you use the Stage Changes button in the Source Control tab in VS Code (the little + sign next to a file when you hover over it). You need to do this not only for new files, but for all files that you have modified or deleted since the last commit. Files in the staging area be included in the next commit.\nOnce you have added one or many changed files to the staging area, you can commit those changes to the repository. To commit changes, you need to enter a commit message describing the changes you have made and then click on the Commit button in the Source Control tab in VS Code (the checkmark icon). You can also use the keyboard shortcut (Command+Enter on Mac or Ctrl+Enter on Windows or Linux) to commit your changes. This will create a new commit, i.e., a new snapshot, with the changes you have staged.\nYou can see all your commits in the Source Control tab under Commits. You can click on a commit to see the changes that were made in that commit. You can also right-click on the commit to access the commit details, including the commit message, the author, and the date and time of the commit.\n\n\n8.5.5 Syncing with the remote repository\nAfter committing your changes locally in Visual Studio Code, the next step is to synchronize these changes with your remote repository on GitHub. This process involves two main actions: pulling changes from the remote repository and pushing your local changes to the remote.\n\n8.5.5.1 Pulling changes from the remote repository\nBefore you push your changes, it‚Äôs a good practice to pull any updates that others might have made to the remote repository. This ensures that your local repository is up-to-date. In VS Code, you can pull changes by clicking on the ... (more actions) button in the Source Control tab and selecting Pull. Alternatively, you can use the keyboard shortcut (Command+Shift+P on Mac or Ctrl+Shift+P on Windows/Linux) and type Git: Pull in the command palette. Pulling changes will merge updates from the remote repository into your local branch. If there are no conflicts, the merge will happen automatically.\n\n\n8.5.5.2 Pushing changes to the remote repository\nOnce your local branch is up-to-date and you‚Äôve committed your changes, you‚Äôre ready to push these changes to the remote repository. In the Source Control tab, click on the ... button and select Push. This will upload your commits to the remote repository on GitHub. You can also use the keyboard shortcut (Command+Shift+P on Mac or Ctrl+Shift+P on Windows/Linux) and type Git: Push in the command palette. If you‚Äôre pushing to a branch that doesn‚Äôt exist on the remote, VS Code will automatically create this branch in the remote repository.\n\n\n8.5.5.3 Resolving merge conflicts\nOccasionally, when you pull changes from the remote repository, you may encounter merge conflicts. These occur when changes in the remote repository overlap with your local changes in a way that Git can‚Äôt automatically resolve. VS Code provides tools to help resolve these conflicts. Conflicted files will be marked in the Source Control tab. You can open these files and choose which changes to keep. After resolving conflicts, you‚Äôll need to stage and commit the merged files before pushing.\nRegularly pulling and pushing changes will keep your local and remote repositories synchronized. This is crucial in collaborative projects to ensure everyone is working with the most current version of the project.\n\n\n\n8.5.6 Branching and merging\nBefore using Git, whenever I wanted to try something new in my code, I would make a copy of the entire project folder and work on that copy. This was a tedious process, and it was easy to lose track of which version was the most recent. With Git, branching makes this process much easier. Branching allows you to create a copy of your project, called a branch, and work on that branch without affecting the main project. Once you‚Äôre satisfied with the changes you‚Äôve made in your branch, you can merge those changes back into the main project. This process is much more efficient and less error-prone than manually copying and pasting files.\n\n8.5.6.1 Creating a New Branch\nIn VS Code, you can create a new branch by clicking on the branch name in the bottom left corner, then selecting Create new branch.... Give your branch a descriptive name that reflects its purpose. You can switch between branches by clicking on the branch name in the bottom left corner and selecting the branch you want to work on.\nAfter creating and switching to your new branch, any changes you make are confined to that branch. You can stage and commit changes in this branch as you would in the main branch.\nYou can also choose to publish your branch to the remote repository. This will create a copy of your branch on GitHub. This is useful if you want to collaborate with others on this branch, or to use GitHub to backup the branch. Note that once the branch is published, others who have access to the repository will be able to see that branch. To publish your branch, click on the ... button in the Source Control tab and select Publish Branch....\n\n\n8.5.6.2 Merging Branches\nOnce you‚Äôve completed the work in your branch and you‚Äôre satisfied with the changes, you‚Äôll want to merge these changes back into the main branch. Before merging, ensure your branch is up-to-date with the main branch. You can do this by checking out the main branch and pulling the latest changes, then switching back to your branch and merging the main branch into it. After that, you are ready to merge your branch into the main branch. After merging, you can delete your branch if you no longer need it. This avoids cluttering the repository with branches that are no longer needed.\nIn the Source Control tab, click on the ... button, select Merge Branch..., and choose the branch you want to merge into your current branch. If there are no conflicts, VS Code will complete the merge. VS Code will also ask you if you want to delete the merged branch.\nMerge conflicts happen when the same lines of code have been changed differently in both branches. VS Code will notify you if there are conflicts that need resolution. The first time, Git will also need you to confiure how you want to handle merge conflicts by entering one of the following commands in the terminal:\n\ngit config pull.rebase false: This command sets the pull behavior to merge. When you pull from a remote repository, Git will merge any incoming commits with your current branch. This is the one I usually use.\ngit config pull.rebase true: This command sets the pull behavior to rebase. Instead of merging incoming commits, Git will reapply your local commits on top of the incoming commits, creating a linear commit history.\ngit config pull.ff only: This command sets the pull behavior to fast-forward only. Git will only update your branch if it can fast-forward, meaning the main branch has not changed since you created your branch. If the main branch has new commits, Git will not pull the changes and you‚Äôll need to manually merge or rebase.\n\nConflicted files will be marked in the Source Control tab. Open these files, and VS Code will highlight the conflicting changes. Choose which changes to keep, then save the file, stage, and commit the resolved files. Once all conflicts are resolved and changes are committed, the branches are successfully merged. If you‚Äôve merged into your local main branch, don‚Äôt forget to push these changes to the remote repository to keep everything synchronized.\n\n\n\n8.5.7 Pull requests and code reviews\nA pull request (PR) is a method in GitHub to propose changes from one branch to another, typically from a feature into the main branch. It‚Äôs a request to pull in your changes. The name is a bit misleading because it‚Äôs not related to the pull command in Git. You can think of it as a ‚Äúmerge request‚Äù instead. When you create a PR, you‚Äôre initiating a discussion about your proposed changes. Your collaborators can review the code, leave comments, request changes, or approve the PR.\n\n8.5.7.1 Creating a Pull Request in GitHub\nOnce you have pushed your branch to the remote repository, you can create a PR. Navigate to the repository on GitHub.com. GitHub often shows a prompt to create a PR for recent branches. If not, go to the Pull Requests tab and click New pull request. Select your branch and the branch you want to merge into (usually the main branch). When creating a PR, include a clear title and a detailed description of the changes. This helps reviewers understand the context and purpose of the changes. You can also assign reviewers to the PR, add labels, and set a milestone. Once you‚Äôre satisfied with the PR, click Create pull request. Any assigned reviewers will be notified of the PR and can begin reviewing it.\n\n\n8.5.7.2 Code Reviews\nCollaborators can review the changes in a PR by navigating to the Files changed tab within the PR. Reviewers can leave comments on specific lines of code, general comments on the PR, and suggest changes. They can also pull the branch locally and test the changes themselves. Once the review is complete, the reviewer can approve the PR, request changes, or leave a comment. If changes are requested, the PR author can make the requested changes and push them to the branch. The PR will be automatically updated with the new changes. Once the PR is approved, it can be merged into the target branch. Based on the feedback, you might need to make additional commits to your branch. These updates will automatically appear in the PR. This back-and-forth can continue until the changes are satisfactory.\n\n\n8.5.7.3 Merging the Pull Request\nOnce the PR is approved and any conflicts are resolved, you can merge it into the target branch. This is typically done via the ‚ÄòMerge pull request‚Äô button on GitHub. After merging, it‚Äôs a good practice to delete the feature branch from the remote repository to keep the branch list tidy.\n\n\n8.5.7.4 Best Practices for Pull Requests and Code Reviews\n\nSmall, Focused Changes: Aim for smaller, manageable PRs that focus on a specific feature or fix. This makes code reviews more efficient and less overwhelming.\nClear Communication: Use clear, descriptive messages in both your PRs and commits. This helps reviewers understand your thought process and the changes made.\nConstructive Feedback: When reviewing, offer constructive and respectful feedback. Code reviews are not just about finding mistakes but also about sharing knowledge and improving the codebase collaboratively.\n\nPull requests and code reviews are vital for maintaining high-quality code and fostering collaboration in your finance research projects. While not yet commonly used in academia, I have found them the perfect tools for collaborating on research projects. They ensure that every change is scrutinized and understood by all collaborators, and they foster a culture of peer review and collective improvement as the project progresses.\nGitHub offers many other features that can be useful for research projects. I list them at the end of this post and will cover them in a future post.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#github-for-research-code",
    "href": "python/git/index.html#github-for-research-code",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.6 GitHub for research code",
    "text": "8.6 GitHub for research code\nIn empirical finance research, the ability to reproduce results is more important now than ever, especially that most top journals require authors to share their code and data. In this section, I will discuss some best practices I have adopted for using GitHub to manage research code, with an emphasis on reproducibility, documentation, and effective use of GitHub‚Äôs features.\nThe first thing to consider after creating a new repository is the structure of your project. A well-organized project is easier to navigate and understand, and it makes it easier for others to reproduce your work. There is no one-size-fits-all approach to organizing a project, but the following project structure is a good starting point:\nproject\n‚îú‚îÄ‚îÄ data/\n‚îú‚îÄ‚îÄ docs/\n‚îú‚îÄ‚îÄ output/\n    ‚îú‚îÄ‚îÄ figures/\n    ‚îî‚îÄ‚îÄ tables/\n‚îú‚îÄ‚îÄ src/\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ .env\n‚îú‚îÄ‚îÄ .env-example\n‚îú‚îÄ‚îÄ conf.yaml\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ uv.lock\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ requirements.txt\nSo, which files should you commit to your repository? Here are some guidelines:\nYou should include:\n\nConfiguration files: Files like .json, .yml, or .ini are crucial for ensuring that your project can be set up and run by others with the exact same parameters you used. In my example the conf.yaml file contains the configuration parameters for the project and should be included in the repository.\nSource code: Include all scripts and code files that are essential for your analysis or model. In my example, the src directory contains all the Python scripts used in the project.\nDocumentation: Any files that help explain your project, especially markdown files with notes. In my example, the docs directory contains the documentation for the project and should be included in the repository. If your documentation is generated from source files, such as Markdown or Latex, then you should include the source files in the repository, not the generated files. Your repository should also include a README file at the root of the repository that provides an overview of your project, its purpose, and how to use it.\nDependencies: For projects in languages like Python, a file listing the dependencies is essential. This file lists all the external libraries and their specific versions needed for your project. This ensures that anyone cloning your repository can easily install the necessary dependencies and run your code in an environment identical to yours. In my example, I use uv to manage dependencies, so I include the pyproject.toml and uv.lock files. The lock file records the exact versions of all dependencies, ensuring reproducible environments across machines. I also include a requirements.txt file for users who prefer to install dependencies using pip instead of uv.\n.gitignore file: This file tells Git which files or folders to ignore in a project. Typically, you‚Äôll want to exclude certain files from being tracked ‚Äì like temporary files, local configuration files, files containing sensitive information, or large data files. GitHub offer templates, but they seem to be missing a few things. For example, if you are on Mac you will want to add .DS_Store to your ignore file. gitignore.io is another useful resource for generating .gitignore files that are much more comprehensive. Make sure to also add the .gitignore file to your repository.\n\nFinally, make sure that you include in the .gitignore file all the files and directories that you should not include in the repository.\nYou should not include:\n\nData files: While large datasets might not be feasible to store on GitHub, even small datasets can be problematic if they are updated often. Instead, consider including sample datasets or scripts that automatically fetch or generate data, or sharing your data among collaborators using a cloud storage service like Dropbox or Google Drive. There exist tools like DVC that can help you manage large datasets with version control, but I have not used them myself.\nSensitive data and local configuration files: Do not include sensitive information like passwords or API keys, or computer-specific configuration parameters such as local paths. Instead, you should include an example file with the expected parameters that need to be set in the configuration file. In my example, I use a .env file to store sensitive and local information, and I include a .env-example file that contains the name of the environment variable that needs to be set in the .env file. I would then include the .env-example file in the repository, but not the .env file. I also include the .env file in the .gitignore file so that it is not included in the repository.\nOutput: You should not include output files in the repository. Instead, you should include the code that generates the output files. Every collaborator should be able to generate the results in his environment. In my example, the output directory contains the figures and tables generated by the code in the src directory.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#git-and-jupyter-notebooks",
    "href": "python/git/index.html#git-and-jupyter-notebooks",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.7 Git and Jupyter notebooks",
    "text": "8.7 Git and Jupyter notebooks\nJupyter Notebooks are a popular tool for data analysis and visualization. They allow users to combine code, text, and visuals in one document, making it easy to share and collaborate on data science projects. However, Jupyter Notebooks have many shortcomings when it comes to replicability and using them with Git can be challenging.\n\n8.7.1 Challenges with Git and Jupyter notebooks\nJupyter Notebooks, while an excellent tool for data analysis and visualization, present unique challenges when used with Git. The core issue lies in their format: Notebooks save both the input (code) and the output (results, graphs, etc.) in a single JSON file. This means that even small changes in the code can lead to large changes in the file, making it difficult for Git to handle diffs and merges effectively. The output sections, especially those with visual content, can create ‚Äúnoise‚Äù in version control. When different users run the same notebook, slight differences in output can appear, leading to unnecessary conflicts. Because Git keeps track of the full history of the notebook, the size of the repository can grow quickly, especially if the notebook contains large outputs such as images. This can make it difficult to share and collaborate on notebooks.\n\n\n8.7.2 VS Code Notebook Diff Viewer\nRecognizing these challenges, tools like Visual Studio Code have introduced features to help. The VS Code diff viewer (the tool that shows differences in files due to changes) supports Jupyter notebooks, allowing users to compare and understand changes between notebook versions more easily. This tool provides a clearer visualization of differences in the code, reducing the complexity involved in tracking changes in notebooks in Git.\n\n\n8.7.3 Using Notebooks with Online Platforms\nDespite these challenges, Jupyter Notebooks remain a popular and powerful tool for data analysis and research. Their interactive nature and the ability to combine code, text, and visuals in one document make them invaluable.\nPlatforms like Binder and Google Colab integrate well with Jupyter Notebooks hosted on GitHub. These platforms can automatically create interactive, shareable environments from notebooks, making them more accessible for collaborative work and education. By using these platforms, researchers can share their notebooks in a more user-friendly and interactive format, ensuring that others can easily replicate and experiment with their findings.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#github-for-writing",
    "href": "python/git/index.html#github-for-writing",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.8 GitHub for writing",
    "text": "8.8 GitHub for writing\nGitHub is not just for code; it‚Äôs also an excellent platform for tracking your writing, especially if you are using formats based on plain-text files such as Markdown (like the Quarto publishing system) and LaTeX.\nThe same principles for organizing code projects apply to writing projects. You should include all the files that are essential to generate the output of your project, such as the source (e.g.¬†.md, .tex, and .bib) files, configuration files, and tables and figures. You should avoid including the output files, such as PDFs, or HTML files.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#tagging-releases-for-milestones",
    "href": "python/git/index.html#tagging-releases-for-milestones",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.9 Tagging releases for milestones",
    "text": "8.9 Tagging releases for milestones\nThere are times when you want to create a snapshot of your project, including the output, at a specific point in time. For example, when you submit a paper to a journal, you want to create a snapshot of the project at that point in time. This allows you to keep track of the changes made in between revisions. GitHub provides a way to do this using tags and releases.\nWhen you reach a significant milestone in your writing ‚Äì such as the completion of a draft, submission to a journal, or final revisions ‚Äì you can create a tag and a corresponding release.\nTo create a tag and release, head to the repository on GitHub.com and click on Create a new release under Releases in the right sidebar. Enter a tag version number and a title for the release. You can also add release notes summarizing the changes or updates in this version. Finally, attach the output files (e.g.¬†PDFs) to the release. Click Publish release to create the release. You can then download the release files or share the release link with others.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#publishing-your-code-on-github",
    "href": "python/git/index.html#publishing-your-code-on-github",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.10 Publishing your code on GitHub",
    "text": "8.10 Publishing your code on GitHub\nIn empirical finance academic research, sharing your code has become increasingly important. Publishing your code enhances the transparency and reproducibility of your research. It allows peers to review, replicate, and build upon your work, contributing to the collective knowledge of the field. Making your code available can also increase the citation and impact of your research, as it provides tangible artifacts that others can use and reference. Finally, it is also a requirement for publishing in many journals, including the top ones.\nJournals will publish your code alongside your paper, so why should you also publish it on GitHub?\nFor me, the main reason is to keep control over my code. By publishing your code on GitHub, you retain control over it. You can continue to make changes to it, and update it as needed. Other researchers who visit your GitHub repository can also be exposed to your other work, increasing the visibility of your research. Finally, GitHub offers a platform for collaboration and feedback, allowing others to flag issues, contribute to your work, and build upon it.\nTo publish your code on GitHub, all you need to do is set the visibility of your repository to public. If you don‚Äôt want to share the full history of your code, you can create a new repository and upload the latest version of your code.\nMake sure to include a README file that explains what your project does, how to set it up, and how to use it. Documentation is key to making your code accessible to others (and to reducing the number of questions you get about your code). You can also include instructions on how to cite your code in the README file. Finally, you should also include a LICENSE file to clearly state how others can use your code.\nOnce you have completed these steps, your code is published! If you want a DOI for your repository, you can use Zenodo, which allows you to mint a DOI for your GitHub repository.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#other-github-features-for-academic-researchers",
    "href": "python/git/index.html#other-github-features-for-academic-researchers",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.11 Other GitHub features for academic researchers",
    "text": "8.11 Other GitHub features for academic researchers\nIn addition to the core features of GitHub, many other tools and functionalities can be useful for academic researchers. I plan on covering most of them in future posts. Here are the ones that I use the most:\n\n8.11.1 Project Management Tools\nGitHub offers several tools to help you manage your projects, including Projects, Issues, Discussions, and Wikis. These tools can be used to organize your work, track tasks, and collaborate with others.\n\n\n8.11.2 GitHub Copilot\nGitHub Copilot is an AI coding assistant. It can do code completion, suggest functions, and even generate code based on comments. There is also a Copilot Chat powered by GPT-4 that can answer questions about code while being aware of the context. When you allow it, it can consult your private repositories to provide more relevant suggestions.\nSeriously, if you haven‚Äôt tried it yet, you should. It‚Äôs a game-changer, and new features are being added all the time. And it will work for text too if you write your Markdown or LaTeX files in VS Code.\n\n\n8.11.3 GitHub Pages\nGitHub Pages is a free service that allows you to host static websites directly from GitHub. This can be useful for hosting project websites, blogs, or personal websites. My personal website and this blog are both hosted on GitHub Pages.\n\n\n8.11.4 GitHub Classroom\nGitHub Classroom simplifies the use of GitHub in classroom settings. It‚Äôs a toolset that automates the repetitive tasks involved in grading and feedback, making it easier to use GitHub for coursework and assignments in a research or academic context. I have been using it for three years and it has been a game-changer for me. While it‚Äôs not bug-free, it has saved me countless hours of grading and feedback. Automated grading has a monthly limit after which you need to pay, but the cost is minimal and well worth it.\n\n\n8.11.5 GitHub Actions\nGitHub Actions is a powerful tool that allows you to automate workflows. You can set up CI/CD pipelines2 to automate testing, building, and deploying your applications or research code. GitHub Actions are small scripts that run in response to events in your repository, such as commit or pull requests. For researchers, it can be used to automate the testing of code or even automate routine data processing tasks.\n\n\n8.11.6 GitHub Codespaces\nGitHub Codespaces provide a fully featured cloud development environment accessible directly from GitHub. Your code lives in a remote server, and you get a complete VS Code environment in your browser. This can be particularly useful for researchers who want to quickly experiment with code or collaborate without the need to set up a local development environment. It is also great to ensure maximum replicability of the code you distribute, as the environment is identical for everyone.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#footnotes",
    "href": "python/git/index.html#footnotes",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "",
    "text": "Historically, this branch was called master, but GitHub has recently changed the default branch name to main to avoid the racially charged connotations of the word master.‚Ü©Ô∏é\nContinuous integration and continuous development‚Ü©Ô∏é",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html",
    "href": "python/ai-for-coding/index.html",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "",
    "text": "9.1 Why This Chapter Exists\nArtificial intelligence tools have transformed how programmers write code. Large language models can explain syntax, generate functions from natural language descriptions, and debug error messages in seconds. For students learning Python for empirical finance research, these tools offer an appealing shortcut: why struggle with cryptic error messages when an AI can explain them? Why write boilerplate code by hand when an AI can generate it instantly?\nThis chapter addresses a fundamental tension. AI coding tools can genuinely accelerate your work and reduce frustration, but they can also undermine the skill formation that makes you a capable researcher. The difference between these outcomes depends entirely on how you use these tools. Used wisely, AI becomes a force multiplier for your growing expertise. Used carelessly, it becomes a crutch that prevents you from developing the understanding you need.\nWe begin with the core trade-offs, then cover the practical landscape of AI coding tools, and conclude with a workflow designed to help you benefit from AI assistance while still building genuine programming competence.\nThe premise of this chapter is straightforward: AI can accelerate learning, but it cannot replace it. If you cannot read, understand, and debug the code that AI produces, you are flying blind. Learning Python remains non-negotiable for credible empirical research. AI shifts where your effort goes, not whether effort is required.\nConsider what happens when an AI writes code for you. The tool produces syntactically correct Python that might even run without errors on your first attempt. But what happens when you need to modify that code for a different dataset? What happens when it produces incorrect results that look plausible? What happens when a collaborator asks you to explain your methodology? If you cannot answer these questions, the AI-generated code is a liability rather than an asset.\nThe goal of this chapter is not to discourage you from using AI tools. They are genuinely useful, and most professional programmers now use them in some capacity. The goal is to help you use these tools in ways that build rather than erode your capabilities. This requires understanding what AI tools can and cannot do, recognizing when to rely on them and when to step back, and developing habits that keep you in control of your own code.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#why-this-chapter-exists",
    "href": "python/ai-for-coding/index.html#why-this-chapter-exists",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "",
    "text": "WarningAI does not understand your research\n\n\n\nAI coding assistants have no conception of what makes empirical research valid. They cannot distinguish between code that runs and code that correctly implements your methodology. They cannot verify that your variable definitions match your research design. They cannot ensure that your sample selection avoids look-ahead bias. These judgments require domain expertise that only you can provide.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#ai-skill-formation-and-career-constraints",
    "href": "python/ai-for-coding/index.html#ai-skill-formation-and-career-constraints",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.2 AI, Skill Formation, and Career Constraints",
    "text": "9.2 AI, Skill Formation, and Career Constraints\nThe most important reason to develop genuine Python skills, even when AI tools are available, is that over-reliance on AI slows long-run skill acquisition. Programming proficiency comes from struggling with problems, making mistakes, and building mental models of how code works. When AI removes that struggle, it also removes the learning.\nThink about the difference between using a calculator for arithmetic and understanding how arithmetic works. Calculators are faster and more reliable for computation, but if you never learned arithmetic, you cannot estimate whether an answer is reasonable. You cannot catch errors. You cannot extend the calculation in ways the calculator does not support. The same dynamic applies to AI and coding. AI can produce code faster than you can type it, but if you never developed the underlying understanding, you cannot evaluate, modify, or debug what it produces.\n\n9.2.1 The industry reality\nMany financial institutions restrict or prohibit external AI tools. Banks, asset managers, and hedge funds handle sensitive data and proprietary strategies. Sending code snippets or error messages to external AI services creates compliance and security risks that many firms will not accept. Some institutions run air-gapped systems where internet access is simply unavailable. Others have policies that prohibit sharing any code or data with third parties, including AI providers.\nIf you are targeting industry roles after your degree, this matters. Code you write with AI assistance still needs to be maintained without it. When you join a firm with restrictive policies, you need to be able to read, understand, and modify code on your own. The skills you develop now determine whether you will be effective in those environments.\nEven in academic settings, AI dependence creates problems. Universities and journals increasingly have policies that restrict or require disclosure of AI use in research. Peer reviewers may ask you to explain or modify your methodology. Collaborators may need to extend your code. You may need to debug issues years after the original analysis. In all these cases, you need genuine understanding of what your code does and why.\n\n\n9.2.2 Building transferable skills\nThe bottom line is that AI is a supplement, not a crutch. The goal is to develop skills that remain valuable regardless of what tools are available. This means understanding Python syntax, data structures, and control flow well enough to write code from scratch when necessary. It means being able to read and understand code you did not write. It means having mental models of how your analysis works that go beyond the specific implementation.\nWhen you use AI tools, pay attention to whether you are learning or just producing output. If you find yourself repeatedly asking the AI to solve similar problems, that is a signal to step back and build understanding. If you cannot explain what the AI-generated code does, you should not use it.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#ethical-academic-and-scientific-constraints",
    "href": "python/ai-for-coding/index.html#ethical-academic-and-scientific-constraints",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.3 Ethical, Academic, and Scientific Constraints",
    "text": "9.3 Ethical, Academic, and Scientific Constraints\nBefore discussing how to use AI tools effectively, we need to address the constraints that govern their use in academic and professional settings. These are not optional guidelines. They are requirements that determine whether your work is acceptable.\n\n9.3.1 Academic integrity and authorship\nThe fundamental principle is simple: responsibility for correctness always lies with the researcher. When you submit code as part of a research project or assignment, you are asserting that the code correctly implements your methodology and that you understand what it does. AI assistance does not change this responsibility. If AI-generated code contains errors that invalidate your results, you bear the consequences.\nDifferent institutions and courses have different policies about AI use. Some prohibit it entirely for certain assignments. Others permit it with disclosure requirements. The specific rules vary, but the underlying principle is consistent: you must be able to explain and defend every line of code you submit. If you cannot, the work is not genuinely yours, regardless of how it was produced.\n\n\n9.3.2 Reproducibility and auditability\nEmpirical research must be reproducible. Other researchers should be able to examine your methodology, run your code, and verify your results. This requires that your code be inspectable and explainable. AI-generated code that you do not understand fails this requirement, even if it produces the correct output.\nWhen reviewers or collaborators ask about your implementation, you need to be able to explain why you made specific choices. Why did you winsorize at the 1st and 99th percentiles rather than the 5th and 95th? Why did you cluster standard errors at the firm level rather than the industry level? These questions require understanding that goes beyond the code itself to the research design it implements.\n\n\n9.3.3 Data confidentiality\nNever paste proprietary data, credentials, or restricted information into AI tools. This applies to actual data values, but also to code that reveals the structure of proprietary datasets or the logic of trading strategies. Most AI tools send your input to external servers for processing. Even tools that claim privacy protections may retain data for training or analysis.\nIn practice, this means being careful about what you share. Error messages that include data values should be sanitized before sharing. Code that implements proprietary methodology should be described in general terms rather than pasted directly. When in doubt, assume that anything you share with an AI tool could become public.\n\n\n\n\n\n\nCautionA simple rule for submissions\n\n\n\nIf you would not submit work without AI, you should not submit it with AI. AI can help you produce better work faster, but it cannot make you capable of work you could not otherwise do. When you submit code, you are asserting that you could have written it yourself given sufficient time. If that is not true, the submission is not appropriate.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#mental-models-for-using-ai-effectively",
    "href": "python/ai-for-coding/index.html#mental-models-for-using-ai-effectively",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.4 Mental Models for Using AI Effectively",
    "text": "9.4 Mental Models for Using AI Effectively\nThe key to using AI tools effectively is having the right mental model for what they are and what they can do. This section provides frameworks for thinking about AI that will help you use it productively, especially as a beginner.\n\n9.4.1 AI as a junior assistant\nThe most useful mental model is to think of AI as a junior assistant that writes drafts quickly and carelessly. Like a junior employee, AI can produce output fast, but that output requires review and correction. The assistant has broad knowledge but limited judgment. They will confidently produce work that contains subtle errors. They will follow instructions literally even when those instructions are ambiguous or misguided.\nThis mental model has several implications. First, you should expect to review and edit everything the AI produces. Just as you would not submit a junior employee‚Äôs first draft, you should not use AI-generated code without careful examination. Second, the quality of your instructions matters enormously. Vague requests produce vague outputs. Specific, well-structured prompts produce better results. Third, you remain responsible for the final product. The assistant helps, but you make the decisions.\n\n\n9.4.2 Why precise prompts matter\nPrecise prompts matter more than model choice. The difference between a good prompt and a bad prompt often exceeds the difference between AI models. A well-crafted prompt includes context about what you are trying to accomplish, specific requirements for the output, and examples of what success looks like. A poor prompt leaves the AI guessing about your intentions.\nConsider the difference between these prompts:\n\nPoor: ‚ÄúWrite code to calculate returns‚Äù\nBetter: ‚ÄúWrite a Python function that calculates simple returns from a list of prices. The function should take a list of floats representing daily closing prices and return a list of floats representing daily returns. Handle the edge case of the first day, which has no previous price, by returning None for that position.‚Äù\n\nThe second prompt specifies the input format, output format, programming language, and edge case handling. It gives the AI enough information to produce something useful. The first prompt leaves all these decisions to the AI, which may or may not match your needs.\n\n\n9.4.3 The iteration loop\nEffective AI use follows an iteration loop: prompt, inspect, test, revise. You start with a prompt describing what you need. You inspect the output to understand what the AI produced. You test the code to verify it works correctly. You revise either the prompt or the code based on what you learned.\n\n\n\n\n\n\nflowchart LR\n    A[Prompt] --&gt; B[Inspect]\n    B --&gt; C[Test]\n    C --&gt; D{Works?}\n    D --&gt;|No| E[Revise]\n    E --&gt; A\n    D --&gt;|Yes| F[Done]\n\n\n\n\nFigure¬†9.1: The AI-assisted coding iteration loop\n\n\n\n\n\nThis loop emphasizes that AI assistance is not a one-shot process. You should expect multiple iterations, especially for complex tasks. Each iteration teaches you something about both the problem and the AI‚Äôs capabilities. Over time, you learn to write better prompts and to anticipate where AI is likely to make mistakes.\n\n\n9.4.4 Treat every response as a hypothesis\nPerhaps the most important mental model is to treat every AI response as a hypothesis, not an answer. The AI is proposing a solution that might be correct. Your job is to test that hypothesis by reading the code, understanding what it does, and verifying that it behaves correctly.\nThis mindset prevents the most dangerous failure mode: accepting AI output without verification. When you treat AI responses as hypotheses, you naturally engage your critical faculties. You ask questions. You check assumptions. You test edge cases. This engagement is precisely what builds understanding.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#general-purpose-chatbots-for-research-coding",
    "href": "python/ai-for-coding/index.html#general-purpose-chatbots-for-research-coding",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.5 General-Purpose Chatbots for Research Coding",
    "text": "9.5 General-Purpose Chatbots for Research Coding\nGeneral-purpose AI chatbots are the most familiar category of AI coding tools. These are web-based interfaces where you type questions in natural language and receive conversational responses. The major tools in this category include OpenAI ChatGPT, Anthropic Claude, Google Gemini, and Microsoft Copilot in chat mode. Each has different strengths and pricing, but for most research coding tasks, the differences are less important than how you use them. For research coding, they serve as explanation-first tools: they excel at helping you understand rather than at producing production-ready code.\n\n9.5.1 What they are good at\nChatbots excel at explaining syntax and error messages. When you encounter a Python error that you do not understand, a chatbot can explain what went wrong in plain language. It can describe what the error message means, why it occurred, and how to fix it. This is often faster and more helpful than searching documentation or Stack Overflow.\nThey are also good at translating ideas into rough code sketches. If you have a conceptual understanding of what you want to do but are unsure how to express it in Python, a chatbot can provide a starting point. The code may need refinement, but it gives you something concrete to work with and learn from.\nClarifying library usage is another strength. Python has thousands of packages with varying documentation quality. A chatbot can explain how to use a specific function, what parameters it accepts, and what it returns. It can compare alternatives and explain trade-offs. This is particularly helpful when learning new libraries.\n\n\n9.5.2 What they are bad at\nChatbots are poor at designing correct empirical workflows. They do not understand your research question, your data structure, or the methodological requirements of your field. The most recent models, especially those with reasoning capabilities, are improving in this area, but not yet to the point where you can trust them to be reliable without verification. They can produce code that runs but implements the wrong analysis. Detecting these errors requires domain expertise that the chatbot lacks.\nThey also struggle with understanding your project context by default. Each conversation with a chatbot starts fresh. The AI does not know what code you have already written, what data you are working with, or what you have tried before. You need to provide this context explicitly, which is time-consuming and error-prone.\n\n\n9.5.3 Typical research uses\nFor empirical finance research, chatbots are most useful for:\n\nDebugging tracebacks: Paste an error message and ask for an explanation. The chatbot can often identify the problem and suggest fixes.\nRefactoring simple logic: Describe what your current code does and ask for a cleaner implementation. Review the suggestion carefully before adopting it.\nLearning new concepts: Ask for explanations of Python features, statistical methods, or library functions. Use the chatbot as an interactive tutorial.\nGenerating boilerplate: Request template code for common patterns like reading CSV files or setting up matplotlib figures. Customize for your specific needs.\n\n\n\n\n\n\n\nTipUsing chatbots for learning\n\n\n\nChatbots make excellent tutors. When you do not understand a piece of code, ask the AI to explain it line by line. When you are confused about a concept, ask for multiple examples at different difficulty levels. When you get stuck on a problem, ask for hints rather than solutions. This approach builds understanding rather than dependence.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#in-editor-code-completion-and-inline-assistance",
    "href": "python/ai-for-coding/index.html#in-editor-code-completion-and-inline-assistance",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.6 In-Editor Code Completion and Inline Assistance",
    "text": "9.6 In-Editor Code Completion and Inline Assistance\nIn-editor completion tools provide a different kind of AI assistance. Rather than conversational interaction, they offer continuous, low-friction suggestions as you type. The major tools in this category are GitHub Copilot, which integrates with Visual Studio Code and other editors, and Cursor, an editor with deep AI integration built in. Cursor is a fork of VS Code, so it offers a familiar interface and supports most VS Code extensions, excluding those published by Microsoft. These tools analyze your current file and context to predict what you are likely to write next, then offer to complete it for you.\nIf you have used auto-complete on your phone, you already understand the basic experience. The AI predicts what you are likely to type next and offers to complete it for you. Most of the time the suggestions are helpful and save keystrokes. But occasionally the AI makes silly mistakes, inserting words that are grammatically correct but contextually wrong. Code completion works the same way: usually helpful, occasionally nonsensical, and always requiring your judgment about whether to accept.\n\n9.6.1 How completion differs from chat\nThe interaction model is fundamentally different from chatbots. With a chatbot, you formulate a request, submit it, wait for a response, then evaluate and possibly revise. With completion tools, suggestions appear automatically as you type. You accept with a keystroke or ignore by continuing to type. The feedback loop is measured in seconds rather than minutes.\nThis difference has important implications. Completion tools are embedded in your normal workflow. You do not need to context-switch to another application. The AI sees your entire file and often your entire project, so it has much more context than a chatbot conversation. Suggestions are smaller and more incremental, completing lines or functions rather than generating entire programs.\n\n\n9.6.2 Why this is often safer for beginners\nIn-editor completion is often safer for beginners because it produces smaller changes that are easier to evaluate. When a chatbot generates a 50-line function, understanding and verifying it requires significant effort. When a completion tool suggests finishing the line you are already writing, you can evaluate it immediately. The suggestion is within the context you already understand.\nThe immediate visual context also helps. You see the suggestion alongside your existing code. You can compare it to what you were planning to write. Obvious errors are more apparent because they conflict with the surrounding code that you wrote and understand.\n\n\n9.6.3 Common risks\nThe primary risk with completion tools is silent logical errors. The AI might suggest syntactically correct code that does not do what you intend. Because suggestions appear quickly and accepting them is easy, you might accept code without fully processing what it does. This is especially dangerous for statistical operations where the code runs without errors but produces incorrect results.\nOver-engineered suggestions are another issue. The AI might suggest more complex solutions than necessary, introducing abstractions or edge case handling that you do not need. This complexity makes the code harder to understand and maintain. Simpler code that you fully understand is usually better than sophisticated code that you do not.\n\n\n9.6.4 Improving suggestions with context\nCompletion tools work better when they have more context about your code. The AI analyzes your file and project to understand what you are trying to do, and richer context leads to better suggestions.\nType hints and docstrings, discussed in Chapter 4, serve double duty here. They make your code more readable for humans and more understandable for AI. When you declare that a function takes a pd.DataFrame and returns a float, the AI can suggest code that correctly uses DataFrame methods and returns an appropriate value. When you write a docstring explaining that a function calculates annualized volatility from daily returns, the AI understands the domain and can suggest relevant implementations.\nThis creates a virtuous cycle. Writing type hints and docstrings is good practice regardless of AI tools. But if you use completion tools, that investment pays additional dividends through better suggestions. The same practices that make your code maintainable also make it more AI-friendly.\nConversely, poorly documented code with no type hints gives the AI little to work with. Suggestions will be more generic and less likely to match your intent. If you find that completion suggestions are consistently unhelpful, consider whether adding context to your code might improve them.\n\n\n9.6.5 When to accept a suggestion\nA useful rule of thumb: accept a suggestion when you could have written it yourself, just more slowly. This standard ensures that you understand what you are accepting. The AI is saving you keystrokes, not doing your thinking.\nIf a suggestion surprises you, pause before accepting. Either you are learning something new (good), or the suggestion is wrong (bad). Either way, the surprise is a signal that you should engage more deeply rather than just accepting. Take time to understand why the AI suggested what it did.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#coding-agents",
    "href": "python/ai-for-coding/index.html#coding-agents",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.7 Coding Agents",
    "text": "9.7 Coding Agents\nCoding agents represent the most powerful and most dangerous category of AI coding tools. Unlike completion tools that suggest small changes, agents can make extensive modifications across multiple files. Unlike chatbots that produce isolated code snippets, agents can execute code, observe results, and iterate. The major tools in this category include GitHub Copilot in agent mode, Anthropic‚Äôs Claude Code, and OpenAI Codex. Both Claude Code and OpenAI Codex are available as command-line tools and as VS Code extensions. Cursor also includes deep agent integration. These tools evolve rapidly, so specific recommendations become outdated quickly, but at the time of writing, Claude Code with the Opus 4.5 model is my tool of choice.\nFor all AI coding tools, but especially agents, reading the product documentation is essential to achieve the best results. Each tool behaves differently, with its own strengths, limitations, and recommended workflows. Adapting your approach to the specific tool you use will greatly improve the quality of the output you get. Time spent learning the tool‚Äôs features and best practices pays off quickly.\n\n\n\n\n\n\nNote Video\n\n\n\nIn this video, I go through an example use case of Claude Code with a financial research application.\n\n\n\n\n9.7.1 What coding agents actually do\nAgents operate with a significant degree of autonomy. You describe a goal, and the agent takes a series of actions to achieve it: writing code, running tests, reading error messages, and revising its approach. Some agents can browse documentation, search codebases, and even interact with external services. They do in minutes what might take you hours.\nThis capability is transformative for experienced developers who can evaluate agent output quickly. An agent can implement a feature, write tests, and handle edge cases while the developer reviews and directs. The human remains in control but operates at a higher level of abstraction.\n\n\n9.7.2 Why they are dangerous for novice programmers\nFor novice programmers, agents are dangerous precisely because of their power. An agent might modify your code in ways you do not understand. It might introduce bugs that are difficult to detect because you do not know what was changed. It might solve problems in ways that work but that you cannot maintain or extend.\nThe core issue is verification. When an agent makes changes across multiple files, verifying correctness requires understanding all those files. A novice lacks this understanding. They may accept changes that seem to work but contain subtle errors. They may lose track of what their code does and why. They may find themselves unable to make further modifications without agent assistance.\n\n\n\n\n\n\nWarningThe danger of ‚Äúfix everything‚Äù requests\n\n\n\nNever ask an agent to ‚Äúfix everything‚Äù or ‚Äúmake it work‚Äù on code you do not understand. This request gives the agent maximum autonomy with minimum oversight. You have no way to evaluate whether the resulting changes are correct. Even if the code runs, it may not implement what you intended. Even if it produces plausible output, that output may be wrong.\n\n\n\n\n9.7.3 Appropriate research use cases\nAgents are appropriate when you already understand the code being modified. Useful applications include:\n\nRefactoring code you already understand: If you know what the code does and want to improve its structure, an agent can help. You can verify that the refactored version preserves the original behavior.\nGenerating tests or documentation drafts: Agents can produce initial test cases or docstrings that you then review and refine. The human provides the specification; the agent provides the implementation.\nApplying consistent changes across files: When you need to rename a variable, update a function signature, or apply a similar change in many places, agents can do this quickly and reliably.\nRapid prototyping: Agents can quickly produce working code to test whether an idea is feasible or to explore how a library works. Treat this as throw-away code for learning and experimentation, not as the foundation for your actual analysis.\n\n\n\n9.7.4 Inappropriate use cases\nAgents are inappropriate when you cannot verify their output:\n\nWriting core empirical logic: The code that implements your research methodology must be code you understand. Agents should not write this code for you.\nModifying unfamiliar code: If you do not understand code before the agent modifies it, you definitely will not understand it after.\nDebugging without understanding: When code does not work, the solution is understanding, not automation. An agent might fix the symptom while leaving the underlying problem.\n\n\n\n9.7.5 Version control is essential\nUsing version control with Git is not optional when working with coding agents. Agents make extensive changes across multiple files, and you need the ability to review those changes before accepting them and to revert if something goes wrong. Git‚Äôs diff tools let you see exactly what the agent modified, line by line. Without version control, you have no systematic way to understand or undo what the agent did.\nCommit frequently when working with agents. Before asking the agent to make changes, ensure your working directory is clean. After the agent finishes, review the diff carefully before committing. If the changes are wrong, you can discard them and try a different approach.\nFor advanced workflows, Git‚Äôs worktree feature lets you maintain multiple working directories from the same repository, each on a different branch. This enables running multiple agents in parallel on separate tasks without conflicts. Each agent works in its own worktree, and you merge the results when ready.\n\n\n\n\n\n\nNote Video\n\n\n\nFor a quick introduction to git worktrees, see this video.\n\n\n\n\n\n9.7.6 Security and permissions\nCoding agents require significant permissions to do their work. They need to read your files, write new code, and often execute commands. This power creates security risks that you should take seriously.\nBe cautious about what you allow agents to access. An agent with permission to execute arbitrary shell commands could potentially damage your system, exfiltrate data, or install malicious software. Most agents have safety measures, but these are not foolproof. Never run agents with elevated privileges, and be wary of granting access to sensitive directories or credentials.\nDevelopment containers, discussed in Chapter 10, provide an effective safety layer. By running agents inside a container, you isolate them from your main system. Even if something goes wrong, the damage is contained. The container can be reset to a clean state, and your host system remains protected. For any serious agent use, especially with less-established tools, running in a devcontainer is a sensible precaution.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#ai-assisted-workflow-for-research",
    "href": "python/ai-for-coding/index.html#ai-assisted-workflow-for-research",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.8 AI-Assisted Workflow for Research",
    "text": "9.8 AI-Assisted Workflow for Research\nThis section presents a workflow designed to help you benefit from AI assistance while maintaining understanding and control. The principles apply regardless of which specific tools you use.\n\n9.8.1 Start from human-written pseudocode\nBefore asking AI for help, write pseudocode describing what you want to accomplish. This forces you to think through the logic before delegating to AI. The pseudocode becomes both your specification for the AI and your reference for evaluating its output.\nPseudocode does not need to be syntactically correct Python. It should describe the steps of your computation in terms you understand:\n# Calculate portfolio returns for each month\n# For each month:\n#   Get the weights at the start of the month\n#   Get the stock returns during the month\n#   Multiply each weight by its corresponding return\n#   Sum to get portfolio return\n# Return the list of monthly portfolio returns\nWith this pseudocode, you can ask AI to help implement specific steps. You have a clear reference for whether the implementation matches your intention.\n\n\n9.8.2 Use AI to reduce syntax friction, not to invent logic\nAI should help you express ideas you already have, not generate ideas for you. If you do not know what analysis to run, AI cannot tell you. If you know what analysis to run but are unsure of the Python syntax, AI can help.\nThis distinction keeps you in the role of researcher rather than consumer of AI output. You make the methodological decisions. AI helps you implement those decisions efficiently. The resulting code reflects your understanding because it implements your logic.\n\n\n9.8.3 Run and test after every AI-assisted change\nNever accumulate multiple AI-generated changes before testing. After each change, run the code and verify it behaves correctly. Check edge cases. Compare results against manual calculations or known benchmarks. This immediate feedback catches errors early when they are easy to diagnose.\nIf you accept several AI suggestions before testing, and something goes wrong, you do not know which change caused the problem. You may need to undo everything and start over. Testing incrementally avoids this.\n\n\n\n\n\n\nWarningAI-generated tests require careful review\n\n\n\nAI tools are good at writing tests, which can accelerate your workflow. However, when AI writes both the code and the tests, you lose an important check on correctness. Review AI-generated tests carefully to ensure they actually test the right behavior and cover edge cases. A test that passes is only meaningful if the test itself is correct.\nBe especially careful with coding agents. When tests fail, agents have a tendency to modify the tests to make them pass rather than fixing the underlying code. This defeats the purpose of testing entirely. Always review diffs carefully to ensure the agent fixed the function, not the test.\n\n\n\n\n\n\n\n\nflowchart TB\n    A[Write pseudocode] --&gt; B[Implement one step]\n    B --&gt; C{AI help&lt;br&gt;needed?}\n    C --&gt;|Yes| D[Prompt AI with&lt;br&gt;specific request]\n    D --&gt; E[Review output&lt;br&gt;line by line]\n    E --&gt; F[Modify if needed]\n    F --&gt; G[Test immediately]\n    C --&gt;|No| G\n    G --&gt; H{Works&lt;br&gt;correctly?}\n    H --&gt;|No| I[Debug with&lt;br&gt;understanding]\n    I --&gt; B\n    H --&gt;|Yes| J[Commit to&lt;br&gt;version control]\n    J --&gt; K{More steps&lt;br&gt;remaining?}\n    K --&gt;|Yes| B\n    K --&gt;|No| L[Done]\n\n\n\n\nFigure¬†9.2: AI-assisted workflow for research coding\n\n\n\n\n\n\n\n9.8.4 Use version control aggressively between iterations\nCommit working code to version control before making AI-assisted changes. If the changes break something, you can easily revert. This safety net encourages experimentation while protecting against mistakes.\nGood commit messages document what you changed and why. When you later review your project history, you can see the evolution of your analysis. This is valuable for debugging, for collaboration, and for your own understanding. AI coding assistants are good at writing detailed commit messages that summarize the changes made. Just make sure you review the message before submitting the commit to ensure it accurately describes the work.\n\n\n9.8.5 Never merge AI-generated code you cannot explain line by line\nThe final check is simple: can you explain every line of the code? If someone asked why a particular line exists, could you answer? If the code stopped working, would you know where to look?\nIf you cannot answer these questions, you should not use the code. Take time to understand it, or ask the AI to explain it, or write it yourself. The short-term cost of slower progress is worth the long-term benefit of genuine understanding.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#local-and-open-models",
    "href": "python/ai-for-coding/index.html#local-and-open-models",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.9 Local and Open Models",
    "text": "9.9 Local and Open Models\nCloud-based AI tools are convenient but not always appropriate. This section covers alternatives that run on your own hardware, avoiding the need to send data to external servers.\nLocal models address several concerns that matter for research. Your prompts and code never leave your machine, eliminating concerns about data retention, training on your inputs, or security breaches at the provider. Organizations that prohibit external AI tools may permit local ones since the code stays within your controlled environment. Local inference has no per-token charges, so after the initial setup, usage costs only electricity. And local models work without internet access, which matters for air-gapped systems or unreliable connections.\nRunning AI models locally requires significant computing resources. Effective local models require Apple Silicon Macs or PCs with modern GPUs; older hardware often cannot run useful models at reasonable speed. Memory is usually the binding constraint, with larger models requiring more of it. On Apple computers, the system uses unified memory where RAM is shared between the CPU and GPU, so a machine with 32GB or more of RAM can run substantial models. On PCs, the relevant constraint is typically VRAM on the GPU rather than system RAM, since the model must fit in GPU memory to run efficiently. Model files themselves are large, often multiple gigabytes each, so keeping several models available requires substantial storage. Local AI also requires more technical setup than cloud services: you need to install software, download models, and configure integration with your editor.\nTwo popular options for running AI models locally are Ollama and LM Studio. Ollama is a command-line tool that makes running local models straightforward, handling model downloads, memory management, and inference. Many editor integrations support Ollama as a backend. LM Studio provides a graphical interface for downloading and running local models, making it more accessible for users uncomfortable with command-line tools, and includes a built-in chat interface.\nSeveral tools support local model backends for coding assistance. Some configurations of GitHub Copilot allow using local models instead of cloud APIs, though capabilities may be reduced. Open-source alternatives like Continue provide Copilot-like functionality with local model support. Command-line agents like Claude Code can also be configured with local model providers, though this requires additional setup.\nLocal models currently trail cloud models in capability. The most capable models require hardware that exceeds what most individuals own, so local models are typically smaller and less capable. Cloud providers invest heavily in tooling that local solutions cannot match, and you are responsible for updates, troubleshooting, and configuration. These trade-offs may be acceptable when privacy or institutional requirements demand local operation, but for learning purposes, cloud tools are usually more practical.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#summary-rules-you-should-actually-follow",
    "href": "python/ai-for-coding/index.html#summary-rules-you-should-actually-follow",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.10 Summary: Rules You Should Actually Follow",
    "text": "9.10 Summary: Rules You Should Actually Follow\nThis chapter has covered many considerations, but effective AI use comes down to a few key principles:\nIf you cannot read the code, you should not use the code. This is the fundamental rule. AI can help you write code faster, but you must understand what you are using. If you cannot explain it, you cannot debug it, maintain it, or defend it.\nAI accelerates feedback, not understanding. The AI can produce code quickly and identify errors quickly. But understanding comes from your engagement with the code. Use AI to speed up the feedback loop, not to skip the learning.\nIn research and industry, credibility beats convenience. Your reputation depends on the quality and correctness of your work. AI-generated code that contains errors damages your credibility. Code you understand and can explain builds it.\nStart with what you know. Write pseudocode before asking for help. Use AI to implement your ideas, not to have ideas for you. The research decisions are yours; AI helps with the implementation.\nTest relentlessly. Every AI-generated change should be followed by testing. Catch errors early, when they are easy to diagnose. Never assume that code is correct because AI produced it.\nMaintain version control. Commit working code before making changes. Create a safety net that lets you experiment confidently and recover from mistakes.\nTreat AI responses as hypotheses. The AI is proposing solutions, not providing answers. Your job is to evaluate those proposals critically, accept what works, and reject what does not.\nThese principles will serve you regardless of how AI tools evolve. The specific tools will change. The specific capabilities will improve. But the fundamental dynamic remains: AI is a tool that amplifies your capabilities, for better or worse. Used wisely, it makes you more effective. Used carelessly, it prevents you from becoming effective at all.\nThe choice is yours.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html",
    "href": "python/devcontainers/index.html",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "",
    "text": "10.1 Setting Up Devcontainers in VS Code\nIn empirical finance research, the tools and methodologies we employ play a crucial role in ensuring the integrity and reproducibility of our findings. One such powerful tool is containerization, which allows you to encapsulate your code and its dependencies into a standardized unit. Development containers, or devcontainers, provide a convenient way to create isolated environments for your data analysis tasks, ensuring that your code runs consistently across different systems.\nContainers are essentially lightweight, portable environments that package up code and all its dependencies, ensuring that the software runs consistently across different computing environments. This consistency is particularly valuable in research settings where the reproducibility of results is paramount. By containerizing their code, researchers can avoid the ‚Äúit works on my machine‚Äù problem, ensuring that their analyses can be replicated by others, regardless of the underlying system configurations. You might be familiar with the use of tools like uv or pyenv for managing Python environments to separate dependencies for different projects. Containers take this concept a step further by encapsulating the entire environment, including the operating system, runtime, libraries, and configurations. This ensures that the code runs identically on any machine, making it easier for researchers to share their work and for others to verify their findings.\nThis encapsulation also provides an added level of safety and security that is essential in research settings. By isolating the code and its dependencies from the host system, containers protect the environment from potential security vulnerabilities or malware disguised as python librairies. This isolation ensures that the code runs in a controlled environment, reducing the risk of unintended interactions with the host system. For additional security, you can use third-party services like Snyk to scan your dependencies for known vulnerabilities, or use curated package repositories like Anaconda or PyX that vet packages before making them available.\nFinally, containers facilitate collaboration among researchers. When a project is containerized, collaborators can easily set up their environment by simply running the container, eliminating the often cumbersome process of manually installing and configuring dependencies. This ease of setup promotes a more efficient workflow and reduces the likelihood of errors, making collaborative research more streamlined and productive. Even if you work solo, containers improve the resiliency of your research workflows, making it easy to recover from a broken or stolen computer by reinstalling your project on a new computer without worrying about compatibility issues. You can even run your code in the cloud with services like GitHub Codespaces, ensuring that your analyses are not tied to a specific machine or operating system.\nIn this chapter, I provide a step-by-step guide on setting up devcontainers in VS Code, with a focus on supporting Python uv and mounting local directories for file storage.\nAll the code and configurations used in this tutorial are available in the GitHub repository.\nDevelopment containers are a feature in VS Code that leverages Docker1 to create and manage containers specifically for development purposes. This allows developers and researchers to work within a consistent environment, which is crucial for complex data analysis tasks. Setting up devcontainers in VS Code is straightforward, but there are a few prerequisites you need to have in place: Docker, Visual Studio Code, and the Dev Containers extension. If you‚Äôre on macOS, you can install Docker using Homebrew:\nTo begin, you‚Äôll need to create a .devcontainer folder in your project directory. Inside this folder, you should create a devcontainer.json file, which will define the configuration for your development container. This file specifies the base image for the container, any additional tools or libraries that need to be installed, and other settings related to the development environment. By configuring this file, you can tailor the container to meet the specific needs of your data analysis tasks. This can be done using the Dev Containers extension in VS Code, which provides a user-friendly interface for creating and managing devcontainers. To get started, simply invoke Dev Containers: Add Development Container Configuration Files... from the command palette in VS Code and follow the prompts to create your devcontainer.json file.\nThis will prompt you to select a base image for your container, configure any additional tools or libraries, and set up other environment settings. As default options, I like to use the following:\nOnce you have configured your devcontainer.json file, you should get a prompt to reopen the project in the container. This will build the container based on the specified configuration and open your project within the containerized environment. You can verify that the container is running by checking the status bar in VS Code, which should indicate that you are working in a containerized environment.\nYou can always rebuild the container by invoking the Dev Containers: Rebuild and Reopen in Container command from the command palette. This will recreate the container based on the latest configuration settings, ensuring that your development environment is up-to-date and consistent with your project requirements.\nHere is what a simple devcontainer.json file (minus the comments) looks like:\nThis configuration references a shell script that will be executed after the container is created. This script handles installing uv and setting up the project dependencies (more on this later).",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#setting-up-devcontainers-in-vs-code",
    "href": "python/devcontainers/index.html#setting-up-devcontainers-in-vs-code",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "",
    "text": "brew install --cask docker\n\n\n\nSetup location: in workspace\nBase image: Python 3.12 image from Microsoft\nFeatures: Any additional tools you need (e.g., Quarto, GitHub CLI)\n\n\n\n\n\n\n\nTipPython version\n\n\n\nWhile the current latest version is 3.14, a the time of writing the latest Python base image from Microsoft is 3.12. This is not much of an issue because uv will install its own Python version when creating your environment based on your specification in pyproject.toml.\n\n\n\n\n\n{\n  \"name\": \"Python 3\",\n  \"image\": \"mcr.microsoft.com/devcontainers/python:1-3.12-bullseye\",\n  \"postCreateCommand\": \"./.devcontainer/postCreateCommand.sh\"\n}",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#images",
    "href": "python/devcontainers/index.html#images",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "10.2 Images",
    "text": "10.2 Images\nImages are a crucial aspect of containerization, as they define the base environment for your development container. When setting up a devcontainer in VS Code, you can choose from a variety of pre-built images that provide different programming languages, tools, and libraries. These images serve as the foundation for your development environment, ensuring that the necessary dependencies are available for your data analysis tasks. By tying your devcontainer to a specific image, you can guarantee that your code runs consistently across different systems, making it easier to share and replicate your work.\nNote: Most images are based on Linux distributions, so you may need to adjust your code or configurations if you are used to working on Windows or macOS. However, the differences are usually minimal and can be easily managed within the container.\nNote 2: Most images specify the environment (i.e.¬†Linux version, Python version, etc.), but not the architecture, which makes them compatible with most systems. For example, most PCs and older Macs use Intel or AMD CPUs with the x86_64 architecture, while newer Macs with Apple Silicon have the arm64 architecture. The images are usually compatible with both architectures, but that means that the environment will not be 100% identical if you run the container on different architectures. This is usually not a problem for data analysis tasks, but it‚Äôs something to keep in mind if you are having issues with your code running differently on different systems.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#features",
    "href": "python/devcontainers/index.html#features",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "10.3 Features",
    "text": "10.3 Features\nFeatures are additional tools or libraries that can be installed in your development container to enhance its functionality. These features can include language-specific tools, package managers, or development environments that are tailored to your project requirements. By specifying features in your devcontainer.json file, you can extend the capabilities of your container and ensure that it is well-suited for your data analysis tasks. You can find a list of available features at containers.dev/features. For example, you can add Quarto support with the ghcr.io/rocker-org/devcontainer-features/quarto-cli:1 feature, or add the GitHub CLI with ghcr.io/devcontainers/features/github-cli:1.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#uv",
    "href": "python/devcontainers/index.html#uv",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "10.4 uv",
    "text": "10.4 uv\nuv is an extremely fast Python package and project manager written in Rust, designed to replace tools like pip, pip-tools, pipx, poetry, and more. It simplifies the process of creating, managing, and sharing Python projects by providing a unified interface for dependency management, packaging, and virtual environment handling. uv uses a pyproject.toml file to define project dependencies, scripts, and configurations, making it easy to manage project settings and requirements. By integrating uv into your devcontainer setup, you can streamline your development workflow and ensure that your Python projects are well-organized and reproducible.\nFor example here is a pyproject.toml file that defines the dependencies for a Python project:\n[project]\nname = \"my-project\"\nversion = \"0.1.0\"\ndescription = \"My research project\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13,&lt;4.0\"\ndependencies = [\n    \"pandas&gt;=2.2.2\",\n    \"numpy&gt;=2.0.0\",\n]\n\n[dependency-groups]\ndev = [\n    \"pytest&gt;=7.2.0\",\n    \"ruff&gt;=0.11.5\",\n]\nWith uv, project dependencies are defined in the standard [project] section following PEP 621, rather than in a tool-specific section. This makes your pyproject.toml file more portable across different Python tools. You can also define development dependencies in a separate [dependency-groups] section, which allows you to install them only when needed.\nTo set up uv in your devcontainer, create a postCreateCommand.sh script in your .devcontainer folder:\n#!/usr/bin/env bash\n\n# Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install project dependencies\nuv sync\nThis script installs uv and then runs uv sync to install all project dependencies into a virtual environment (.venv). The uv sync command reads your pyproject.toml file and creates a reproducible environment with all specified dependencies.\nTo use this script, reference it in your devcontainer.json file:\n  \"postCreateCommand\": \"./.devcontainer/postCreateCommand.sh\"\nMake sure the script is executable by running chmod +x .devcontainer/postCreateCommand.sh before committing it to your repository.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#mounting-local-directories",
    "href": "python/devcontainers/index.html#mounting-local-directories",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "10.5 Mounting Local Directories",
    "text": "10.5 Mounting Local Directories\nBy default, the development container is isolated from the host system, except for the workspace directory, which is mounted into the container. This ensures that your project files are accessible within the container, allowing you to work on your code seamlessly. However, there are cases where you may need to access files or directories outside the workspace, such as large data files. To achieve this, you can mount local directories into the development container, making them available within the container environment.\nTo mount a local directory, you can add the mounts property to your devcontainer.json file, specifying the source path on the host and the target path in the container.\nHere is an example configuration for mounting a local directory:\n  \"mounts\": [\"source=/path/to/local/directory,target=/workspace/data,type=bind,consistency=cached\"]\nThis setup ensures that the directory /path/to/local/directory on your host machine is accessible within the container at /workspace/data. This approach provides the flexibility to work with local files while benefiting from the isolated environment of the container, except for the mounted directories.\nNote: This mounting process limits the flexibility of the container, as the source directory must be available on the host system. If you are working on a shared project or need to access files from different locations, you may need to consider alternative approaches, such as using a networked file system or cloud storage. As far as I know, there is no way to specify the source directory using an environment variable, so each collaborator would need to update the devcontainer.json file with their local path.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#vs-code-extensions",
    "href": "python/devcontainers/index.html#vs-code-extensions",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "10.6 VS Code Extensions",
    "text": "10.6 VS Code Extensions\nYou can also use the devcontainer.json file to configure default VS Code seettings and install VS Code extensions in your development container. When working with devcontainers, you will notice that not all the extensions you have installed on your host system are available in the container environment. Specifically, extensions that are mostly used for the host system (think UI), such as themes or language packs, will still be available. However, extensions that require access to the container environment, such as language servers or debuggers, will need to be installed. For example, the python image will install the Python extension, but you may need to install additional extensions for specific tasks, such as the Jupyter extension for working with Jupyter notebooks or with the interactive window. It can also be useful to make sure that all collaborators use the same formatting tools, such as Ruff.\nTo address this, you can specify the extensions and settings you want to install in the devcontainer.json file. For example, this will install Jupyter and Ruff, configure Ruff as the default formatter, and point VS Code to the virtual environment created by uv:\n    \"customizations\": {\n        \"vscode\": {\n            \"extensions\": [\n                \"ms-toolsai.jupyter\",\n                \"charliermarsh.ruff\"\n            ],\n            \"settings\": {\n                \"[python]\": {\n                    \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n                    \"editor.codeActionsOnSave\": {\n                        \"source.fixAll\": \"explicit\",\n                        \"source.organizeImports\": \"explicit\"\n                    }\n                },\n                \"python.defaultInterpreterPath\": \"${workspaceFolder}/.venv/bin/python\",\n                \"editor.formatOnSave\": true\n            }\n        }\n    }\nNote the python.defaultInterpreterPath setting, which tells VS Code to use the Python interpreter in the .venv virtual environment created by uv sync. This ensures that VS Code uses the correct Python environment with all your project dependencies.\nYou can find the extension IDs by looking at the extension in VS Code, then clicking on the gear icon and selecting ‚ÄúCopy Extension ID‚Äù.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#limitations",
    "href": "python/devcontainers/index.html#limitations",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "10.7 Limitations",
    "text": "10.7 Limitations\nDevcontainers are a powerful tool for creating isolated development environments, but they do have some limitations. One of the main drawbacks is the overhead associated with running containers, which can slow down the development process, especially for large projects or resource-intensive tasks. It‚Äôs not an issue that I have found to be significant, but it‚Äôs something to keep in mind if you are working on a particularly demanding project or have limited system resources.\nAdditionally, the isolation provided by containers can make it challenging to use some resources from the host system, such as GPUs or hardware peripherals. While it is possible to pass through devices to the container using Docker, this process can be complex and may not be suitable for all use cases. For example, on Apple Silicon Macs, even if the Linux container could access the GPU, there are no Linux drivers available for the GPU, so it would not be able to use it.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#footnotes",
    "href": "python/devcontainers/index.html#footnotes",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "",
    "text": "Other alternatives such as Podman and Colima can be used, but the official documentation is centered around Docker. See the VS Code documentation for supported alternatives.‚Ü©Ô∏é",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "data-analysis/index.html",
    "href": "data-analysis/index.html",
    "title": "Working with Data",
    "section": "",
    "text": "This part covers the essential tools and techniques for working with data in Python. We introduce three powerful libraries‚Äîpandas, Polars, and DuckDB‚Äîand demonstrate how to use them for common data manipulation tasks in empirical finance.\nThe chapters in this part progressively build your data manipulation skills:\n\nIntroduction to DataFrames: A high-level overview of pandas, Polars, and DuckDB\nData Input and File Formats: Loading data from CSV, Parquet, and Excel files\nData Cleaning: Handling duplicates, missing data, and validation\nData Structuring and Aggregation: Grouping, aggregation, and working with keys\nReshaping Data: Converting between long and wide formats\nJoins and Merges: Combining datasets correctly\n\nMost concepts are demonstrated with examples in pandas, Polars, and DuckDB, allowing you to choose the right tool for your needs and understand how to translate between them.",
    "crumbs": [
      "Working with Data"
    ]
  },
  {
    "objectID": "data-analysis/dataframes/index.html",
    "href": "data-analysis/dataframes/index.html",
    "title": "11¬† Introduction to DataFrames",
    "section": "",
    "text": "11.1 Overview\nIn empirical finance, we work mostly with structured data: stock prices organized by date and ticker, financial statements arranged by company and quarter, portfolio returns indexed by time and strategy. The DataFrame abstraction is the fundamental tool for handling this kind of tabular data in Python.\nThis chapter introduces three major DataFrame libraries in the Python ecosystem: pandas (the established standard), Polars (a modern, high-performance alternative), and DuckDB (which brings SQL to DataFrames). Understanding when and how to use each tool will significantly impact your productivity and the performance of your empirical work.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Introduction to DataFrames</span>"
    ]
  },
  {
    "objectID": "data-analysis/dataframes/index.html#the-dataframe-abstraction",
    "href": "data-analysis/dataframes/index.html#the-dataframe-abstraction",
    "title": "11¬† Introduction to DataFrames",
    "section": "11.2 The DataFrame Abstraction",
    "text": "11.2 The DataFrame Abstraction\n\n11.2.1 What is a DataFrame?\nA DataFrame is a two-dimensional labeled data structure with columns of potentially different types. Think of it as a spreadsheet in code, a database table with an API, or a generalization of a matrix that allows mixed types.\nBut a DataFrame is more than just a container. It‚Äôs an abstraction that encapsulates:\n\nData storage: Efficient memory layout for columnar data\nIndexing: Labels for rows and columns enabling semantic access\nOperations: A rich API for transformations, aggregations, and joins\nType system: Handling heterogeneous data types within a single structure\n\n\n\n11.2.2 Why DataFrames Matter in Finance\nIn empirical finance research, DataFrames solve several critical problems:\nData Alignment: When you merge stock returns with company characteristics, the DataFrame automatically handles date and identifier alignment. No manual loops checking if dates match.\nMissing Data: Financial datasets are riddled with missing values. DataFrames provide principled methods for handling missing values that respect the underlying data structure.\nPerformance: Modern DataFrame libraries use columnar storage and vectorized operations, making them orders of magnitude faster than row-by-row processing.\nExpressiveness: Complex operations like ‚Äúcalculate rolling volatility by stock, then merge with quarterly earnings‚Äù can be expressed in a few lines of readable code.\n\n\n\n\n\n\nNoteThe Columnar Advantage\n\n\n\nDataFrames store data column-by-column rather than row-by-row. This means:\n\nOperations on a single column (like calculating returns) are blazingly fast\nMemory access patterns are optimized for modern CPUs\nCompression works better when similar data is stored together\nAggregations can skip irrelevant columns entirely\n\nThis is why DataFrames can process millions of rows efficiently while naive Python loops struggle.\n\n\n\n\n11.2.3 Core Concepts\nAll DataFrame libraries share several fundamental concepts:\nColumns and Rows: The basic 2D structure. Columns typically represent variables (price, volume, return), while rows represent observations (a specific date, company, or transaction).\nIndex: A special column (or set of columns) that labels rows. In finance, this is often a date or a combination of date and identifier.\nSchema: The data type of each column. Strongly-typed schemas enable optimization and catch errors early.\nOperations: Transformations that produce new DataFrames. These include filtering, selecting, aggregating, and joining.\n\n\n\n\n\n\nNoteBeyond two dimensions\n\n\n\nIf you need more than two dimensions for your data, the xarray library in Python can be helpful. It provides labeled, multi-dimensional arrays that extend the DataFrame concept to higher dimensions. While less widespread in finance than pandas or Polars, xarray is useful for working with panel data or other multi-dimensional structures.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Introduction to DataFrames</span>"
    ]
  },
  {
    "objectID": "data-analysis/dataframes/index.html#pandas-overview",
    "href": "data-analysis/dataframes/index.html#pandas-overview",
    "title": "11¬† Introduction to DataFrames",
    "section": "11.3 pandas Overview",
    "text": "11.3 pandas Overview\npandas is the original DataFrame library for Python and remains the most widely used. Created by Wes McKinney in 2008 for financial data analysis, it has become the de facto standard for data manipulation in Python. While pandas is no longer my go-to library for data processing (I now mostly use Polars for that), pandas remains the standard and is unavoidable because many additional libraries for plotting and statistics rely on pandas DataFrames. If you only learn one DataFrame library, it should be this one.\n\n11.3.1 Basic Operations\nLet‚Äôs start with creating and manipulating a simple DataFrame:\n\n1import pandas as pd\n\n2df = pd.DataFrame({\n    'ticker': ['AAPL', 'AAPL', 'MSFT', 'MSFT', 'GOOG', 'GOOG'],\n3    'date': pd.date_range('2024-01-01', periods=6, freq='D')[:6],\n    'price': [150.5, 152.3, 380.2, 378.9, 140.1, 142.5],\n    'volume': [1000000, 1050000, 800000, 820000, 950000, 980000]\n})\n\n4df\n\n\n1\n\nThe standard convention is to import pandas as pd, allowing you to reference it with this short alias throughout your code.\n\n2\n\nCreate a DataFrame from a dictionary where keys become column names.\n\n3\n\npd.date_range() generates a sequence of dates with the specified frequency ('D' for daily).\n\n4\n\nIn Jupyter notebooks and Quarto, simply placing a DataFrame at the end of a cell displays it with nice formatting.\n\n\n\n\n\n\n\n\n\n\n\nticker\ndate\nprice\nvolume\n\n\n\n\n0\nAAPL\n2024-01-01\n150.5\n1000000\n\n\n1\nAAPL\n2024-01-02\n152.3\n1050000\n\n\n2\nMSFT\n2024-01-03\n380.2\n800000\n\n\n3\nMSFT\n2024-01-04\n378.9\n820000\n\n\n4\nGOOG\n2024-01-05\n140.1\n950000\n\n\n5\nGOOG\n2024-01-06\n142.5\n980000\n\n\n\n\n\n\n\nKey characteristics of this DataFrame:\n\nIndex: Automatically created as integers (0-5)\nColumns: Four columns with mixed types (object, datetime64, float64, int64)\nShape: 6 rows √ó 4 columns\n\n\n\n11.3.2 Indexing and Selection\npandas provides multiple ways to select data, which can be confusing at first but powerful once mastered. pandas is somewhat unique in this regard: Polars and DuckDB don‚Äôt rely on indexing the same way pandas does‚Äîthey basically treat all columns the same. In pandas, the index column (or columns, if there‚Äôs a multi-index) has a special status that affects how the DataFrame behaves.\n\n11.3.2.1 Column Selection\n\n# Select a single column (returns a Series)\nprices = df['price']\nprices\n\n0    150.5\n1    152.3\n2    380.2\n3    378.9\n4    140.1\n5    142.5\nName: price, dtype: float64\n\n\n\n# Select multiple columns (returns a DataFrame)\nsubset = df[['ticker', 'price']]\nsubset\n\n\n\n\n\n\n\n\nticker\nprice\n\n\n\n\n0\nAAPL\n150.5\n\n\n1\nAAPL\n152.3\n\n\n2\nMSFT\n380.2\n\n\n3\nMSFT\n378.9\n\n\n4\nGOOG\n140.1\n\n\n5\nGOOG\n142.5\n\n\n\n\n\n\n\nNotice that the double brackets in the second example aren‚Äôt a different operator‚Äîwe‚Äôre still using the same square bracket indexing, but instead of passing a single column name, we‚Äôre passing a list of column names. When you pass a list, pandas returns a DataFrame; when you pass a single column name, it returns a Series.\n\n\n11.3.2.2 Row Selection with .loc and .iloc\npandas distinguishes between label-based indexing (.loc) and position-based indexing (.iloc):\n\n# Select rows 0-2 by position\ndf.iloc[0:3]\n\n\n\n\n\n\n\n\nticker\ndate\nprice\nvolume\n\n\n\n\n0\nAAPL\n2024-01-01\n150.5\n1000000\n\n\n1\nAAPL\n2024-01-02\n152.3\n1050000\n\n\n2\nMSFT\n2024-01-03\n380.2\n800000\n\n\n\n\n\n\n\n\n# If we set an index, we can use .loc with labels\ndf_indexed = df.set_index('date')\ndf_indexed.loc['2024-01-01':'2024-01-03']\n\n\n\n\n\n\n\n\nticker\nprice\nvolume\n\n\ndate\n\n\n\n\n\n\n\n2024-01-01\nAAPL\n150.5\n1000000\n\n\n2024-01-02\nAAPL\n152.3\n1050000\n\n\n2024-01-03\nMSFT\n380.2\n800000\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip.loc vs .iloc vs []\n\n\n\n\nUse .loc[row_labels, column_labels] for label-based indexing\nUse .iloc[row_positions, column_positions] for integer position-based indexing\nUse [] for simple column selection or boolean masking\n\nThe distinction prevents ambiguity: df[0] is an error (is it column 0 or row 0?), but df.iloc[0] and df['column_0'] are clear.\n\n\n\n\n11.3.2.3 Boolean Indexing\nFiltering data is a core operation in empirical finance:\n\n# Select rows where price &gt; 150\nhigh_price = df[df['price'] &gt; 150]\nhigh_price\n\n\n\n\n\n\n\n\nticker\ndate\nprice\nvolume\n\n\n\n\n0\nAAPL\n2024-01-01\n150.5\n1000000\n\n\n1\nAAPL\n2024-01-02\n152.3\n1050000\n\n\n2\nMSFT\n2024-01-03\n380.2\n800000\n\n\n3\nMSFT\n2024-01-04\n378.9\n820000\n\n\n\n\n\n\n\nThe expression inside the square brackets (df['price'] &gt; 150) is itself a Series of Boolean values. Let‚Äôs look at what it produces:\n\n# The condition produces a Boolean Series\ndf['price'] &gt; 150\n\n0     True\n1     True\n2     True\n3     True\n4    False\n5    False\nName: price, dtype: bool\n\n\nThe rows that get selected are those where the Boolean value is True.\n\n# Multiple conditions: AAPL stocks with price &gt; 150\naapl_high = df[(df['ticker'] == 'AAPL') & (df['price'] &gt; 150)]\naapl_high\n\n\n\n\n\n\n\n\nticker\ndate\nprice\nvolume\n\n\n\n\n0\nAAPL\n2024-01-01\n150.5\n1000000\n\n\n1\nAAPL\n2024-01-02\n152.3\n1050000\n\n\n\n\n\n\n\n\n# Using query method for more readable syntax\nresult = df.query('ticker == \"AAPL\" and price &gt; 150')\nresult\n\n\n\n\n\n\n\n\nticker\ndate\nprice\nvolume\n\n\n\n\n0\nAAPL\n2024-01-01\n150.5\n1000000\n\n\n1\nAAPL\n2024-01-02\n152.3\n1050000\n\n\n\n\n\n\n\n\n\n\n11.3.3 Creating New Columns\nA common operation is creating new columns from existing ones. When you perform arithmetic operations between columns, pandas aligns them by index and applies the operation element by element:\n\n# Create a new column by multiplying two existing columns\n1df['dollar_volume'] = df['price'] * df['volume']\ndf\n\n\n1\n\nElement-wise multiplication: each row‚Äôs price is multiplied by that row‚Äôs volume.\n\n\n\n\n\n\n\n\n\n\n\nticker\ndate\nprice\nvolume\ndollar_volume\n\n\n\n\n0\nAAPL\n2024-01-01\n150.5\n1000000\n150500000.0\n\n\n1\nAAPL\n2024-01-02\n152.3\n1050000\n159915000.0\n\n\n2\nMSFT\n2024-01-03\n380.2\n800000\n304160000.0\n\n\n3\nMSFT\n2024-01-04\n378.9\n820000\n310698000.0\n\n\n4\nGOOG\n2024-01-05\n140.1\n950000\n133095000.0\n\n\n5\nGOOG\n2024-01-06\n142.5\n980000\n139650000.0\n\n\n\n\n\n\n\nWhen you multiply or add a scalar, it applies to all elements in the column:\n\n# Multiply a column by a scalar\n1df['price_cents'] = df['price'] * 100\ndf\n\n\n1\n\nThe scalar 100 is applied to every element in the price column.\n\n\n\n\n\n\n\n\n\n\n\nticker\ndate\nprice\nvolume\ndollar_volume\nprice_cents\n\n\n\n\n0\nAAPL\n2024-01-01\n150.5\n1000000\n150500000.0\n15050.0\n\n\n1\nAAPL\n2024-01-02\n152.3\n1050000\n159915000.0\n15230.0\n\n\n2\nMSFT\n2024-01-03\n380.2\n800000\n304160000.0\n38020.0\n\n\n3\nMSFT\n2024-01-04\n378.9\n820000\n310698000.0\n37890.0\n\n\n4\nGOOG\n2024-01-05\n140.1\n950000\n133095000.0\n14010.0\n\n\n5\nGOOG\n2024-01-06\n142.5\n980000\n139650000.0\n14250.0\n\n\n\n\n\n\n\nYou can also combine columns with different operations:\n\n# Combining columns with arithmetic\ndf['avg_price_per_share'] = df['dollar_volume'] / df['volume']\ndf[['ticker', 'price', 'avg_price_per_share']]\n\n\n\n\n\n\n\n\nticker\nprice\navg_price_per_share\n\n\n\n\n0\nAAPL\n150.5\n150.5\n\n\n1\nAAPL\n152.3\n152.3\n\n\n2\nMSFT\n380.2\n380.2\n\n\n3\nMSFT\n378.9\n378.9\n\n\n4\nGOOG\n140.1\n140.1\n\n\n5\nGOOG\n142.5\n142.5\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteIndex alignment in operations\n\n\n\nWhen you perform operations between two columns (or two DataFrames), pandas automatically aligns them by their index. This is powerful but can also be a source of subtle bugs if your indices don‚Äôt match as expected. We‚Äôll cover sorting, grouping, and aggregation operations in a later chapter.\n\n\n\n\n11.3.4 Strengths and Limitations\nStrengths:\n\nMature ecosystem with extensive documentation\nRich functionality covering nearly every data manipulation need\nExcellent integration with other libraries (scikit-learn, statsmodels, matplotlib)\nFlexible indexing system enabling time-series analysis\nWide adoption means abundant examples, Stack Overflow answers, and AI coding assistance\n\nLimitations:\n\nPerformance can degrade with multi-gigabyte datasets, depending on available RAM\nMemory usage is higher than necessary due to row-based implementation details\nAPI inconsistencies accumulated over 15+ years of development\nSingle-threaded execution for most operations\nString operations are particularly slow",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Introduction to DataFrames</span>"
    ]
  },
  {
    "objectID": "data-analysis/dataframes/index.html#polars-overview",
    "href": "data-analysis/dataframes/index.html#polars-overview",
    "title": "11¬† Introduction to DataFrames",
    "section": "11.4 Polars Overview",
    "text": "11.4 Polars Overview\nPolars is a modern DataFrame library written in Rust, designed from the ground up for performance. It addresses many of pandas‚Äô limitations while maintaining a familiar API for common operations.\nPolars differs from pandas in several important ways:\nLazy evaluation allows you to write your data processing code without immediately executing it. Polars builds a query plan and defers execution until you explicitly collect the results. This enables the query optimizer to analyze the entire pipeline and find efficiencies‚Äîsuch as eliminating unnecessary computations, reordering operations, or pushing filters earlier in the pipeline.\nParallel execution means Polars can automatically use multiple CPU cores for operations that support it. Unlike pandas, which is largely single-threaded, Polars can process different parts of your data concurrently, leading to significant speedups on modern multi-core machines.\nApache Arrow memory representation is an efficient columnar format that Polars uses internally. Arrow improves interoperability with other libraries that support the format (pandas includes Arrow as an optional backend), and generally makes operations more efficient. Importantly, Arrow explicitly supports missing values as a distinct type rather than relying on NaN, which is important for accurate calculations and proper handling of missing data. Arrow also enables efficient reading and writing to the optimized Parquet file format.\nThe expression API provides a composable syntax for data transformations that is not identical to pandas. Users coming from the R tidyverse may find it feels more familiar than pandas‚Äô approach. Expressions can be combined and optimized together, leading to more efficient execution.\n\n11.4.1 Basic Operations\n\n1import polars as pl\n\ndf_pl = pl.DataFrame({\n    'ticker': ['AAPL', 'AAPL', 'MSFT', 'MSFT', 'GOOG', 'GOOG'],\n    'date': pd.date_range('2024-01-01', periods=6, freq='D'),\n    'price': [150.5, 152.3, 380.2, 378.9, 140.1, 142.5],\n    'volume': [1000000, 1050000, 800000, 820000, 950000, 980000]\n})\n\ndf_pl\n\n\n1\n\nThe standard convention is to import Polars as pl.\n\n\n\n\n\nshape: (6, 4)\n\n\n\nticker\ndate\nprice\nvolume\n\n\nstr\ndatetime[ns]\nf64\ni64\n\n\n\n\n\"AAPL\"\n2024-01-01 00:00:00\n150.5\n1000000\n\n\n\"AAPL\"\n2024-01-02 00:00:00\n152.3\n1050000\n\n\n\"MSFT\"\n2024-01-03 00:00:00\n380.2\n800000\n\n\n\"MSFT\"\n2024-01-04 00:00:00\n378.9\n820000\n\n\n\"GOOG\"\n2024-01-05 00:00:00\n140.1\n950000\n\n\n\"GOOG\"\n2024-01-06 00:00:00\n142.5\n980000\n\n\n\n\n\n\n\n\n11.4.2 The Expression API\nPolars introduces an expression-based API that is more composable than pandas:\n\n# Select and transform columns\nresult = df_pl.select([\n1    pl.col('ticker'),\n    pl.col('price'),\n2    (pl.col('price') * pl.col('volume')).alias('dollar_volume')\n])\nresult\n\n\n1\n\npl.col() references a column by name.\n\n2\n\n.alias() names the resulting column.\n\n\n\n\n\nshape: (6, 3)\n\n\n\nticker\nprice\ndollar_volume\n\n\nstr\nf64\nf64\n\n\n\n\n\"AAPL\"\n150.5\n1.505e8\n\n\n\"AAPL\"\n152.3\n1.59915e8\n\n\n\"MSFT\"\n380.2\n3.0416e8\n\n\n\"MSFT\"\n378.9\n3.10698e8\n\n\n\"GOOG\"\n140.1\n1.33095e8\n\n\n\"GOOG\"\n142.5\n1.3965e8\n\n\n\n\n\n\n\n# Filtering with expressions\nhigh_price_pl = df_pl.filter(pl.col('price') &gt; 150)\nhigh_price_pl\n\n\nshape: (4, 4)\n\n\n\nticker\ndate\nprice\nvolume\n\n\nstr\ndatetime[ns]\nf64\ni64\n\n\n\n\n\"AAPL\"\n2024-01-01 00:00:00\n150.5\n1000000\n\n\n\"AAPL\"\n2024-01-02 00:00:00\n152.3\n1050000\n\n\n\"MSFT\"\n2024-01-03 00:00:00\n380.2\n800000\n\n\n\"MSFT\"\n2024-01-04 00:00:00\n378.9\n820000\n\n\n\n\n\n\n\n# Chaining operations\nresult = (\n    df_pl\n    .filter(pl.col('ticker') == 'AAPL')\n1    .with_columns((pl.col('price') * 1.1).alias('price_plus_10pct'))\n    .select(['ticker', 'date', 'price', 'price_plus_10pct'])\n)\nresult\n\n\n1\n\n.with_columns() adds or replaces columns in the DataFrame.\n\n\n\n\n\nshape: (2, 4)\n\n\n\nticker\ndate\nprice\nprice_plus_10pct\n\n\nstr\ndatetime[ns]\nf64\nf64\n\n\n\n\n\"AAPL\"\n2024-01-01 00:00:00\n150.5\n165.55\n\n\n\"AAPL\"\n2024-01-02 00:00:00\n152.3\n167.53\n\n\n\n\n\n\n\n\n11.4.3 Lazy Evaluation\nPolars‚Äô lazy API builds a query plan and optimizes it before execution:\n\n1lazy_df = df_pl.lazy()\n\n# Build a query (nothing executes yet)\nlazy_query = (\n    lazy_df\n    .filter(pl.col('volume') &gt; 850000)\n    .group_by('ticker')\n    .agg([\n        pl.col('price').mean().alias('avg_price'),\n        pl.col('volume').sum().alias('total_volume')\n    ])\n    .sort('avg_price', descending=True)\n)\n\n2result = lazy_query.collect()\nresult\n\n\n1\n\n.lazy() converts an eager DataFrame to a lazy frame.\n\n2\n\n.collect() executes the optimized query plan and returns the result.\n\n\n\n\n\nshape: (2, 3)\n\n\n\nticker\navg_price\ntotal_volume\n\n\nstr\nf64\ni64\n\n\n\n\n\"AAPL\"\n151.4\n2050000\n\n\n\"GOOG\"\n141.3\n1930000\n\n\n\n\n\n\n\n\n\n\n\n\nImportantWhen to Use Lazy Evaluation\n\n\n\nLazy evaluation shines when:\n\nYou have a complex chain of operations\nYour data is large enough that optimization matters\nYou‚Äôre reading from files (Polars can push down filters and projections)\n\nFor small datasets and simple operations, the overhead of optimization may not be worth it. Use eager mode (regular DataFrames) for interactive exploration.\n\n\n\n\n11.4.4 Grouping and Aggregation\n\n# Group by with multiple aggregations\nsummary_pl = df_pl.group_by('ticker').agg([\n    pl.col('price').mean().alias('avg_price'),\n    pl.col('price').std().alias('std_price'),\n    pl.col('volume').sum().alias('total_volume')\n])\nsummary_pl\n\n\nshape: (3, 4)\n\n\n\nticker\navg_price\nstd_price\ntotal_volume\n\n\nstr\nf64\nf64\ni64\n\n\n\n\n\"MSFT\"\n379.55\n0.919239\n1620000\n\n\n\"GOOG\"\n141.3\n1.697056\n1930000\n\n\n\"AAPL\"\n151.4\n1.272792\n2050000\n\n\n\n\n\n\n\n# Window functions for returns calculation\ndf_pl_sorted = df_pl.sort(['ticker', 'date'])\ndf_with_returns = df_pl_sorted.with_columns(\n    pl.col('price').pct_change().over('ticker').alias('return')\n)\ndf_with_returns\n\n\nshape: (6, 5)\n\n\n\nticker\ndate\nprice\nvolume\nreturn\n\n\nstr\ndatetime[ns]\nf64\ni64\nf64\n\n\n\n\n\"AAPL\"\n2024-01-01 00:00:00\n150.5\n1000000\nnull\n\n\n\"AAPL\"\n2024-01-02 00:00:00\n152.3\n1050000\n0.01196\n\n\n\"GOOG\"\n2024-01-05 00:00:00\n140.1\n950000\nnull\n\n\n\"GOOG\"\n2024-01-06 00:00:00\n142.5\n980000\n0.017131\n\n\n\"MSFT\"\n2024-01-03 00:00:00\n380.2\n800000\nnull\n\n\n\"MSFT\"\n2024-01-04 00:00:00\n378.9\n820000\n-0.003419\n\n\n\n\n\n\n\n\n11.4.5 Strengths and Limitations\nStrengths:\n\nSignificantly faster than pandas, especially for large datasets\nLower memory usage thanks to Arrow format\nAutomatic parallelization\nLazy evaluation enables query optimization\nMore consistent API design\nExcellent handling of string and categorical data\n\nLimitations:\n\nSmaller ecosystem (fewer third-party integrations)\nLess mature documentation and community resources\nSome advanced pandas features not yet implemented\nDifferent mental model requires learning curve\nIndex-less design may require adaptation for time-series workflows",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Introduction to DataFrames</span>"
    ]
  },
  {
    "objectID": "data-analysis/dataframes/index.html#duckdb-and-sql-for-dataframes",
    "href": "data-analysis/dataframes/index.html#duckdb-and-sql-for-dataframes",
    "title": "11¬† Introduction to DataFrames",
    "section": "11.5 DuckDB and SQL for DataFrames",
    "text": "11.5 DuckDB and SQL for DataFrames\nDuckDB is an in-process SQL database optimized for analytical queries. While not strictly a DataFrame library, it provides a powerful alternative interface for DataFrame operations through SQL.\nWhile this book focuses mostly on Python, SQL is also a very important language for financial data analytics because it is the language most databases use. Many third-party databases can be accessed through SQL from Python and return DataFrames, but DuckDB presents a nice alternative that lets you write SQL directly within your Python environment. DuckDB doesn‚Äôt need to run inside Python‚Äîit can also operate as a standalone tool‚Äîbut using it within Python makes it easy to take partial results and transfer them to pandas or Polars for further processing. Like Polars, DuckDB uses Arrow for its memory representation, enabling efficient data exchange between tools.\n\n11.5.1 Why SQL for DataFrames?\nSQL has several advantages:\n\nDeclarative: Describe what you want, not how to get it\nOptimized: Query optimizers can find efficient execution plans\nFamiliar: Many finance professionals already know SQL\nStandard: SQL skills transfer across tools and platforms\n\n\n\n11.5.2 Basic Usage\n\nimport duckdb\n\nresult = duckdb.sql(\"\"\"\n    SELECT ticker,\n           AVG(price) as avg_price,\n           SUM(volume) as total_volume\n    FROM df\n    GROUP BY ticker\n    ORDER BY avg_price DESC\n1\"\"\").df()\n\nresult\n\n\n1\n\n.df() converts the DuckDB result to a pandas DataFrame.\n\n\n\n\n\n\n\n\n\n\n\nticker\navg_price\ntotal_volume\n\n\n\n\n0\nMSFT\n379.55\n1620000.0\n\n\n1\nAAPL\n151.40\n2050000.0\n\n\n2\nGOOG\n141.30\n1930000.0\n\n\n\n\n\n\n\nNotice that we didn‚Äôt need to import the DataFrame into DuckDB explicitly. DuckDB automatically detects pandas DataFrames in the scope and makes them queryable.\n\n\n11.5.3 Advanced SQL Operations\n\n# Window functions for returns\nreturns_sql = duckdb.sql(\"\"\"\n    SELECT ticker,\n           date,\n           price,\n           price / LAG(price) OVER (PARTITION BY ticker ORDER BY date) - 1 as return\n    FROM df\n    ORDER BY ticker, date\n\"\"\").df()\n\nreturns_sql\n\n\n\n\n\n\n\n\nticker\ndate\nprice\nreturn\n\n\n\n\n0\nAAPL\n2024-01-01\n150.5\nNaN\n\n\n1\nAAPL\n2024-01-02\n152.3\n0.011960\n\n\n2\nGOOG\n2024-01-05\n140.1\nNaN\n\n\n3\nGOOG\n2024-01-06\n142.5\n0.017131\n\n\n4\nMSFT\n2024-01-03\n380.2\nNaN\n\n\n5\nMSFT\n2024-01-04\n378.9\n-0.003419\n\n\n\n\n\n\n\n\n# Complex filtering and aggregation\nanalysis = duckdb.sql(\"\"\"\n    WITH high_volume AS (\n        SELECT *\n        FROM df\n        WHERE volume &gt; 900000\n    )\n    SELECT\n        ticker,\n        COUNT(*) as n_high_volume_days,\n        AVG(price) as avg_price_on_high_volume\n    FROM high_volume\n    GROUP BY ticker\n\"\"\").df()\n\nanalysis\n\n\n\n\n\n\n\n\nticker\nn_high_volume_days\navg_price_on_high_volume\n\n\n\n\n0\nGOOG\n2\n141.3\n\n\n1\nAAPL\n2\n151.4\n\n\n\n\n\n\n\n\n\n11.5.4 Integration with Multiple Sources\nDuckDB can query multiple DataFrame types simultaneously:\n\n# Query both pandas and Polars DataFrames in one query\ncombined = duckdb.sql(\"\"\"\n    SELECT\n        p.ticker,\n        p.avg_price as pandas_avg,\n        pl.avg_price as polars_avg\n    FROM (SELECT ticker, AVG(price) as avg_price FROM df GROUP BY ticker) p\n    JOIN (SELECT ticker, AVG(price) as avg_price FROM df_pl GROUP BY ticker) pl\n        ON p.ticker = pl.ticker\n\"\"\").df()\n\ncombined\n\n\n\n\n\n\n\n\nticker\npandas_avg\npolars_avg\n\n\n\n\n0\nGOOG\n141.30\n141.30\n\n\n1\nAAPL\n151.40\n151.40\n\n\n2\nMSFT\n379.55\n379.55\n\n\n\n\n\n\n\n\n\n11.5.5 DuckDB Relations API\nFor a more programmatic interface, DuckDB provides a relational API:\n\n1rel = duckdb.sql(\"SELECT * FROM df\")\n\n# Chain operations\nresult = (\n    rel\n    .filter(\"volume &gt; 900000\")\n    .aggregate(\"ticker, AVG(price) as avg_price\")\n    .order(\"avg_price DESC\")\n)\n\nresult.df()\n\n\n1\n\nCreate a relation from a DataFrame that can be queried programmatically.\n\n\n\n\n\n\n\n\n\n\n\nticker\navg_price\n\n\n\n\n0\nAAPL\n151.4\n\n\n1\nGOOG\n141.3\n\n\n\n\n\n\n\n\n\n11.5.6 Strengths and Limitations\nStrengths:\n\nExtremely fast for analytical queries\nOptimized query plans can outperform hand-written DataFrame code\nSQL is a standard, transferable skill\nSeamless integration with pandas and Polars\nCan query files directly without loading into memory\nExcellent for joins and complex aggregations\n\nLimitations:\n\nSQL syntax can be verbose for simple operations\nLess suitable for row-wise operations or complex custom logic\nDebugging SQL queries can be harder than Python code\nSome operations are more natural in a DataFrame API\nEmbedded database means limited concurrency",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Introduction to DataFrames</span>"
    ]
  },
  {
    "objectID": "data-analysis/dataframes/index.html#choosing-between-pandas-polars-and-duckdb",
    "href": "data-analysis/dataframes/index.html#choosing-between-pandas-polars-and-duckdb",
    "title": "11¬† Introduction to DataFrames",
    "section": "11.6 Choosing Between pandas, Polars, and DuckDB",
    "text": "11.6 Choosing Between pandas, Polars, and DuckDB\nThe right tool depends on your specific use case. Here‚Äôs a decision framework:\n\n11.6.1 When to Use pandas\nChoose pandas when:\n\nYou‚Äôre working with small to medium datasets (&lt;1GB in memory)\nYou need integration with scikit-learn, statsmodels, or other pandas-centric libraries\nYou‚Äôre learning and want maximum community support\nYour code needs to be maintained by others who know pandas\nYou need pandas‚Äô advanced time-series functionality (business day calendars, time zone handling)\n\nExample use case: Exploratory analysis of a few thousand stocks with daily data for factor model estimation.\n\n\n11.6.2 When to Use Polars\nChoose Polars when:\n\nYou‚Äôre working with large datasets (&gt;1GB)\nPerformance is critical (e.g., production pipelines, real-time systems)\nYou want cleaner, more maintainable code through the expression API\nYou need parallel processing without explicit threading code\nString manipulation performance matters\n\nExample use case: Processing tick-by-tick trade data for thousands of stocks to compute intraday liquidity measures.\n\n\n11.6.3 When to Use DuckDB\nChoose DuckDB when:\n\nYou‚Äôre comfortable with SQL and prefer declarative queries\nYou need to join multiple large datasets\nYour data lives in files (CSV, Parquet) and you want to query without loading\nYou need maximum analytical query performance\nYou‚Äôre aggregating or filtering large datasets\n\nExample use case: Joining quarterly earnings data with daily stock returns across thousands of companies and years.\n\n\n11.6.4 Hybrid Approaches\nYou don‚Äôt need to choose just one. Modern workflows often combine tools:\n\n# Read large file with DuckDB\nduckdb_result = duckdb.sql(\"\"\"\n    SELECT ticker, date, price, volume\n    FROM 'large_dataset.parquet'\n    WHERE date &gt;= '2020-01-01'\n    AND ticker IN ('AAPL', 'MSFT', 'GOOG')\n\"\"\")\n\n# Convert to Polars for further processing\ndf_polars = pl.from_arrow(duckdb_result.arrow())\n\n# Do complex feature engineering in Polars\nfeatures = df_polars.with_columns([\n    pl.col('price').pct_change().over('ticker').alias('return'),\n    pl.col('volume').rolling_mean(20).over('ticker').alias('avg_volume_20d')\n])\n\n# Convert to pandas for sklearn\ndf_pandas = features.to_pandas()\n\n# Use in machine learning pipeline\n# from sklearn.ensemble import RandomForestRegressor\n# model = RandomForestRegressor()\n# ...\n\n\n\n\n\n\n\nTipPractical Recommendation\n\n\n\nFor MSc-level empirical finance work, I recommend:\n\nLearn pandas first: It‚Äôs the standard, and you‚Äôll encounter it everywhere\nAdd Polars for performance: When pandas becomes slow, rewrite critical sections in Polars\nUse DuckDB for data wrangling: ETL pipelines and complex joins are often cleaner in SQL\nProfile your code: Don‚Äôt optimize prematurely‚Äîmeasure where time is actually spent\n\nThe best practitioners know all three and choose the right tool for each task.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Introduction to DataFrames</span>"
    ]
  },
  {
    "objectID": "data-analysis/dataframes/index.html#summary",
    "href": "data-analysis/dataframes/index.html#summary",
    "title": "11¬† Introduction to DataFrames",
    "section": "11.7 Summary",
    "text": "11.7 Summary\nDataFrames are the fundamental abstraction for tabular data in empirical finance. This chapter introduced three powerful tools:\npandas: The established standard with broad ecosystem support. Best for general-purpose data analysis, learning, and integration with statistical libraries.\nPolars: A modern, high-performance alternative with lazy evaluation and automatic parallelization. Best for large datasets and performance-critical applications.\nDuckDB: SQL interface for analytical queries on DataFrames and files. Best for complex joins, aggregations, and data pipeline work.\nThe key insight is that these tools are complementary. Understanding when to use each‚Äîand how to combine them‚Äîwill make you significantly more productive in empirical finance research.\nIn the next chapters, we‚Äôll dive deeper into specific operations: data cleaning, merging and joining, time-series operations, and efficient computation with large datasets.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Introduction to DataFrames</span>"
    ]
  },
  {
    "objectID": "data-analysis/dataframes/index.html#further-reading",
    "href": "data-analysis/dataframes/index.html#further-reading",
    "title": "11¬† Introduction to DataFrames",
    "section": "11.8 Further Reading",
    "text": "11.8 Further Reading\n\npandas documentation: Comprehensive guide to pandas\nPolars User Guide: Official Polars documentation\nDuckDB documentation: Complete DuckDB reference\nMcKinney, W. (2022). Python for Data Analysis, 3rd Edition. O‚ÄôReilly. (by pandas creator)\nVanderPlas, J. (2016). Python Data Science Handbook. O‚ÄôReilly.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Introduction to DataFrames</span>"
    ]
  },
  {
    "objectID": "data-analysis/io/index.html",
    "href": "data-analysis/io/index.html",
    "title": "12¬† Data Input and File Formats",
    "section": "",
    "text": "12.1 CSV Files\nThe first step in any empirical analysis is getting your data into a form where you can work with it. In the world of finance, data comes in many shapes and sizes: CSV files exported from Bloomberg, Excel spreadsheets from analysts, proprietary formats from data vendors, and increasingly, large datasets stored in efficient binary formats. Understanding how to work with different file formats efficiently is crucial for productive data analysis.\nThe choice of file format matters more than you might think. A format that works well for a small dataset of a few hundred stocks might become painfully slow when you‚Äôre dealing with millions of trades. Similarly, while Excel is ubiquitous in finance, it has limitations when working with large datasets or when you need to ensure reproducibility. In this chapter, we‚Äôll explore the most common file formats you‚Äôll encounter in empirical finance and learn how to work with them using both pandas and polars, two powerful Python libraries for data manipulation.\nPandas has long been the standard for data analysis in Python, offering a rich API and extensive functionality. Polars is a newer library built in Rust that offers significantly faster performance, especially for larger datasets, while maintaining a similar conceptual model. Throughout this chapter, we‚Äôll show examples using both libraries so you can choose the right tool for your specific needs.\nCSV (Comma-Separated Values) is perhaps the most universal data format. It‚Äôs simple, human-readable, and supported by virtually every data tool and programming language. In finance, you‚Äôll frequently encounter CSV files: historical price data from exchanges, bulk downloads from data vendors, exports from Bloomberg terminals, and outputs from other analysis tools.\nThe beauty of CSV lies in its simplicity. Each line represents a row of data, with values separated by commas (or sometimes other delimiters like tabs or semicolons). The first line typically contains column names. Here‚Äôs what a simple CSV file of stock prices might look like:\nHowever, this simplicity comes with some drawbacks. CSV files store everything as text, so data types must be inferred when reading. They don‚Äôt compress particularly well compared to binary formats. And for very large files, parsing text can be slow compared to reading binary data directly.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Data Input and File Formats</span>"
    ]
  },
  {
    "objectID": "data-analysis/io/index.html#csv-files",
    "href": "data-analysis/io/index.html#csv-files",
    "title": "12¬† Data Input and File Formats",
    "section": "",
    "text": "date,ticker,price,volume\n2024-01-02,AAPL,185.64,52000000\n2024-01-02,MSFT,374.58,28000000\n2024-01-03,AAPL,184.25,48000000\n2024-01-03,MSFT,371.32,25000000\n\n\n12.1.1 Reading CSV Files with Pandas\n\n\n\n\n\n\nNotepandas supports many file formats\n\n\n\nPandas supports reading and writing a wide variety of file formats beyond CSV, including Excel spreadsheets, SAS data sets, Stata data sets, and many others. However, not all functionality is implemented in pandas itself‚Äîit relies on third-party libraries for some formats.\nFor CSV files specifically, pandas includes multiple parsing engines. The Python engine is the most flexible but also the slowest. You can also use the C engine (the default) or the PyArrow engine for better performance. Whenever you need to handle a nonstandard CSV file or work with a different file format, consult the pandas I/O documentation for guidance on which engine or approach to use.\n\n\nReading a CSV file with pandas is straightforward using the read_csv() function:\nimport pandas as pd\n\n# Basic CSV reading\ndf = pd.read_csv(\"stock_prices.csv\")\n\n# View the first few rows\nprint(df.head())\nThe read_csv() function is remarkably flexible and can handle many variations and edge cases you‚Äôll encounter with real-world data:\n# Specify date columns to parse\ndf = pd.read_csv(\n    \"stock_prices.csv\",\n1    parse_dates=[\"date\"],\n2    index_col=\"date\"\n)\n\n# Handle different delimiters (e.g., semicolon-separated)\n3df = pd.read_csv(\"data.csv\", sep=\";\")\n\n# Skip rows or specify column names\ndf = pd.read_csv(\n    \"data.csv\",\n4    skiprows=2,\n5    names=[\"date\", \"price\", \"vol\"],\n6    header=None\n)\n\n# Handle missing values\ndf = pd.read_csv(\n    \"data.csv\",\n7    na_values=[\".\", \"NA\", \"n/a\"]\n)\n\n# Read only specific columns\ndf = pd.read_csv(\n    \"large_file.csv\",\n8    usecols=[\"date\", \"ticker\", \"price\"]\n)\n\n1\n\nConvert date column to datetime during parsing\n\n2\n\nUse the date column as the DataFrame index\n\n3\n\nUse semicolon as delimiter instead of comma\n\n4\n\nSkip the first 2 rows of the file\n\n5\n\nProvide custom column names\n\n6\n\nIndicate the file has no header row\n\n7\n\nTreat additional strings as missing values (NaN)\n\n8\n\nRead only the specified columns, ignoring others\n\n\n\n\n\n\n\n\nWarningMemory considerations with large CSV files\n\n\n\nWhen reading very large CSV files, pandas loads the entire file into memory. If your CSV file is larger than your available RAM, you have several options:\n\nRead the file in chunks using the chunksize parameter\nUse usecols to read only the columns you need\nUse polars with lazy evaluation (discussed below)\nConvert the file to a more efficient format like Parquet\n\n\n\nFor truly large files, you can process them in chunks:\n# Process CSV in chunks\nchunk_size = 100000\nresults = []\n\n1for chunk in pd.read_csv(\"large_file.csv\", chunksize=chunk_size):\n    # Process each chunk\n2    filtered = chunk[chunk[\"volume\"] &gt; 1000000]\n3    results.append(filtered)\n\n# Combine all results\n4df = pd.concat(results, ignore_index=True)\n\n1\n\nRead the file in chunks of 100,000 rows at a time\n\n2\n\nFilter each chunk to keep only rows with high volume\n\n3\n\nAppend filtered results to a list\n\n4\n\nCombine all filtered chunks into a single DataFrame\n\n\nPandas also supports reading compressed CSV files directly. If you have a CSV file compressed as .gz, .bz2, .zip, or .xz, pandas will automatically decompress it:\n# Read a gzip-compressed CSV file\ndf = pd.read_csv(\"stock_prices.csv.gz\")\n\n# Read from a ZIP archive containing a single CSV file\ndf = pd.read_csv(\"data.zip\")\n\n\n12.1.2 Writing CSV Files with Pandas\nWriting data to CSV is just as easy using the to_csv() method:\n# Basic CSV writing\ndf.to_csv(\"output.csv\")\n\n# Customize the output\ndf.to_csv(\n    \"output.csv\",\n    index=False,           # Don't write row indices\n    float_format=\"%.4f\",   # Format floats to 4 decimal places\n    encoding=\"utf-8\"       # Specify encoding\n)\n\n# Write to a compressed file\ndf.to_csv(\"output.csv.gz\", compression=\"gzip\")\n\n\n12.1.3 Reading CSV Files with Polars\nPolars offers two ways to read CSV files: eager reading with read_csv() and lazy reading with scan_csv(). Eager reading loads the entire file into memory immediately, while lazy reading creates a query plan that can be optimized before execution.\nimport polars as pl\n\n# Eager reading - similar to pandas\ndf = pl.read_csv(\"stock_prices.csv\")\n\n# Specify data types for better performance\ndf = pl.read_csv(\n    \"stock_prices.csv\",\n1    dtypes={\"ticker\": pl.Categorical, \"volume\": pl.Int64},\n2    try_parse_dates=True\n)\n\n# Lazy reading - more efficient for large files\n3lazy_df = pl.scan_csv(\"large_file.csv\")\n\n# Build up a query (not executed yet)\nresult = (\n    lazy_df\n4    .filter(pl.col(\"volume\") &gt; 1000000)\n5    .select([\"date\", \"ticker\", \"price\"])\n    .group_by(\"ticker\")\n6    .agg(pl.col(\"price\").mean().alias(\"avg_price\"))\n)\n\n# Execute the optimized query\n7final_df = result.collect()\n\n1\n\nExplicitly specify data types for columns\n\n2\n\nAutomatically parse date-like columns\n\n3\n\nCreate a lazy frame that defers reading until needed\n\n4\n\nFilter rows (not executed yet)\n\n5\n\nSelect only the columns we need (not executed yet)\n\n6\n\nCalculate average price by ticker (not executed yet)\n\n7\n\nExecute the optimized query plan and return the result\n\n\nThe lazy approach is particularly powerful because polars can optimize the entire query before executing it. For example, if you‚Äôre filtering rows and then selecting specific columns, polars can read only those columns for only the rows that pass the filter, dramatically reducing I/O.\n\n\n\n\n\n\nTipLazy evaluation for large datasets\n\n\n\nWhen working with CSV files that are large but still fit in memory after filtering, using scan_csv() followed by filtering operations can be much faster than read_csv(). Polars will optimize the query to avoid reading unnecessary data.\nThis is especially valuable when your CSV file has many columns but you only need a few, or when you‚Äôll be filtering most rows out early in your analysis.\n\n\n\n\n12.1.4 Writing CSV Files with Polars\nWriting CSV files with polars is similarly straightforward:\n# Write a DataFrame to CSV\ndf.write_csv(\"output.csv\")\n\n# You can also write to a file-like object\nwith open(\"output.csv\", \"w\") as f:\n    df.write_csv(f)",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Data Input and File Formats</span>"
    ]
  },
  {
    "objectID": "data-analysis/io/index.html#parquet-and-apache-arrow",
    "href": "data-analysis/io/index.html#parquet-and-apache-arrow",
    "title": "12¬† Data Input and File Formats",
    "section": "12.2 Parquet and Apache Arrow",
    "text": "12.2 Parquet and Apache Arrow\nWhile CSV files are universal and human-readable, they‚Äôre not the most efficient format for large datasets. This is where Parquet comes in. Parquet is a columnar storage format originally developed for use in the Hadoop ecosystem but now widely used across the data science world. It offers several key advantages over CSV:\n\nCompression: Parquet files are typically 5-10x smaller than equivalent CSV files\nSpeed: Reading Parquet is much faster because data is stored in a binary format optimized for quick access\nType preservation: Data types are stored in the file, so there‚Äôs no need for type inference\nComplex data types: Parquet handles complex types like datetimes with timezone information natively, encoding them exactly as-is with no loss of information or conversion errors\nColumn selection: You can read only specific columns without parsing the entire file\nPredicate pushdown: Filters can be applied during reading, avoiding loading unnecessary data\n\nThe columnar storage format is key to Parquet‚Äôs efficiency. Rather than storing data row by row (like CSV), Parquet stores each column‚Äôs data together. This means if you want to read just a few columns, Parquet only needs to read those column chunks, not the entire file. It also means values in the same column can be compressed very efficiently since similar values are stored together.\nParquet is built on Apache Arrow, a cross-language development platform for in-memory data. Arrow defines a standardized columnar memory format that is highly efficient for analytical operations. Both pandas and polars can work with Arrow, and polars is built on Arrow from the ground up.\n\n\n\n\n\n\nTipWhen to use Parquet\n\n\n\nParquet is an excellent choice for:\n\nLarge datasets that you‚Äôll read multiple times\nDatasets where you often need only a subset of columns\nSharing data between different tools and languages\nLong-term data storage with efficient compression\n\nStick with CSV for:\n\nSmall datasets where file size doesn‚Äôt matter\nData that needs to be human-readable\nSharing with tools that don‚Äôt support Parquet\nQuick exports for inspection in Excel or text editors\n\nWhen I receive a dataset larger than a few megabytes, my first step is usually to run a script that reads it and converts it to Parquet. All subsequent processing then benefits from Parquet‚Äôs faster read times and smaller file size.\n\n\n\n12.2.1 Reading and Writing Parquet with Pandas\nPandas makes working with Parquet files very simple:\nimport pandas as pd\n\n# Read a Parquet file\ndf = pd.read_parquet(\"stock_prices.parquet\")\n\n# Read only specific columns\ndf = pd.read_parquet(\n    \"large_file.parquet\",\n    columns=[\"date\", \"ticker\", \"price\"]\n)\n\n# Read using PyArrow as both the engine and in-memory backend\ndf = pd.read_parquet(\n    \"stock_prices.parquet\",\n    engine=\"pyarrow\",\n    dtype_backend=\"pyarrow\"\n)\n\n# Write to Parquet\ndf.to_parquet(\"output.parquet\")\n\n# Write without the index\ndf.to_parquet(\"output.parquet\", index=False)\nUsing dtype_backend=\"pyarrow\" keeps the data in pandas using the PyArrow memory format, which is the same format that polars uses internally. This can provide significant performance improvements for certain operations, though not all pandas features are fully supported with this backend yet.\n\n\n12.2.2 Reading and Writing Parquet with Polars\nPolars has excellent support for Parquet and, like with CSV, offers both eager and lazy reading:\nimport polars as pl\n\n# Eager reading\ndf = pl.read_parquet(\"stock_prices.parquet\")\n\n# Read only specific columns\ndf = pl.read_parquet(\n    \"large_file.parquet\",\n1    columns=[\"date\", \"ticker\", \"price\"]\n)\n\n# Lazy reading with scan_parquet\n2lazy_df = pl.scan_parquet(\"large_file.parquet\")\n\n# Build an optimized query\nresult = (\n    lazy_df\n3    .filter(pl.col(\"date\") &gt;= \"2024-01-01\")\n    .select([\"ticker\", \"price\", \"volume\"])\n    .group_by(\"ticker\")\n    .agg([\n        pl.col(\"price\").mean().alias(\"avg_price\"),\n        pl.col(\"volume\").sum().alias(\"total_volume\")\n    ])\n4    .collect()\n)\n\n# Write to Parquet\ndf.write_parquet(\"output.parquet\")\n\n1\n\nOnly read the specified columns from the Parquet file\n\n2\n\nCreate a lazy frame that defers execution\n\n3\n\nFilters are pushed down to the Parquet reader when possible\n\n4\n\nExecute the query and return the result\n\n\nThe combination of Parquet‚Äôs columnar format and polars‚Äô query optimization can be extremely powerful. When you use scan_parquet(), polars can push filters down to the Parquet reader, meaning only relevant data is loaded into memory.\n\n\n12.2.3 Practical Example: CSV to Parquet Conversion\nA common workflow is to receive data as CSV (perhaps from a vendor or Bloomberg) and immediately convert it to Parquet for more efficient analysis:\nimport pandas as pd\n\n# Read CSV with appropriate data types\ndf = pd.read_csv(\n    \"stock_data.csv\",\n    parse_dates=[\"date\"],\n    dtype={\n        \"ticker\": \"category\",  # Use categorical for string columns with few unique values\n        \"volume\": \"int64\"\n    }\n)\n\n# Save as Parquet\ndf.to_parquet(\"stock_data.parquet\", compression=\"snappy\")\n\n# Future reads will be much faster\ndf_fast = pd.read_parquet(\"stock_data.parquet\")\nFor very large CSV files, you might do this conversion with polars:\nimport polars as pl\n\n# Lazy read CSV, optimize, and write to Parquet\n(\n1    pl.scan_csv(\"large_stock_data.csv\")\n    .select([\n2        pl.col(\"date\").str.to_date(),\n3        pl.col(\"ticker\").cast(pl.Categorical),\n        pl.col(\"price\").cast(pl.Float64),\n        pl.col(\"volume\").cast(pl.Int64)\n    ])\n4    .sink_parquet(\"stock_data.parquet\")\n)\n\n1\n\nCreate a lazy frame from the CSV file\n\n2\n\nParse the date string column to a proper date type\n\n3\n\nConvert ticker to categorical for memory efficiency\n\n4\n\nStream results directly to Parquet without loading into memory\n\n\nThe sink_parquet() method is particularly efficient because it streams the data to disk without loading everything into memory first.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Data Input and File Formats</span>"
    ]
  },
  {
    "objectID": "data-analysis/io/index.html#excel-files",
    "href": "data-analysis/io/index.html#excel-files",
    "title": "12¬† Data Input and File Formats",
    "section": "12.3 Excel Files",
    "text": "12.3 Excel Files\nDespite the advantages of formats like CSV and Parquet, Excel remains ubiquitous in finance. Analysts send Excel files, regulatory filings include Excel attachments, and many financial models live in Excel workbooks. As a result, being able to read from and write to Excel is an essential skill for empirical finance work.\nExcel files (.xlsx) are actually compressed archives containing XML files that describe the workbook structure, data, and formatting. This makes them more complex than simple text formats but also allows them to store multiple sheets, formulas, formatting, and metadata in a single file.\n\n12.3.1 Reading Excel Files with Pandas\nPandas relies on third-party libraries for Excel support. These are optional dependencies that are not installed by default when you install pandas. If a required library is missing, pandas will notify you when you try to use the related function and indicate which package to install. You can then install the missing dependency with uv add.\nPandas provides robust Excel support through the read_excel() function:\nimport pandas as pd\n\n# Read the first sheet\ndf = pd.read_excel(\"financial_data.xlsx\")\n\n# Read a specific sheet by name\n1df = pd.read_excel(\"financial_data.xlsx\", sheet_name=\"Returns\")\n\n# Read a specific sheet by index (0-indexed)\n2df = pd.read_excel(\"financial_data.xlsx\", sheet_name=1)\n\n# Read multiple sheets at once\nsheets_dict = pd.read_excel(\n    \"financial_data.xlsx\",\n3    sheet_name=[\"Returns\", \"Fundamentals\"]\n)\n# Returns a dictionary: {\"Returns\": df1, \"Fundamentals\": df2}\n\n# Read all sheets\n4all_sheets = pd.read_excel(\"financial_data.xlsx\", sheet_name=None)\n\n1\n\nSpecify a sheet by its name\n\n2\n\nSpecify a sheet by its position (0-indexed)\n\n3\n\nRead multiple sheets by passing a list of names\n\n4\n\nUse None to read all sheets as a dictionary\n\n\nExcel files often have headers on multiple rows, merged cells, or data that doesn‚Äôt start in cell A1. Pandas provides parameters to handle these cases:\n# Skip rows at the beginning\ndf = pd.read_excel(\n    \"report.xlsx\",\n    skiprows=3,  # Skip first 3 rows\n    sheet_name=\"Data\"\n)\n\n# Specify header row\ndf = pd.read_excel(\n    \"report.xlsx\",\n    header=2  # Use row 2 (0-indexed) as column names\n)\n\n# Specify which columns to read\ndf = pd.read_excel(\n    \"data.xlsx\",\n    usecols=\"A:D\"  # Read only columns A through D\n)\n\n# Or specify by column names\ndf = pd.read_excel(\n    \"data.xlsx\",\n    usecols=[\"Date\", \"Price\", \"Volume\"]\n)\n\n# Handle dates\ndf = pd.read_excel(\n    \"data.xlsx\",\n    parse_dates=[\"date_column\"]\n)\n\n\n\n\n\n\nWarningExcel file size and performance\n\n\n\nReading Excel files is significantly slower than reading CSV or Parquet files, especially for large datasets. If you‚Äôre repeatedly reading the same Excel file, consider converting it to Parquet for better performance.\nAdditionally, Excel has a row limit of 1,048,576 rows. If you‚Äôre working with larger datasets, you‚Äôll need to use a different format.\n\n\n\n\n12.3.2 Writing Excel Files with Pandas\nWriting to Excel is similarly straightforward:\n# Write to a single sheet\ndf.to_excel(\"output.xlsx\", sheet_name=\"Data\", index=False)\n\n# Write multiple DataFrames to different sheets\nwith pd.ExcelWriter(\"output.xlsx\") as writer:\n    df_returns.to_excel(writer, sheet_name=\"Returns\", index=False)\n    df_fundamentals.to_excel(writer, sheet_name=\"Fundamentals\", index=False)\n    df_summary.to_excel(writer, sheet_name=\"Summary\", index=False)\n\n# Specify float formatting\ndf.to_excel(\n    \"output.xlsx\",\n    sheet_name=\"Data\",\n    index=False,\n    float_format=\"%.4f\"\n)\nYou can also use the ExcelWriter context manager to add formatting:\n# Create an Excel file with formatting\nwith pd.ExcelWriter(\n    \"formatted_output.xlsx\",\n    engine=\"xlsxwriter\"\n) as writer:\n    df.to_excel(writer, sheet_name=\"Data\", index=False)\n\n    # Get the workbook and worksheet objects\n    workbook = writer.book\n    worksheet = writer.sheets[\"Data\"]\n\n    # Add a format\n    format1 = workbook.add_format({\"num_format\": \"$#,##0.00\"})\n\n    # Apply formatting to a column (e.g., column B)\n    worksheet.set_column(\"B:B\", 12, format1)\n\n\n12.3.3 Reading Excel Files with Polars\nPolars also supports reading Excel files, though with a focus on data extraction rather than preserving formatting:\nimport polars as pl\n\n# Read the first sheet\ndf = pl.read_excel(\"financial_data.xlsx\")\n\n# Read a specific sheet\ndf = pl.read_excel(\n    \"financial_data.xlsx\",\n    sheet_name=\"Returns\"\n)\n\n# Read with specific sheet index\ndf = pl.read_excel(\"financial_data.xlsx\", sheet_id=2)\nPolars uses the fastexcel engine by default, which is optimized for speed. For more complex Excel files, you might need to install additional engines like openpyxl or xlsx2csv.\n\n\n12.3.4 Writing Excel Files with Polars\nPolars can write DataFrames to Excel files, though it requires the xlsxwriter library:\nimport polars as pl\n\n# Write to Excel (requires xlsxwriter)\ndf.write_excel(\"output.xlsx\")\n\n# Write to a specific worksheet\ndf.write_excel(\"output.xlsx\", worksheet=\"MyData\")\n\n\n\n\n\n\nTipExcel as a data exchange format\n\n\n\nWhile Excel is convenient for sharing data with non-technical stakeholders, it‚Äôs not ideal for:\n\nLarge datasets (performance and row limits)\nPreserving exact data types (dates can be problematic)\nVersion control (binary format is hard to diff)\nReproducible research (formulas and manual edits aren‚Äôt tracked)\n\nConsider using Excel for final outputs and presentation, but use CSV or Parquet for data storage and analysis pipelines.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Data Input and File Formats</span>"
    ]
  },
  {
    "objectID": "data-analysis/io/index.html#choosing-the-right-format",
    "href": "data-analysis/io/index.html#choosing-the-right-format",
    "title": "12¬† Data Input and File Formats",
    "section": "12.4 Choosing the Right Format",
    "text": "12.4 Choosing the Right Format\nWe‚Äôve covered three common file formats: CSV, Parquet, and Excel. Each has its place in a data analysis workflow. Here‚Äôs a decision guide:\nUse CSV when:\n\nWorking with small to medium datasets (under 1 GB)\nYou need human-readable data\nSharing data with tools that don‚Äôt support other formats\nYou value simplicity and universality over performance\nVersion controlling data (text files work well with git)\n\nUse Parquet when:\n\nWorking with large datasets (multiple GB or more)\nYou‚Äôll read the data multiple times\nYou often need only a subset of columns\nPerformance and storage efficiency matter\nSharing data between different tools and languages\nBuilding data pipelines and ETL processes\n\nUse Excel when:\n\nSharing data with business users\nYou need multiple related tables in one file (multiple sheets)\nThe recipient expects Excel format\nWorking with small datasets where performance doesn‚Äôt matter\nYou need to include formatting, formulas, or charts\n\nIn practice, many empirical finance workflows involve all three: receiving data in Excel or CSV, converting to Parquet for efficient analysis, and exporting results back to Excel for presentation.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Data Input and File Formats</span>"
    ]
  },
  {
    "objectID": "data-analysis/io/index.html#best-practices-for-data-io",
    "href": "data-analysis/io/index.html#best-practices-for-data-io",
    "title": "12¬† Data Input and File Formats",
    "section": "12.5 Best Practices for Data I/O",
    "text": "12.5 Best Practices for Data I/O\nAs you work with different file formats, keep these best practices in mind:\n1. Specify data types explicitly\nWhen reading data, especially CSV files, explicitly specifying data types can improve both performance and correctness:\n# Pandas\ndf = pd.read_csv(\n    \"data.csv\",\n    dtype={\n        \"ticker\": \"category\",\n        \"volume\": \"int64\",\n        \"price\": \"float64\"\n    },\n    parse_dates=[\"date\"]\n)\n\n# Polars\ndf = pl.read_csv(\n    \"data.csv\",\n    dtypes={\n        \"ticker\": pl.Categorical,\n        \"volume\": pl.Int64,\n        \"price\": pl.Float64\n    },\n    try_parse_dates=True\n)\n2. Use compression for large files\nBoth CSV and Parquet support compression. For CSV files you‚Äôll store long-term, use gzip compression:\n# Pandas - automatic compression based on filename\ndf.to_csv(\"data.csv.gz\")\n\n# Explicit compression\ndf.to_parquet(\"data.parquet\", compression=\"snappy\")\n3. Consider storage vs.¬†memory tradeoffs\nParquet files with strong compression (like gzip or zstd) are smaller on disk but take longer to read. For files you‚Äôll read frequently, snappy compression offers a good balance. For archival storage, use stronger compression.\n4. Validate data after reading\nAfter reading data, especially from external sources, validate that it matches your expectations:\n\nimport pandas as pd\nimport numpy as np\n\n# Sample data with some issues to detect\ndf = pd.DataFrame({\n    \"date\": pd.to_datetime([\"2024-01-02\", \"2024-01-02\", \"2024-01-03\", \"2024-01-03\"]),\n    \"ticker\": [\"AAPL\", \"MSFT\", \"AAPL\", \"AAPL\"],\n    \"price\": [185.64, 374.58, None, 184.25],\n    \"volume\": [52000000, 28000000, 48000000, 48000000]\n})\n\n\n# Check for missing values\ndf.isnull().sum()\n\ndate      0\nticker    0\nprice     1\nvolume    0\ndtype: int64\n\n\n\n# Check data types\ndf.dtypes\n\ndate      datetime64[ns]\nticker            object\nprice            float64\nvolume             int64\ndtype: object\n\n\n\n# Basic statistics\ndf.describe()\n\n\n\n\n\n\n\n\ndate\nprice\nvolume\n\n\n\n\ncount\n4\n3.000000\n4.000000e+00\n\n\nmean\n2024-01-02 12:00:00\n248.156667\n4.400000e+07\n\n\nmin\n2024-01-02 00:00:00\n184.250000\n2.800000e+07\n\n\n25%\n2024-01-02 00:00:00\n184.945000\n4.300000e+07\n\n\n50%\n2024-01-02 12:00:00\n185.640000\n4.800000e+07\n\n\n75%\n2024-01-03 00:00:00\n280.110000\n4.900000e+07\n\n\nmax\n2024-01-03 00:00:00\n374.580000\n5.200000e+07\n\n\nstd\nNaN\n109.488024\n1.083205e+07\n\n\n\n\n\n\n\n\n# Check for duplicates\ndf.duplicated().sum()\n\nnp.int64(0)\n\n\n5. Document your data sources\nKeep a record of where your data came from, when it was downloaded, and any transformations applied. This is crucial for reproducibility:\n# Add metadata when saving\nmetadata = {\n    \"source\": \"Bloomberg Terminal\",\n    \"download_date\": \"2024-01-15\",\n    \"description\": \"Daily stock prices for S&P 500 constituents\"\n}\n\n# Parquet supports metadata\ndf.to_parquet(\"data.parquet\", metadata=metadata)\nWorking with data files is the foundation of empirical finance research. By understanding the strengths and limitations of different file formats, and by using the right tools for each format, you can build efficient, reproducible data analysis pipelines. In the next chapter, we‚Äôll explore data cleaning techniques to prepare your data for analysis.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Data Input and File Formats</span>"
    ]
  },
  {
    "objectID": "data-analysis/cleaning/index.html",
    "href": "data-analysis/cleaning/index.html",
    "title": "13¬† Data Cleaning",
    "section": "",
    "text": "13.1 Detecting and Handling Duplicates\nData cleaning is one of the most critical and time-consuming steps in any empirical analysis. In finance, working with real-world data means confronting issues like duplicate records, missing observations, data entry errors, and outliers. These problems are not merely technical annoyances‚Äîthey can lead to incorrect conclusions, flawed trading strategies, or misleading research findings if not handled properly.\nThe goal of data cleaning is not to make data ‚Äúperfect‚Äù but to make it suitable for analysis. This means understanding the nature of data quality issues, their potential causes, and the trade-offs involved in different cleaning approaches. In this chapter, we will cover the essential techniques for detecting and handling common data quality problems using Python‚Äôs main data manipulation libraries: pandas and Polars.\nBefore we begin, let‚Äôs understand an important principle: data cleaning decisions should be documented and reproducible. Every choice you make‚Äîwhether to drop duplicates, impute missing values, or remove outliers‚Äîaffects your results. Your code should clearly show what cleaning steps were taken and why, so that others (including your future self) can understand and validate your approach.\nDuplicate records are a common problem in financial datasets. They can arise from multiple sources: data feed errors that transmit the same trade twice, mistakes in data aggregation where the same record appears in multiple files, or simple human error in manual data entry. Duplicates are particularly problematic because they can distort statistical analyses, inflate trading volumes, and create artificial patterns in the data.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-analysis/cleaning/index.html#detecting-and-handling-duplicates",
    "href": "data-analysis/cleaning/index.html#detecting-and-handling-duplicates",
    "title": "13¬† Data Cleaning",
    "section": "",
    "text": "13.1.1 Identifying duplicates\nThe first step in handling duplicates is identifying them. A duplicate can be defined in different ways depending on your data structure and business context.\n\n\n\n\n\n\nNoteTypes of duplicates\n\n\n\n\nComplete duplicates: All columns have identical values\nPartial duplicates: Only certain key columns are identical (e.g., same date and ticker, but different price)\nNear duplicates: Values are very similar but not exactly identical (often due to floating-point precision)\n\nThe appropriate definition depends on your data and use case.\n\n\nLet‚Äôs start with a simple example using stock price data:\n\nimport pandas as pd\nimport polars as pl\nfrom datetime import datetime, timedelta\n\n# Create sample stock price data with duplicates\ndates = pd.date_range('2024-01-01', periods=5, freq='D')\ndata = {\n1    'date': list(dates) + [dates[2]],  # Duplicate date\n    'ticker': ['AAPL', 'AAPL', 'AAPL', 'AAPL', 'AAPL', 'AAPL'],\n    'price': [150.0, 152.5, 155.0, 153.0, 157.0, 155.0],\n    'volume': [1000000, 1100000, 1200000, 950000, 1300000, 1200000]\n}\n\ndf_pd = pd.DataFrame(data)\ndf_pd\n\n\n1\n\nAdding a duplicate of the third date to create a duplicate record.\n\n\n\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\n1000000\n\n\n1\n2024-01-02\nAAPL\n152.5\n1100000\n\n\n2\n2024-01-03\nAAPL\n155.0\n1200000\n\n\n3\n2024-01-04\nAAPL\n153.0\n950000\n\n\n4\n2024-01-05\nAAPL\n157.0\n1300000\n\n\n5\n2024-01-03\nAAPL\n155.0\n1200000\n\n\n\n\n\n\n\nNow let‚Äôs detect duplicates. The duplicated() method returns a boolean Series indicating which rows are duplicates:\n\n# Check for complete duplicates\ndf_pd.duplicated()\n\n0    False\n1    False\n2    False\n3    False\n4    False\n5     True\ndtype: bool\n\n\nWe can also check for duplicates based on specific columns:\n\n# Check for duplicates based on specific columns (date and ticker)\ndf_pd.duplicated(subset=['date', 'ticker'])\n\n0    False\n1    False\n2    False\n3    False\n4    False\n5     True\ndtype: bool\n\n\nTo see which rows are duplicates, we can filter the DataFrame:\n\n# See which rows are duplicates (keep=False marks all duplicates)\ndf_pd[df_pd.duplicated(subset=['date', 'ticker'], keep=False)]\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\n\n\n\n\n2\n2024-01-03\nAAPL\n155.0\n1200000\n\n\n5\n2024-01-03\nAAPL\n155.0\n1200000\n\n\n\n\n\n\n\nThe keep parameter controls which duplicate to mark:\n\nkeep='first' (default): Mark all duplicates as True except the first occurrence\nkeep='last': Mark all duplicates as True except the last occurrence\nkeep=False: Mark all duplicates as True, including the first occurrence\n\nNow let‚Äôs see the same operations in Polars:\n\ndf_pl = pl.DataFrame(data)\ndf_pl\n\n\nshape: (6, 4)\n\n\n\ndate\nticker\nprice\nvolume\n\n\ndatetime[Œºs]\nstr\nf64\ni64\n\n\n\n\n2024-01-01 00:00:00\n\"AAPL\"\n150.0\n1000000\n\n\n2024-01-02 00:00:00\n\"AAPL\"\n152.5\n1100000\n\n\n2024-01-03 00:00:00\n\"AAPL\"\n155.0\n1200000\n\n\n2024-01-04 00:00:00\n\"AAPL\"\n153.0\n950000\n\n\n2024-01-05 00:00:00\n\"AAPL\"\n157.0\n1300000\n\n\n2024-01-03 00:00:00\n\"AAPL\"\n155.0\n1200000\n\n\n\n\n\n\nTo find duplicates in Polars, we use is_duplicated():\n\n# Check for duplicates based on date and ticker\ndf_pl.filter(pl.struct(['date', 'ticker']).is_duplicated())\n\n\nshape: (2, 4)\n\n\n\ndate\nticker\nprice\nvolume\n\n\ndatetime[Œºs]\nstr\nf64\ni64\n\n\n\n\n2024-01-03 00:00:00\n\"AAPL\"\n155.0\n1200000\n\n\n2024-01-03 00:00:00\n\"AAPL\"\n155.0\n1200000\n\n\n\n\n\n\n\n\n13.1.2 Removing duplicates\nOnce we‚Äôve identified duplicates, we need to decide how to handle them. The most common approaches are:\n\nDrop all duplicates: Keep only the first (or last) occurrence\nAggregate duplicates: Combine duplicate rows using aggregation (e.g., average prices)\nManual investigation: For small numbers of duplicates, inspect each case individually\n\nLet‚Äôs see how to implement these approaches. In pandas, we use drop_duplicates():\n\n# pandas: Drop duplicates, keeping first occurrence\ndf_pd_clean = df_pd.drop_duplicates(subset=['date', 'ticker'], keep='first')\ndf_pd_clean\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\n1000000\n\n\n1\n2024-01-02\nAAPL\n152.5\n1100000\n\n\n2\n2024-01-03\nAAPL\n155.0\n1200000\n\n\n3\n2024-01-04\nAAPL\n153.0\n950000\n\n\n4\n2024-01-05\nAAPL\n157.0\n1300000\n\n\n\n\n\n\n\nIn Polars, we use the unique() method:\n\n# Polars: Drop duplicates\ndf_pl_clean = df_pl.unique(subset=['date', 'ticker'], keep='first')\ndf_pl_clean\n\n\nshape: (5, 4)\n\n\n\ndate\nticker\nprice\nvolume\n\n\ndatetime[Œºs]\nstr\nf64\ni64\n\n\n\n\n2024-01-03 00:00:00\n\"AAPL\"\n155.0\n1200000\n\n\n2024-01-04 00:00:00\n\"AAPL\"\n153.0\n950000\n\n\n2024-01-01 00:00:00\n\"AAPL\"\n150.0\n1000000\n\n\n2024-01-02 00:00:00\n\"AAPL\"\n152.5\n1100000\n\n\n2024-01-05 00:00:00\n\"AAPL\"\n157.0\n1300000\n\n\n\n\n\n\nFor the aggregation approach, suppose we want to take the average price and total volume when duplicates exist:\n\n# pandas: Aggregate duplicates\ndf_pd_agg = df_pd.groupby(['date', 'ticker'], as_index=False).agg({\n    'price': 'mean',\n    'volume': 'sum'\n})\ndf_pd_agg\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\n1000000\n\n\n1\n2024-01-02\nAAPL\n152.5\n1100000\n\n\n2\n2024-01-03\nAAPL\n155.0\n2400000\n\n\n3\n2024-01-04\nAAPL\n153.0\n950000\n\n\n4\n2024-01-05\nAAPL\n157.0\n1300000\n\n\n\n\n\n\n\n\n# Polars: Aggregate duplicates\ndf_pl_agg = df_pl.group_by(['date', 'ticker']).agg([\n    pl.col('price').mean(),\n    pl.col('volume').sum()\n])\ndf_pl_agg\n\n\nshape: (5, 4)\n\n\n\ndate\nticker\nprice\nvolume\n\n\ndatetime[Œºs]\nstr\nf64\ni64\n\n\n\n\n2024-01-01 00:00:00\n\"AAPL\"\n150.0\n1000000\n\n\n2024-01-03 00:00:00\n\"AAPL\"\n155.0\n2400000\n\n\n2024-01-05 00:00:00\n\"AAPL\"\n157.0\n1300000\n\n\n2024-01-02 00:00:00\n\"AAPL\"\n152.5\n1100000\n\n\n2024-01-04 00:00:00\n\"AAPL\"\n153.0\n950000\n\n\n\n\n\n\n\n\n\n\n\n\nWarningCommon pitfall: implicit duplicates\n\n\n\nBe careful about duplicates that arise from data structure decisions. For example, if you merge two datasets and accidentally create a many-to-many join, you may generate duplicates without realizing it. Always check the row count before and after joins to ensure you haven‚Äôt inadvertently created duplicates. We discuss joins in detail in Chapter 16.\n\n\n\n\n13.1.3 Best practices for duplicate handling\n\nAlways investigate before removing: Look at a sample of duplicates to understand why they exist\nDocument your decisions: Add comments explaining which columns define a duplicate and why\nKeep track of removals: Log how many duplicates you removed and their characteristics\nConsider the source: Different data sources may have different duplicate patterns\n\n\n# Example: Comprehensive duplicate handling with logging\ndef handle_duplicates(df, subset_cols, keep='first', verbose=True):\n    \"\"\"\n    Remove duplicates with logging.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input dataframe\n    subset_cols : list\n        Columns that define a duplicate\n    keep : str\n        Which duplicate to keep ('first', 'last', or False)\n    verbose : bool\n        Whether to print diagnostic information\n\n    Returns\n    -------\n    pd.DataFrame\n        Cleaned dataframe\n    \"\"\"\n    initial_rows = len(df)\n    duplicated_mask = df.duplicated(subset=subset_cols, keep=keep)\n    n_duplicates = duplicated_mask.sum()\n\n    if verbose:\n        print(f\"Initial rows: {initial_rows}\")\n        print(f\"Duplicates found: {n_duplicates}\")\n        if n_duplicates &gt; 0:\n            print(f\"Duplicate rows (sample):\")\n            print(df[duplicated_mask].head())\n\n    df_clean = df[~duplicated_mask].copy()\n\n    if verbose:\n        print(f\"Final rows: {len(df_clean)}\")\n        print(f\"Rows removed: {initial_rows - len(df_clean)}\")\n\n    return df_clean\n\n# Use the function\ndf_cleaned = handle_duplicates(df_pd, subset_cols=['date', 'ticker'])\n\nInitial rows: 6\nDuplicates found: 1\nDuplicate rows (sample):\n        date ticker  price   volume\n5 2024-01-03   AAPL  155.0  1200000\nFinal rows: 5\nRows removed: 1",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-analysis/cleaning/index.html#missing-data-detection-and-imputation",
    "href": "data-analysis/cleaning/index.html#missing-data-detection-and-imputation",
    "title": "13¬† Data Cleaning",
    "section": "13.2 Missing Data: Detection and Imputation",
    "text": "13.2 Missing Data: Detection and Imputation\nMissing data is ubiquitous in financial datasets. Stock markets close on holidays, companies report earnings quarterly but not daily, economic indicators are released with delays, and data collection systems occasionally fail. How you handle missing data can significantly impact your analysis, and there is rarely a single ‚Äúcorrect‚Äù approach.\n\n13.2.1 Understanding missing data mechanisms\nBefore deciding how to handle missing data, it‚Äôs important to understand why data might be missing. Statisticians classify missing data into three categories:\n\nMissing Completely at Random (MCAR): The probability that a value is missing is unrelated to any observed or unobserved data. For example, if a data feed randomly drops 1% of all observations due to network issues, the missing data is MCAR.\nMissing at Random (MAR): The probability that a value is missing depends on observed data but not on the missing value itself. For example, small-cap stocks might have more missing volume data than large-cap stocks, but conditional on market cap, the missingness is random.\nMissing Not at Random (MNAR): The probability that a value is missing depends on the missing value itself. For example, companies might be less likely to report earnings when performance is poor, or traders might fail to report large losses.\n\n\n\n\n\n\n\nImportantWhy this matters\n\n\n\nThe type of missingness affects which imputation methods are valid. MCAR is the easiest to handle (you can often just drop missing observations), while MNAR is the most problematic because missing data itself carries information. Unfortunately, in real financial data, you often cannot definitively determine which mechanism is operating, so you need to think carefully about the context.\n\n\n\n\n13.2.2 Detecting missing data\nPython uses different representations for missing data depending on the data type:\n\nNaN (Not a Number): Used for missing floating-point values\nNone: Python‚Äôs built-in null value\nNaT (Not a Time): Used for missing datetime values\npd.NA: pandas‚Äô new missing indicator for all dtypes\n\n\n\n\n\n\n\nNoteHistorical context: missing values in pandas\n\n\n\nInitially, pandas used NumPy as its backend and could not represent missing values explicitly. Instead, it used NaN for floats and None (a Python object) for strings. This approach had important limitations:\n\nFor integers, pandas would first convert all values in the column to float and use NaN to represent missing values. This meant you couldn‚Äôt have a truly integer column with missing values.\nNaN has a specific meaning in floating-point arithmetic: it represents a value that is ‚Äúnot a number,‚Äù such as the result of an undefined operation (e.g., log(-1)). Reusing NaN to mean ‚Äúmissing‚Äù conflates two distinct concepts and complicates handling of true NaN results.\n\nPolars and newer backends for pandas (such as PyArrow) explicitly support missing data indicators for all column types, avoiding these limitations.\n\n\nLet‚Äôs create a dataset with missing values:\n\nimport numpy as np\n\ndata_missing = {\n    'date': pd.date_range('2024-01-01', periods=10, freq='D'),\n    'ticker': ['AAPL'] * 10,\n    'price': [150.0, 152.5, np.nan, 153.0, 157.0, np.nan, 159.0, 158.5, np.nan, 161.0],\n    'volume': [1000000, np.nan, 1200000, 950000, np.nan, 1100000, 1300000, np.nan, 1250000, 1400000],\n    'dividend': [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n}\n\ndf_missing = pd.DataFrame(data_missing)\ndf_missing\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\n1000000.0\n0.00\n\n\n1\n2024-01-02\nAAPL\n152.5\nNaN\n0.00\n\n\n2\n2024-01-03\nAAPL\nNaN\n1200000.0\n0.00\n\n\n3\n2024-01-04\nAAPL\n153.0\n950000.0\n0.25\n\n\n4\n2024-01-05\nAAPL\n157.0\nNaN\n0.00\n\n\n5\n2024-01-06\nAAPL\nNaN\n1100000.0\n0.00\n\n\n6\n2024-01-07\nAAPL\n159.0\n1300000.0\n0.00\n\n\n7\n2024-01-08\nAAPL\n158.5\nNaN\n0.00\n\n\n8\n2024-01-09\nAAPL\nNaN\n1250000.0\n0.00\n\n\n9\n2024-01-10\nAAPL\n161.0\n1400000.0\n0.00\n\n\n\n\n\n\n\nNow let‚Äôs detect missing values. The isna() method returns a boolean DataFrame indicating which values are missing:\n\ndf_missing.isna()\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n5\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n6\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n7\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n8\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n9\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\nWe can count missing values per column:\n\ndf_missing.isna().sum()\n\ndate        0\nticker      0\nprice       3\nvolume      3\ndividend    0\ndtype: int64\n\n\nOr calculate the percentage of missing values:\n\ndf_missing.isna().mean() * 100\n\ndate         0.0\nticker       0.0\nprice       30.0\nvolume      30.0\ndividend     0.0\ndtype: float64\n\n\nTo see rows with any missing values:\n\ndf_missing[df_missing.isna().any(axis=1)]\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\n\n\n1\n2024-01-02\nAAPL\n152.5\nNaN\n0.0\n\n\n2\n2024-01-03\nAAPL\nNaN\n1200000.0\n0.0\n\n\n4\n2024-01-05\nAAPL\n157.0\nNaN\n0.0\n\n\n5\n2024-01-06\nAAPL\nNaN\n1100000.0\n0.0\n\n\n7\n2024-01-08\nAAPL\n158.5\nNaN\n0.0\n\n\n8\n2024-01-09\nAAPL\nNaN\n1250000.0\n0.0\n\n\n\n\n\n\n\nIn Polars, we use null_count() to count missing values:\n\n# Polars version - convert from pandas DataFrame\n# (Polars uses None/null for missing values, not np.nan)\ndf_missing_pl = pl.from_pandas(df_missing)\n\ndf_missing_pl.null_count()\n\n\nshape: (1, 5)\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n0\n0\n3\n3\n0\n\n\n\n\n\n\nWe can filter rows with missing values in a specific column:\n\ndf_missing_pl.filter(pl.col('price').is_null())\n\n\nshape: (3, 5)\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\ndatetime[ns]\nstr\nf64\nf64\nf64\n\n\n\n\n2024-01-03 00:00:00\n\"AAPL\"\nnull\n1.2e6\n0.0\n\n\n2024-01-06 00:00:00\n\"AAPL\"\nnull\n1.1e6\n0.0\n\n\n2024-01-09 00:00:00\n\"AAPL\"\nnull\n1.25e6\n0.0\n\n\n\n\n\n\n\n\n13.2.3 Handling missing data: deletion\nThe simplest approach to missing data is to delete it. There are two main strategies:\n\nListwise deletion (complete case analysis): Rows with any missing values are completely dropped from the sample, so they are not used in any calculation or analysis. This approach is simple and ensures consistency across analyses, but can significantly reduce sample size if missingness is widespread.\nPairwise deletion: For each calculation or analysis (e.g., for each regression), use all observations for which we have the required variables. This approach maximizes the use of available data but can lead to different sample sizes for different analyses, making comparisons difficult.\n\nListwise deletion removes any row with missing values:\n\ndf_complete = df_missing.dropna()\ndf_complete\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\n1000000.0\n0.00\n\n\n3\n2024-01-04\nAAPL\n153.0\n950000.0\n0.25\n\n\n6\n2024-01-07\nAAPL\n159.0\n1300000.0\n0.00\n\n\n9\n2024-01-10\nAAPL\n161.0\n1400000.0\n0.00\n\n\n\n\n\n\n\n\nprint(f\"Rows removed: {len(df_missing) - len(df_complete)}\")\n\nRows removed: 6\n\n\nWe can also drop rows only if specific columns are missing:\n\ndf_price_complete = df_missing.dropna(subset=['price'])\ndf_price_complete\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\n1000000.0\n0.00\n\n\n1\n2024-01-02\nAAPL\n152.5\nNaN\n0.00\n\n\n3\n2024-01-04\nAAPL\n153.0\n950000.0\n0.25\n\n\n4\n2024-01-05\nAAPL\n157.0\nNaN\n0.00\n\n\n6\n2024-01-07\nAAPL\n159.0\n1300000.0\n0.00\n\n\n7\n2024-01-08\nAAPL\n158.5\nNaN\n0.00\n\n\n9\n2024-01-10\nAAPL\n161.0\n1400000.0\n0.00\n\n\n\n\n\n\n\nOr drop columns with any missing values:\n\ndf_no_missing_cols = df_missing.dropna(axis=1)\ndf_no_missing_cols\n\n\n\n\n\n\n\n\ndate\nticker\ndividend\n\n\n\n\n0\n2024-01-01\nAAPL\n0.00\n\n\n1\n2024-01-02\nAAPL\n0.00\n\n\n2\n2024-01-03\nAAPL\n0.00\n\n\n3\n2024-01-04\nAAPL\n0.25\n\n\n4\n2024-01-05\nAAPL\n0.00\n\n\n5\n2024-01-06\nAAPL\n0.00\n\n\n6\n2024-01-07\nAAPL\n0.00\n\n\n7\n2024-01-08\nAAPL\n0.00\n\n\n8\n2024-01-09\nAAPL\n0.00\n\n\n9\n2024-01-10\nAAPL\n0.00\n\n\n\n\n\n\n\nIn Polars, we use drop_nulls():\n\ndf_complete_pl = df_missing_pl.drop_nulls()\ndf_complete_pl\n\n\nshape: (4, 5)\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\ndatetime[ns]\nstr\nf64\nf64\nf64\n\n\n\n\n2024-01-01 00:00:00\n\"AAPL\"\n150.0\n1e6\n0.0\n\n\n2024-01-04 00:00:00\n\"AAPL\"\n153.0\n950000.0\n0.25\n\n\n2024-01-07 00:00:00\n\"AAPL\"\n159.0\n1.3e6\n0.0\n\n\n2024-01-10 00:00:00\n\"AAPL\"\n161.0\n1.4e6\n0.0\n\n\n\n\n\n\n\n# Drop rows with null in specific column\ndf_price_complete_pl = df_missing_pl.drop_nulls(subset=['price'])\ndf_price_complete_pl\n\n\nshape: (7, 5)\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\ndatetime[ns]\nstr\nf64\nf64\nf64\n\n\n\n\n2024-01-01 00:00:00\n\"AAPL\"\n150.0\n1e6\n0.0\n\n\n2024-01-02 00:00:00\n\"AAPL\"\n152.5\nnull\n0.0\n\n\n2024-01-04 00:00:00\n\"AAPL\"\n153.0\n950000.0\n0.25\n\n\n2024-01-05 00:00:00\n\"AAPL\"\n157.0\nnull\n0.0\n\n\n2024-01-07 00:00:00\n\"AAPL\"\n159.0\n1.3e6\n0.0\n\n\n2024-01-08 00:00:00\n\"AAPL\"\n158.5\nnull\n0.0\n\n\n2024-01-10 00:00:00\n\"AAPL\"\n161.0\n1.4e6\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\nWarningWhen deletion is problematic\n\n\n\nDeletion can introduce bias if:\n\nYou lose a large percentage of your data\nMissingness is not random (MAR or MNAR)\nMissing data occurs in key variables\n\nIn time series analysis, deleting observations can break the temporal structure of your data. Use deletion cautiously and always check how much data you‚Äôre losing.\n\n\n\n\n13.2.4 Handling missing data: imputation\nImputation means filling in missing values with plausible estimates. There are many imputation strategies, each with different assumptions and use cases.\n\n13.2.4.1 Simple imputation methods\n\nForward fill: Use the last observed value\nBackward fill: Use the next observed value\nMean/median imputation: Replace with the column mean or median\nZero/constant imputation: Replace with zero or another constant\n\nIn empirical finance, we usually want to avoid introducing look-ahead bias or imputing information that was not available at the time of observation. For this reason, forward fill and zero/constant imputation are the most commonly used imputation methods in financial research.\nForward fill uses the last observed value:\n\ndf_ffill = df_missing.copy()\ndf_ffill[['price', 'volume']] = df_ffill[['price', 'volume']].ffill()\ndf_ffill\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\n1000000.0\n0.00\n\n\n1\n2024-01-02\nAAPL\n152.5\n1000000.0\n0.00\n\n\n2\n2024-01-03\nAAPL\n152.5\n1200000.0\n0.00\n\n\n3\n2024-01-04\nAAPL\n153.0\n950000.0\n0.25\n\n\n4\n2024-01-05\nAAPL\n157.0\n950000.0\n0.00\n\n\n5\n2024-01-06\nAAPL\n157.0\n1100000.0\n0.00\n\n\n6\n2024-01-07\nAAPL\n159.0\n1300000.0\n0.00\n\n\n7\n2024-01-08\nAAPL\n158.5\n1300000.0\n0.00\n\n\n8\n2024-01-09\nAAPL\n158.5\n1250000.0\n0.00\n\n\n9\n2024-01-10\nAAPL\n161.0\n1400000.0\n0.00\n\n\n\n\n\n\n\nBackward fill uses the next observed value:\n\ndf_bfill = df_missing.copy()\ndf_bfill[['price', 'volume']] = df_bfill[['price', 'volume']].bfill()\ndf_bfill\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\n1000000.0\n0.00\n\n\n1\n2024-01-02\nAAPL\n152.5\n1200000.0\n0.00\n\n\n2\n2024-01-03\nAAPL\n153.0\n1200000.0\n0.00\n\n\n3\n2024-01-04\nAAPL\n153.0\n950000.0\n0.25\n\n\n4\n2024-01-05\nAAPL\n157.0\n1100000.0\n0.00\n\n\n5\n2024-01-06\nAAPL\n159.0\n1100000.0\n0.00\n\n\n6\n2024-01-07\nAAPL\n159.0\n1300000.0\n0.00\n\n\n7\n2024-01-08\nAAPL\n158.5\n1250000.0\n0.00\n\n\n8\n2024-01-09\nAAPL\n161.0\n1250000.0\n0.00\n\n\n9\n2024-01-10\nAAPL\n161.0\n1400000.0\n0.00\n\n\n\n\n\n\n\nMean imputation replaces missing values with the column mean:\n\ndf_mean = df_missing.copy()\ndf_mean['price'] = df_mean['price'].fillna(df_mean['price'].mean())\ndf_mean['volume'] = df_mean['volume'].fillna(df_mean['volume'].mean())\ndf_mean\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\n\n\n0\n2024-01-01\nAAPL\n150.000000\n1.000000e+06\n0.00\n\n\n1\n2024-01-02\nAAPL\n152.500000\n1.171429e+06\n0.00\n\n\n2\n2024-01-03\nAAPL\n155.857143\n1.200000e+06\n0.00\n\n\n3\n2024-01-04\nAAPL\n153.000000\n9.500000e+05\n0.25\n\n\n4\n2024-01-05\nAAPL\n157.000000\n1.171429e+06\n0.00\n\n\n5\n2024-01-06\nAAPL\n155.857143\n1.100000e+06\n0.00\n\n\n6\n2024-01-07\nAAPL\n159.000000\n1.300000e+06\n0.00\n\n\n7\n2024-01-08\nAAPL\n158.500000\n1.171429e+06\n0.00\n\n\n8\n2024-01-09\nAAPL\n155.857143\n1.250000e+06\n0.00\n\n\n9\n2024-01-10\nAAPL\n161.000000\n1.400000e+06\n0.00\n\n\n\n\n\n\n\nMedian imputation is more robust to outliers:\n\ndf_median = df_missing.copy()\ndf_median['price'] = df_median['price'].fillna(df_median['price'].median())\ndf_median['volume'] = df_median['volume'].fillna(df_median['volume'].median())\ndf_median\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\n1000000.0\n0.00\n\n\n1\n2024-01-02\nAAPL\n152.5\n1200000.0\n0.00\n\n\n2\n2024-01-03\nAAPL\n157.0\n1200000.0\n0.00\n\n\n3\n2024-01-04\nAAPL\n153.0\n950000.0\n0.25\n\n\n4\n2024-01-05\nAAPL\n157.0\n1200000.0\n0.00\n\n\n5\n2024-01-06\nAAPL\n157.0\n1100000.0\n0.00\n\n\n6\n2024-01-07\nAAPL\n159.0\n1300000.0\n0.00\n\n\n7\n2024-01-08\nAAPL\n158.5\n1200000.0\n0.00\n\n\n8\n2024-01-09\nAAPL\n157.0\n1250000.0\n0.00\n\n\n9\n2024-01-10\nAAPL\n161.0\n1400000.0\n0.00\n\n\n\n\n\n\n\nConstant value imputation:\n\ndf_zero = df_missing.copy()\ndf_zero['dividend'] = df_zero['dividend'].fillna(0)\ndf_zero\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\n1000000.0\n0.00\n\n\n1\n2024-01-02\nAAPL\n152.5\nNaN\n0.00\n\n\n2\n2024-01-03\nAAPL\nNaN\n1200000.0\n0.00\n\n\n3\n2024-01-04\nAAPL\n153.0\n950000.0\n0.25\n\n\n4\n2024-01-05\nAAPL\n157.0\nNaN\n0.00\n\n\n5\n2024-01-06\nAAPL\nNaN\n1100000.0\n0.00\n\n\n6\n2024-01-07\nAAPL\n159.0\n1300000.0\n0.00\n\n\n7\n2024-01-08\nAAPL\n158.5\nNaN\n0.00\n\n\n8\n2024-01-09\nAAPL\nNaN\n1250000.0\n0.00\n\n\n9\n2024-01-10\nAAPL\n161.0\n1400000.0\n0.00\n\n\n\n\n\n\n\nIn Polars, we use forward_fill() and fill_null():\n\ndf_ffill_pl = df_missing_pl.select([\n    pl.col('date'),\n    pl.col('ticker'),\n    pl.col('price').forward_fill(),\n    pl.col('volume').forward_fill(),\n    pl.col('dividend')\n])\ndf_ffill_pl\n\n\nshape: (10, 5)\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\ndatetime[ns]\nstr\nf64\nf64\nf64\n\n\n\n\n2024-01-01 00:00:00\n\"AAPL\"\n150.0\n1e6\n0.0\n\n\n2024-01-02 00:00:00\n\"AAPL\"\n152.5\n1e6\n0.0\n\n\n2024-01-03 00:00:00\n\"AAPL\"\n152.5\n1.2e6\n0.0\n\n\n2024-01-04 00:00:00\n\"AAPL\"\n153.0\n950000.0\n0.25\n\n\n2024-01-05 00:00:00\n\"AAPL\"\n157.0\n950000.0\n0.0\n\n\n2024-01-06 00:00:00\n\"AAPL\"\n157.0\n1.1e6\n0.0\n\n\n2024-01-07 00:00:00\n\"AAPL\"\n159.0\n1.3e6\n0.0\n\n\n2024-01-08 00:00:00\n\"AAPL\"\n158.5\n1.3e6\n0.0\n\n\n2024-01-09 00:00:00\n\"AAPL\"\n158.5\n1.25e6\n0.0\n\n\n2024-01-10 00:00:00\n\"AAPL\"\n161.0\n1.4e6\n0.0\n\n\n\n\n\n\n\n# Polars mean imputation\nmean_price = df_missing_pl['price'].mean()\nmean_volume = df_missing_pl['volume'].mean()\n\ndf_mean_pl = df_missing_pl.with_columns([\n    pl.col('price').fill_null(mean_price),\n    pl.col('volume').fill_null(mean_volume)\n])\ndf_mean_pl\n\n\nshape: (10, 5)\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\ndatetime[ns]\nstr\nf64\nf64\nf64\n\n\n\n\n2024-01-01 00:00:00\n\"AAPL\"\n150.0\n1e6\n0.0\n\n\n2024-01-02 00:00:00\n\"AAPL\"\n152.5\n1.1714e6\n0.0\n\n\n2024-01-03 00:00:00\n\"AAPL\"\n155.857143\n1.2e6\n0.0\n\n\n2024-01-04 00:00:00\n\"AAPL\"\n153.0\n950000.0\n0.25\n\n\n2024-01-05 00:00:00\n\"AAPL\"\n157.0\n1.1714e6\n0.0\n\n\n2024-01-06 00:00:00\n\"AAPL\"\n155.857143\n1.1e6\n0.0\n\n\n2024-01-07 00:00:00\n\"AAPL\"\n159.0\n1.3e6\n0.0\n\n\n2024-01-08 00:00:00\n\"AAPL\"\n158.5\n1.1714e6\n0.0\n\n\n2024-01-09 00:00:00\n\"AAPL\"\n155.857143\n1.25e6\n0.0\n\n\n2024-01-10 00:00:00\n\"AAPL\"\n161.0\n1.4e6\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\nNoteChoosing an imputation method for financial data\n\n\n\nThe appropriate method depends on your data and context:\n\nForward fill: Good for prices and slowly-changing variables (assumes last observation is still valid)\nMean/median: Reasonable for cross-sectional data, but can distort distributions and relationships\nZero: Appropriate when zero is a meaningful value (e.g., dividends, corporate actions)\nInterpolation: Useful when you expect smooth changes over time\n\nNever use imputation blindly‚Äîthink about what each method assumes about your data.\n\n\n\n\n13.2.4.2 Linear interpolation\nFor time series data, linear interpolation can provide more realistic estimates than simple forward/backward filling:\n\ndf_interp = df_missing.copy()\ndf_interp['price'] = df_interp['price'].interpolate(method='linear')\ndf_interp['volume'] = df_interp['volume'].interpolate(method='linear')\ndf_interp\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\n\n\n0\n2024-01-01\nAAPL\n150.00\n1000000.0\n0.00\n\n\n1\n2024-01-02\nAAPL\n152.50\n1100000.0\n0.00\n\n\n2\n2024-01-03\nAAPL\n152.75\n1200000.0\n0.00\n\n\n3\n2024-01-04\nAAPL\n153.00\n950000.0\n0.25\n\n\n4\n2024-01-05\nAAPL\n157.00\n1025000.0\n0.00\n\n\n5\n2024-01-06\nAAPL\n158.00\n1100000.0\n0.00\n\n\n6\n2024-01-07\nAAPL\n159.00\n1300000.0\n0.00\n\n\n7\n2024-01-08\nAAPL\n158.50\n1275000.0\n0.00\n\n\n8\n2024-01-09\nAAPL\n159.75\n1250000.0\n0.00\n\n\n9\n2024-01-10\nAAPL\n161.00\n1400000.0\n0.00\n\n\n\n\n\n\n\nTime-based interpolation accounts for irregular spacing and requires a DatetimeIndex:\n\ndf_interp_time = df_missing.copy().set_index('date')\ndf_interp_time['price'] = df_interp_time['price'].interpolate(method='time')\ndf_interp_time = df_interp_time.reset_index()\ndf_interp_time\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\ndividend\n\n\n\n\n0\n2024-01-01\nAAPL\n150.00\n1000000.0\n0.00\n\n\n1\n2024-01-02\nAAPL\n152.50\nNaN\n0.00\n\n\n2\n2024-01-03\nAAPL\n152.75\n1200000.0\n0.00\n\n\n3\n2024-01-04\nAAPL\n153.00\n950000.0\n0.25\n\n\n4\n2024-01-05\nAAPL\n157.00\nNaN\n0.00\n\n\n5\n2024-01-06\nAAPL\n158.00\n1100000.0\n0.00\n\n\n6\n2024-01-07\nAAPL\n159.00\n1300000.0\n0.00\n\n\n7\n2024-01-08\nAAPL\n158.50\nNaN\n0.00\n\n\n8\n2024-01-09\nAAPL\n159.75\n1250000.0\n0.00\n\n\n9\n2024-01-10\nAAPL\n161.00\n1400000.0\n0.00\n\n\n\n\n\n\n\n\n\n13.2.4.3 Group-based imputation\nOften, you want to impute missing values differently for different groups. For example, you might want to impute missing returns for each industry separately:\n\n# Create data with multiple tickers\ndata_grouped = {\n    'date': pd.date_range('2024-01-01', periods=6, freq='D').tolist() * 2,\n    'ticker': ['AAPL'] * 6 + ['MSFT'] * 6,\n    'price': [150.0, np.nan, 155.0, 153.0, np.nan, 161.0,\n              250.0, 252.0, np.nan, 258.0, 260.0, np.nan],\n    'sector': ['Tech'] * 12\n}\n\ndf_grouped = pd.DataFrame(data_grouped)\ndf_grouped\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nsector\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\nTech\n\n\n1\n2024-01-02\nAAPL\nNaN\nTech\n\n\n2\n2024-01-03\nAAPL\n155.0\nTech\n\n\n3\n2024-01-04\nAAPL\n153.0\nTech\n\n\n4\n2024-01-05\nAAPL\nNaN\nTech\n\n\n5\n2024-01-06\nAAPL\n161.0\nTech\n\n\n6\n2024-01-01\nMSFT\n250.0\nTech\n\n\n7\n2024-01-02\nMSFT\n252.0\nTech\n\n\n8\n2024-01-03\nMSFT\nNaN\nTech\n\n\n9\n2024-01-04\nMSFT\n258.0\nTech\n\n\n10\n2024-01-05\nMSFT\n260.0\nTech\n\n\n11\n2024-01-06\nMSFT\nNaN\nTech\n\n\n\n\n\n\n\nForward fill within each ticker group:\n\ndf_grouped_ffill = df_grouped.copy()\ndf_grouped_ffill['price'] = df_grouped_ffill.groupby('ticker')['price'].ffill()\ndf_grouped_ffill\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nsector\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\nTech\n\n\n1\n2024-01-02\nAAPL\n150.0\nTech\n\n\n2\n2024-01-03\nAAPL\n155.0\nTech\n\n\n3\n2024-01-04\nAAPL\n153.0\nTech\n\n\n4\n2024-01-05\nAAPL\n153.0\nTech\n\n\n5\n2024-01-06\nAAPL\n161.0\nTech\n\n\n6\n2024-01-01\nMSFT\n250.0\nTech\n\n\n7\n2024-01-02\nMSFT\n252.0\nTech\n\n\n8\n2024-01-03\nMSFT\n252.0\nTech\n\n\n9\n2024-01-04\nMSFT\n258.0\nTech\n\n\n10\n2024-01-05\nMSFT\n260.0\nTech\n\n\n11\n2024-01-06\nMSFT\n260.0\nTech\n\n\n\n\n\n\n\nMean imputation within each sector:\n\ndf_grouped_mean = df_grouped.copy()\ndf_grouped_mean['price'] = df_grouped_mean.groupby('sector')['price'].transform(\n    lambda x: x.fillna(x.mean())\n)\ndf_grouped_mean\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nsector\n\n\n\n\n0\n2024-01-01\nAAPL\n150.000\nTech\n\n\n1\n2024-01-02\nAAPL\n204.875\nTech\n\n\n2\n2024-01-03\nAAPL\n155.000\nTech\n\n\n3\n2024-01-04\nAAPL\n153.000\nTech\n\n\n4\n2024-01-05\nAAPL\n204.875\nTech\n\n\n5\n2024-01-06\nAAPL\n161.000\nTech\n\n\n6\n2024-01-01\nMSFT\n250.000\nTech\n\n\n7\n2024-01-02\nMSFT\n252.000\nTech\n\n\n8\n2024-01-03\nMSFT\n204.875\nTech\n\n\n9\n2024-01-04\nMSFT\n258.000\nTech\n\n\n10\n2024-01-05\nMSFT\n260.000\nTech\n\n\n11\n2024-01-06\nMSFT\n204.875\nTech\n\n\n\n\n\n\n\nIn Polars, we use the over() method for group-based operations:\n\ndf_grouped_pl = pl.from_pandas(df_grouped)\n\n# Forward fill within groups\ndf_grouped_ffill_pl = df_grouped_pl.with_columns([\n    pl.col('price').forward_fill().over('ticker')\n])\ndf_grouped_ffill_pl\n\n\nshape: (12, 4)\n\n\n\ndate\nticker\nprice\nsector\n\n\ndatetime[ns]\nstr\nf64\nstr\n\n\n\n\n2024-01-01 00:00:00\n\"AAPL\"\n150.0\n\"Tech\"\n\n\n2024-01-02 00:00:00\n\"AAPL\"\n150.0\n\"Tech\"\n\n\n2024-01-03 00:00:00\n\"AAPL\"\n155.0\n\"Tech\"\n\n\n2024-01-04 00:00:00\n\"AAPL\"\n153.0\n\"Tech\"\n\n\n2024-01-05 00:00:00\n\"AAPL\"\n153.0\n\"Tech\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2024-01-02 00:00:00\n\"MSFT\"\n252.0\n\"Tech\"\n\n\n2024-01-03 00:00:00\n\"MSFT\"\n252.0\n\"Tech\"\n\n\n2024-01-04 00:00:00\n\"MSFT\"\n258.0\n\"Tech\"\n\n\n2024-01-05 00:00:00\n\"MSFT\"\n260.0\n\"Tech\"\n\n\n2024-01-06 00:00:00\n\"MSFT\"\n260.0\n\"Tech\"\n\n\n\n\n\n\n\n# Mean imputation within groups\ndf_grouped_mean_pl = df_grouped_pl.with_columns([\n    pl.col('price').fill_null(pl.col('price').mean()).over('sector')\n])\ndf_grouped_mean_pl\n\n\nshape: (12, 4)\n\n\n\ndate\nticker\nprice\nsector\n\n\ndatetime[ns]\nstr\nf64\nstr\n\n\n\n\n2024-01-01 00:00:00\n\"AAPL\"\n150.0\n\"Tech\"\n\n\n2024-01-02 00:00:00\n\"AAPL\"\n204.875\n\"Tech\"\n\n\n2024-01-03 00:00:00\n\"AAPL\"\n155.0\n\"Tech\"\n\n\n2024-01-04 00:00:00\n\"AAPL\"\n153.0\n\"Tech\"\n\n\n2024-01-05 00:00:00\n\"AAPL\"\n204.875\n\"Tech\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2024-01-02 00:00:00\n\"MSFT\"\n252.0\n\"Tech\"\n\n\n2024-01-03 00:00:00\n\"MSFT\"\n204.875\n\"Tech\"\n\n\n2024-01-04 00:00:00\n\"MSFT\"\n258.0\n\"Tech\"\n\n\n2024-01-05 00:00:00\n\"MSFT\"\n260.0\n\"Tech\"\n\n\n2024-01-06 00:00:00\n\"MSFT\"\n204.875\n\"Tech\"",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-analysis/cleaning/index.html#data-validation",
    "href": "data-analysis/cleaning/index.html#data-validation",
    "title": "13¬† Data Cleaning",
    "section": "13.3 Data Validation",
    "text": "13.3 Data Validation\nData validation is the process of checking whether your data meets quality standards and assumptions before analysis. In finance, invalid data can lead to catastrophic errors‚Äîimagine executing a trading strategy based on incorrect prices or publishing research with flawed data.\nValidation encompasses several activities:\n\nChecking data types and formats\nVerifying data ranges and constraints\nIdentifying outliers and anomalies\nEnsuring logical consistency\nCross-checking against external sources\n\n\n\n\n\n\n\nTipAutomated validation with pandera\n\n\n\nFor systematic data validation, consider using pandera, a Python framework for automatic validation of DataFrames. Pandera lets you define schemas that specify expected data types, value ranges, and other constraints, then automatically validates your data against these schemas. It supports both pandas and Polars.\n\n\n\n13.3.1 Validating data types and formats\nFirst, ensure that each column has the correct data type:\n\n# Create sample data with type issues\ndata_types = {\n    'date': ['2024-01-01', '2024-01-02', '2024-01-03', 'invalid'],\n    'ticker': ['AAPL', 'AAPL', 'AAPL', 'AAPL'],\n    'price': ['150.0', '152.5', '155.0', '153.0'],  # Stored as strings\n    'volume': [1000000, 1100000, 1200000, -950000]  # Negative volume\n}\n\ndf_types = pd.DataFrame(data_types)\ndf_types\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\n1000000\n\n\n1\n2024-01-02\nAAPL\n152.5\n1100000\n\n\n2\n2024-01-03\nAAPL\n155.0\n1200000\n\n\n3\ninvalid\nAAPL\n153.0\n-950000\n\n\n\n\n\n\n\n\ndf_types.dtypes\n\ndate      object\nticker    object\nprice     object\nvolume     int64\ndtype: object\n\n\nNow let‚Äôs validate and fix the types. We use errors='coerce' to convert invalid values to NaN instead of raising an error:\n\n# Convert price to numeric, coercing errors to NaN\ndf_types['price'] = pd.to_numeric(df_types['price'], errors='coerce')\n\n# Convert date to datetime\ndf_types['date'] = pd.to_datetime(df_types['date'], errors='coerce')\n\ndf_types\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\n1000000\n\n\n1\n2024-01-02\nAAPL\n152.5\n1100000\n\n\n2\n2024-01-03\nAAPL\n155.0\n1200000\n\n\n3\nNaT\nAAPL\n153.0\n-950000\n\n\n\n\n\n\n\n\ndf_types.dtypes\n\ndate      datetime64[ns]\nticker            object\nprice            float64\nvolume             int64\ndtype: object\n\n\nWe can identify rows with conversion errors by checking for NaN values:\n\ninvalid_dates = df_types['date'].isna()\nprint(f\"Rows with invalid dates: {invalid_dates.sum()}\")\n\nRows with invalid dates: 1\n\n\n\ndf_types[invalid_dates]\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\n\n\n\n\n3\nNaT\nAAPL\n153.0\n-950000\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningCommon type issues in financial data\n\n\n\n\nDates: Mixed formats (MM/DD/YYYY vs DD/MM/YYYY), Excel date serials, timezone issues\nNumbers: Thousands separators, currency symbols, percentage signs stored as text\nMissing values: Coded as -999, ‚ÄòNA‚Äô, empty strings, or other sentinel values\nCategorical variables: Inconsistent capitalization or spelling\n\nAlways check types early in your pipeline and convert them explicitly.\n\n\n\n\n13.3.2 Range and constraint validation\nFinancial data often has natural constraints. Prices must be positive, probabilities must be between 0 and 1, and returns are typically bounded (though extreme events can violate typical ranges).\n\n# Create data with constraint violations\ndata_constraints = {\n    'date': pd.date_range('2024-01-01', periods=5, freq='D'),\n    'ticker': ['AAPL'] * 5,\n    'price': [150.0, -152.5, 155.0, 1000000.0, 157.0],  # Negative and extreme prices\n    'volume': [1000000, 1100000, 1200000, -950000, 1300000],  # Negative volume\n    'return': [0.01, 0.02, -0.01, 5.0, -0.03]  # Extreme return\n}\n\ndf_constraints = pd.DataFrame(data_constraints)\n\n# Define validation rules\ndef validate_price(price):\n    \"\"\"Check if price is valid.\"\"\"\n    return (price &gt; 0) & (price &lt; 100000)\n\ndef validate_volume(volume):\n    \"\"\"Check if volume is valid.\"\"\"\n    return volume &gt;= 0\n\ndef validate_return(ret):\n    \"\"\"Check if return is reasonable.\"\"\"\n    return (ret &gt; -1) & (ret &lt; 1)\n\n# Apply validation\ndf_constraints['price_valid'] = validate_price(df_constraints['price'])\ndf_constraints['volume_valid'] = validate_volume(df_constraints['volume'])\ndf_constraints['return_valid'] = validate_return(df_constraints['return'])\n\ndf_constraints\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\nreturn\nprice_valid\nvolume_valid\nreturn_valid\n\n\n\n\n0\n2024-01-01\nAAPL\n150.0\n1000000\n0.01\nTrue\nTrue\nTrue\n\n\n1\n2024-01-02\nAAPL\n-152.5\n1100000\n0.02\nFalse\nTrue\nTrue\n\n\n2\n2024-01-03\nAAPL\n155.0\n1200000\n-0.01\nTrue\nTrue\nTrue\n\n\n3\n2024-01-04\nAAPL\n1000000.0\n-950000\n5.00\nFalse\nFalse\nFalse\n\n\n4\n2024-01-05\nAAPL\n157.0\n1300000\n-0.03\nTrue\nTrue\nTrue\n\n\n\n\n\n\n\nWe can identify invalid rows by combining the validation columns:\n\ninvalid_mask = ~(df_constraints['price_valid'] &\n                 df_constraints['volume_valid'] &\n                 df_constraints['return_valid'])\nprint(f\"Invalid rows: {invalid_mask.sum()}\")\n\nInvalid rows: 2\n\n\n\ndf_constraints[invalid_mask]\n\n\n\n\n\n\n\n\ndate\nticker\nprice\nvolume\nreturn\nprice_valid\nvolume_valid\nreturn_valid\n\n\n\n\n1\n2024-01-02\nAAPL\n-152.5\n1100000\n0.02\nFalse\nTrue\nTrue\n\n\n3\n2024-01-04\nAAPL\n1000000.0\n-950000\n5.00\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\nIn Polars, we can use expressions for validation:\n\ndf_constraints_pl = pl.DataFrame(data_constraints)\n\n# Add validation columns\ndf_validated_pl = df_constraints_pl.with_columns([\n    ((pl.col('price') &gt; 0) & (pl.col('price') &lt; 100000)).alias('price_valid'),\n    (pl.col('volume') &gt;= 0).alias('volume_valid'),\n    ((pl.col('return') &gt; -1) & (pl.col('return') &lt; 1)).alias('return_valid')\n])\n\ndf_validated_pl\n\n\nshape: (5, 8)\n\n\n\ndate\nticker\nprice\nvolume\nreturn\nprice_valid\nvolume_valid\nreturn_valid\n\n\ndatetime[ns]\nstr\nf64\ni64\nf64\nbool\nbool\nbool\n\n\n\n\n2024-01-01 00:00:00\n\"AAPL\"\n150.0\n1000000\n0.01\ntrue\ntrue\ntrue\n\n\n2024-01-02 00:00:00\n\"AAPL\"\n-152.5\n1100000\n0.02\nfalse\ntrue\ntrue\n\n\n2024-01-03 00:00:00\n\"AAPL\"\n155.0\n1200000\n-0.01\ntrue\ntrue\ntrue\n\n\n2024-01-04 00:00:00\n\"AAPL\"\n1e6\n-950000\n5.0\nfalse\nfalse\nfalse\n\n\n2024-01-05 00:00:00\n\"AAPL\"\n157.0\n1300000\n-0.03\ntrue\ntrue\ntrue\n\n\n\n\n\n\n\n# Filter to invalid rows\ninvalid_pl = df_validated_pl.filter(\n    ~pl.col('price_valid') | ~pl.col('volume_valid') | ~pl.col('return_valid')\n)\ninvalid_pl\n\n\nshape: (2, 8)\n\n\n\ndate\nticker\nprice\nvolume\nreturn\nprice_valid\nvolume_valid\nreturn_valid\n\n\ndatetime[ns]\nstr\nf64\ni64\nf64\nbool\nbool\nbool\n\n\n\n\n2024-01-02 00:00:00\n\"AAPL\"\n-152.5\n1100000\n0.02\nfalse\ntrue\ntrue\n\n\n2024-01-04 00:00:00\n\"AAPL\"\n1e6\n-950000\n5.0\nfalse\nfalse\nfalse\n\n\n\n\n\n\n\n\n13.3.3 Detecting outliers\nOutliers are observations that deviate significantly from other observations. In finance, outliers could indicate:\n\nGenuine extreme events (market crashes, flash crashes)\nData errors (wrong decimal place, data feed glitches)\nSpecial situations (stock splits, spinoffs)\n\nOutlier detection requires domain knowledge. A 50% daily return is an error for a typical stock but normal for a penny stock or during a takeover announcement.\n\n13.3.3.1 Statistical methods for outlier detection\nCommon approaches include:\n\nZ-score method: Flag observations more than N standard deviations from the mean\nIQR method: Flag observations outside 1.5 times the interquartile range\nModified Z-score: Use median absolute deviation instead of standard deviation (more robust)\nDomain-specific rules: Use knowledge about reasonable ranges\n\nChoosing the right method for a particular application is beyond the scope of this book and requires careful consideration of the data characteristics and analysis goals.\n\n\n13.3.3.2 Handling outliers\nOnce you‚Äôve identified outliers, you have several options:\n\nKeep them: If they‚Äôre genuine extreme events\nRemove them: If they‚Äôre clearly errors\nCap/Winsorize: Replace extreme values with a threshold\nTransform the data: Use log transformation or other methods to reduce the impact\nInvestigate: Manually check each outlier\n\n\n\n\n\n\n\nWarningBe cautious with outlier removal\n\n\n\nIn finance, extreme events are often the most important observations. The 2008 financial crisis, COVID-19 market crash, and other tail events are ‚Äúoutliers‚Äù but carry critical information. Before removing outliers:\n\nInvestigate each one individually if possible\nCheck if they coincide with known events (earnings announcements, news, market crises)\nConsider the impact on your conclusions if you keep vs.¬†remove them\nDocument your decision and reasoning\n\nWhen in doubt, perform your analysis both with and without outliers to assess sensitivity.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-analysis/structuring/index.html",
    "href": "data-analysis/structuring/index.html",
    "title": "14¬† Data Structuring and Aggregation",
    "section": "",
    "text": "14.1 Keys, Indices, and Identifiers\nWorking with financial data requires more than knowing how to load datasets and run calculations. The structure of your data‚Äîhow observations are identified, indexed, and organized‚Äîfundamentally shapes what analyses are possible and what mistakes are easy to make. A portfolio returns dataset indexed by date allows straightforward time-series operations but makes cross-sectional comparisons awkward. The same data with a multi-level index on date and ticker enables both temporal and cross-sectional analysis but requires more careful handling of group operations.\nThis chapter covers three essential skills for working with structured financial data. First, we examine how keys, indices, and identifiers work‚Äîthe foundation for correctly organizing observations. Second, we explore grouping and aggregation operations that let you compute statistics within subsets of data (returns by sector, volatility by year, etc.). Third, we discuss common pitfalls specific to financial data, particularly look-ahead bias and survivorship bias, which can invalidate empirical results if not handled carefully.\nFinancial datasets rarely consist of independent observations. Stock returns are measured repeatedly over time for many different securities. Portfolio weights change across rebalancing dates. Trades occur at specific timestamps for specific instruments. To work with such data correctly, you need to understand how observations are identified and how that identification is represented in your data structures.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Data Structuring and Aggregation</span>"
    ]
  },
  {
    "objectID": "data-analysis/structuring/index.html#keys-indices-and-identifiers",
    "href": "data-analysis/structuring/index.html#keys-indices-and-identifiers",
    "title": "14¬† Data Structuring and Aggregation",
    "section": "",
    "text": "14.1.1 What Makes an Observation Unique?\nConsider a dataset of daily stock returns. What information do you need to uniquely identify a single return observation? At minimum, you need to know which stock (ticker symbol or identifier) and which date. Neither alone suffices: Apple‚Äôs ticker AAPL appears thousands of times in a long dataset (once per day), and any given date like 2024-01-15 appears once for every stock in your universe. The combination of ticker and date uniquely identifies each return.\nThis combination is called a composite key or multi-level identifier. Other examples in finance include:\n\nTrade data: exchange + timestamp + order ID\nOptions prices: underlying + expiration + strike + call/put\nCross-country macro data: country + date + variable\nPortfolio holdings: portfolio ID + date + security ID\n\nThe key affects everything downstream: what joins are valid, what grouping operations make sense, what reshaping is possible, and critically, what the index of a time series means.\n\n\n14.1.2 Representing Keys in DataFrames\nThere are two main approaches to representing composite keys in DataFrames: as regular columns in the data or as a special index structure. Both pandas and Polars support the regular column approach, while only pandas supports the index approach.\nApproach 1: Keys as Regular Columns\nThe simplest representation keeps all identifiers as ordinary columns:\nimport pandas as pd\nimport polars as pl\n\n# Pandas version\ndf_pandas = pd.DataFrame({\n    'ticker': ['AAPL', 'AAPL', 'MSFT', 'MSFT'],\n    'date': pd.to_datetime(['2024-01-15', '2024-01-16', '2024-01-15', '2024-01-16']),\n    'return': [0.012, -0.005, 0.008, 0.003]\n})\n\n# Polars version\ndf_polars = pl.DataFrame({\n    'ticker': ['AAPL', 'AAPL', 'MSFT', 'MSFT'],\n    'date': pl.date_range(pl.date(2024, 1, 15), pl.date(2024, 1, 16), \"1d\").repeat_by(2),\n    'return': [0.012, -0.005, 0.008, 0.003]\n})\nThis representation is explicit and flexible. Every piece of identifying information is visible in the data. Operations like filtering, grouping, and joining reference columns by name, making code clear and intentions obvious.\nApproach 2: Pandas MultiIndex\nPandas offers an alternative where identifying columns become a special index structure:\ndf_indexed = df_pandas.set_index(['ticker', 'date'])\nThe result looks like:\n                     return\nticker date\nAAPL   2024-01-15    0.012\n       2024-01-16   -0.005\nMSFT   2024-01-15    0.008\n       2024-01-16    0.003\nNow ticker and date are no longer regular columns but levels of a hierarchical index. This structure enables convenient operations like:\n# Select all observations for AAPL\ndf_indexed.loc['AAPL']\n\n# Select specific ticker-date combination\ndf_indexed.loc[('AAPL', '2024-01-15')]\n\n# Unstack to wide format\ndf_indexed.unstack('ticker')\nThe MultiIndex approach can make certain operations more concise, particularly when working with panel data where you frequently filter by identifier values or reshape between long and wide formats. However, it also introduces complexity. Index levels behave differently from columns: they don‚Äôt appear in df.columns, they require different syntax to modify, and they can complicate operations when you need to treat identifiers as data (e.g., creating a new column based on ticker characteristics).\n\n\n14.1.3 Practical Guidance on Index Choice\nOur general recommendation is to use regular columns for key variables, unless you have an explicit need for a pandas index. Regular columns make code more explicit and portable across libraries. However, a pandas index can simplify your work in specific situations: certain time-series operations (like resampling), creating plots or tables where the index provides axis labels, or when using third-party libraries that expect an index.\n\n\n14.1.4 Temporal Identifiers and DatetimeIndex\nTime is special in financial data. Unlike categorical identifiers like ticker symbols, dates and timestamps have inherent ordering and spacing. A pandas DatetimeIndex (single-level index with datetime values) provides specialized functionality for time-series work:\n# Daily returns with date index\nreturns = pd.Series(\n    [0.012, -0.005, 0.008],\n    index=pd.to_datetime(['2024-01-15', '2024-01-16', '2024-01-17']),\n    name='AAPL'\n)\n\n# Convenient date-based selection\nreturns['2024-01-16']  # Single date\nreturns['2024-01-15':'2024-01-16']  # Date range\n\n# Resampling and frequency conversion\n1monthly_returns = (1 + returns).resample('ME').prod() - 1\n\n# Time-aware operations\n2returns.shift(1)\n3returns.rolling(window=5).mean()\n\n1\n\nCompound daily returns to monthly by converting to gross returns (1 + r), multiplying within each month, then converting back to net returns.\n\n2\n\nLag the series by one period‚Äîuseful for avoiding look-ahead bias.\n\n3\n\nCompute a 5-day rolling mean.\n\n\nThis is one area where pandas indexing genuinely simplifies common operations. The alternative using regular columns requires more verbose syntax:\n# Polars equivalent for date filtering\ndf_polars.filter(\n    (pl.col('date') &gt;= pl.date(2024, 1, 15)) &\n    (pl.col('date') &lt;= pl.date(2024, 1, 16))\n)\n\n# Polars rolling mean with explicit ordering\ndf_polars.sort('date').select([\n    'date',\n    pl.col('return').rolling_mean(window_size=5).alias('ma_5')\n])\nThe pandas DatetimeIndex approach is particularly valuable when working with a single time series (one security or portfolio) where time is the primary organizing principle. For panel data with many securities, a regular date column often proves more flexible.\nPolars encourages explicit column-based operations and provides excellent performance for time-based filtering and aggregation without special index structures. Its approach scales better to large datasets and makes parallelization transparent.\n\n\n14.1.5 Identifier Best Practices\nSeveral practical considerations matter when choosing and working with identifiers:\nUniqueness and Validation\nAlways verify that your chosen keys actually uniquely identify observations:\n# Pandas: check for duplicates in composite key\nduplicates = df_pandas.duplicated(subset=['ticker', 'date'], keep=False)\nif duplicates.any():\n    print(f\"Found {duplicates.sum()} duplicate observations\")\n    print(df_pandas[duplicates])\n\n# Polars: check for duplicates\nduplicate_count = (\n    df_polars\n    .group_by(['ticker', 'date'])\n    .agg(pl.len().alias('count'))\n    .filter(pl.col('count') &gt; 1)\n)\nDuplicate keys often indicate data quality problems: double-counting trades, duplicate downloads, or errors in joins. Finding them early prevents subtle errors in aggregation and analysis.\nIdentifier Stability\nTicker symbols change. Companies get acquired or reorganize. CUSIP identifiers remain stable but aren‚Äôt always available. When working with long time-series, consider using permanent identifiers like PERMNO (from CRSP) or assigning your own stable internal IDs that you map to tickers.\n# Maintaining a ticker-to-permno mapping\nticker_map = pd.DataFrame({\n    'permno': [14593, 10107],\n    'ticker': ['AAPL', 'MSFT'],\n    'start_date': pd.to_datetime(['1980-12-12', '1986-03-13']),\n    'end_date': pd.to_datetime(['2024-12-31', '2024-12-31'])\n})\n\n# Join with date validation to handle ticker changes\ndef get_permno(df, ticker_map):\n    return df.merge(\n        ticker_map,\n        on='ticker',\n        how='left'\n    ).query('date &gt;= start_date and date &lt;= end_date')\nMissing Identifiers\nFinancial data often has gaps. A stock might not trade on certain days. An option chain might lack certain strikes. Design your data structures to make missing data explicit rather than creating ambiguous rows:\n# Bad: using dummy values\ndf_bad = pd.DataFrame({\n    'ticker': ['AAPL', 'NONE', 'MSFT'],  # 'NONE' for missing\n    'return': [0.01, 0.0, 0.02]\n})\n\n# Good: using None/null\ndf_good = pd.DataFrame({\n    'ticker': ['AAPL', None, 'MSFT'],\n    'return': [0.01, None, 0.02]\n})\n\n# Polars explicitly handles null\ndf_polars = pl.DataFrame({\n    'ticker': ['AAPL', None, 'MSFT'],\n    'return': [0.01, None, 0.02]\n})\nThis distinction matters because real zeros and missing values have different meanings in finance. A zero return is information; a missing return means we don‚Äôt know what happened.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Data Structuring and Aggregation</span>"
    ]
  },
  {
    "objectID": "data-analysis/structuring/index.html#grouping-and-aggregation",
    "href": "data-analysis/structuring/index.html#grouping-and-aggregation",
    "title": "14¬† Data Structuring and Aggregation",
    "section": "14.2 Grouping and Aggregation",
    "text": "14.2 Grouping and Aggregation\nMuch of financial data analysis involves computing statistics within subsets: average returns by sector, volatility by year, portfolio weights by strategy. The split-apply-combine pattern‚Äîdividing data into groups, computing something for each group, and combining results‚Äîis fundamental to empirical work.\n\n14.2.1 The GroupBy Operation\nThe core operation splits your dataset into groups based on one or more columns, applies a function to each group, and combines the results. Both pandas and Polars implement this pattern, though with different syntax and performance characteristics.\nBasic GroupBy Example\nConsider computing average returns by sector:\n# Sample data: daily returns for multiple stocks\ndata = pd.DataFrame({\n    'ticker': ['AAPL', 'AAPL', 'MSFT', 'MSFT', 'JPM', 'JPM'],\n    'date': pd.to_datetime(['2024-01-15', '2024-01-16'] * 3),\n    'sector': ['Technology', 'Technology', 'Technology', 'Technology', 'Financials', 'Financials'],\n    'return': [0.012, -0.005, 0.008, 0.003, -0.002, 0.015]\n})\n\n# Pandas groupby\nsector_avg_pandas = data.groupby('sector')['return'].mean()\n\n# Polars groupby\ndata_polars = pl.DataFrame(data)\nsector_avg_polars = (\n    data_polars\n    .group_by('sector')\n    .agg(pl.col('return').mean())\n)\nBoth produce the same result: average returns for Technology and Financials. The key difference is syntax style. Pandas uses method chaining with implicit column selection (['return']), while Polars uses explicit expressions (pl.col('return').mean()).\n\n\n14.2.2 Multiple Aggregations\nReal analysis rarely computes just one statistic. You typically want mean, standard deviation, count, and other metrics simultaneously:\n# Pandas: multiple aggregations\nsector_stats_pandas = data.groupby('sector')['return'].agg([\n    ('mean', 'mean'),\n    ('std', 'std'),\n    ('count', 'count')\n])\n\n# Alternative pandas syntax using dictionary\nsector_stats_pandas_alt = data.groupby('sector').agg({\n    'return': ['mean', 'std', 'count']\n})\n\n# Polars: explicit expression for each statistic\nsector_stats_polars = (\n    data_polars\n    .group_by('sector')\n    .agg([\n        pl.col('return').mean().alias('mean'),\n        pl.col('return').std().alias('std'),\n        pl.col('return').count().alias('count')\n    ])\n)\nThe Polars approach requires more typing but makes each calculation explicit. This verbosity pays off in complex aggregations where you‚Äôre computing different statistics on different columns or using custom expressions.\n\n\n14.2.3 Grouping by Multiple Keys\nFinancial data often requires grouping by multiple dimensions. For example, computing monthly returns by stock requires grouping by both ticker and month:\n# Extended data with more dates\nnp.random.seed(42)\ndates = pd.date_range('2024-01-01', '2024-03-31', freq='D')\ntickers = ['AAPL', 'MSFT', 'JPM']\n\ndata_panel = pd.DataFrame({\n    'ticker': np.repeat(tickers, len(dates)),\n    'date': np.tile(dates, len(tickers)),\n    'return': np.random.normal(0.001, 0.02, len(dates) * len(tickers))\n})\n\n# Add month identifier\ndata_panel['month'] = data_panel['date'].dt.to_period('M')\n\n# Pandas: group by ticker and month\nmonthly_returns_pandas = (\n    data_panel\n    .groupby(['ticker', 'month'])['return']\n    .apply(lambda x: (1 + x).prod() - 1)  # Compound returns\n)\n\n# Polars: explicit grouping\ndata_panel_polars = pl.DataFrame(data_panel)\nmonthly_returns_polars = (\n    data_panel_polars\n    .group_by(['ticker', 'month'])\n    .agg(\n        ((1 + pl.col('return')).product() - 1).alias('monthly_return')\n    )\n)\nNotice the calculation itself: daily returns compound multiplicatively, not additively. This is a common pattern in financial aggregation where the mathematical operation matters. Simple averaging would be incorrect for returns.\n\n\n14.2.4 Custom Aggregation Functions\nSometimes built-in aggregations aren‚Äôt sufficient. You might need to compute Sharpe ratios, apply winsorization, or implement custom risk metrics:\ndef sharpe_ratio(returns, risk_free_rate=0.0):\n    \"\"\"Compute annualized Sharpe ratio from daily returns.\"\"\"\n1    excess_returns = returns - risk_free_rate / 252\n2    return np.sqrt(252) * excess_returns.mean() / excess_returns.std()\n\n# Pandas: apply custom function\nsector_sharpe_pandas = (\n    data_panel\n    .groupby(['ticker'])['return']\n    .apply(sharpe_ratio)\n)\n\n1\n\nConvert annual risk-free rate to daily by dividing by 252 trading days.\n\n2\n\nAnnualize the Sharpe ratio by multiplying by ‚àö252 (volatility scales with ‚àöT).\n\n\n# Polars: requires more explicit approach\n# Option 1: Convert to pandas for complex custom functions\nsector_sharpe_polars_via_pandas = (\n    data_panel_polars\n    .to_pandas()\n    .groupby('ticker')['return']\n    .apply(sharpe_ratio)\n)\n\n# Option 2: Use Polars expressions (more efficient)\ndef sharpe_ratio_polars(returns_col, risk_free_rate=0.0):\n    \"\"\"Sharpe ratio as Polars expression.\"\"\"\n    excess = returns_col - risk_free_rate / 252\n    return (pl.lit(252).sqrt() * excess.mean() / excess.std()).alias('sharpe')\n\nsector_sharpe_polars = (\n    data_panel_polars\n    .group_by('ticker')\n    .agg(sharpe_ratio_polars(pl.col('return')))\n)\nPandas‚Äôs apply method accepts arbitrary Python functions, offering maximum flexibility at the cost of performance (it applies the function to each group sequentially in Python). Polars requires expressing custom operations using its expression language, which enables query optimization and parallel execution but requires more thought about how to structure the calculation.\nFor complex custom functions, pandas is often more convenient. For operations that can be expressed using Polars‚Äôs built-in functions (which cover most standard statistical and mathematical operations), Polars provides better performance.\n\n\n14.2.5 Time-Based Aggregation and Resampling\nFinancial data frequently needs aggregation at different time frequencies: daily data to monthly, tick data to minute bars, etc. Pandas provides convenient resampling functionality:\n# Single time series of daily returns\naapl_returns = (\n    data_panel[data_panel['ticker'] == 'AAPL']\n    .set_index('date')['return']\n)\n\n# Resample to monthly, compounding returns\nmonthly_aapl = (1 + aapl_returns).resample('ME').prod() - 1\n\n# Resample to weekly, computing volatility\nweekly_vol = aapl_returns.resample('W').std() * np.sqrt(5)\n\n# Multiple statistics at monthly frequency\nmonthly_stats = aapl_returns.resample('ME').agg({\n    'return': [\n        ('compound_return', lambda x: (1 + x).prod() - 1),\n        ('volatility', lambda x: x.std() * np.sqrt(21)),\n        ('days', 'count')\n    ]\n})\nFor panel data (multiple securities), combine groupby with resampling:\n# Panel data: group by ticker, then resample\nmonthly_panel = (\n    data_panel\n    .set_index('date')\n    .groupby('ticker')\n    .resample('ME')['return']\n    .apply(lambda x: (1 + x).prod() - 1)\n    .reset_index()\n)\nPolars handles time-based aggregation through explicit grouping by time periods:\n# Polars: group by ticker and month\nmonthly_panel_polars = (\n    data_panel_polars\n    .group_by(['ticker', pl.col('date').dt.month().alias('month')])\n    .agg(\n        ((1 + pl.col('return')).product() - 1).alias('monthly_return')\n    )\n)\n\n# More sophisticated: group by ticker and dynamic time windows\nmonthly_panel_polars_alt = (\n    data_panel_polars\n    .sort(['ticker', 'date'])\n    .group_by_dynamic(\n        'date',\n        every='1mo',\n        by='ticker'\n    )\n    .agg([\n        ((1 + pl.col('return')).product() - 1).alias('monthly_return'),\n        pl.col('return').std().alias('volatility'),\n        pl.col('return').count().alias('days')\n    ])\n)\nThe group_by_dynamic method in Polars provides powerful time-based grouping with clear syntax for the window specification.\n\n\n14.2.6 Performance Considerations\nFor small to medium datasets (thousands to low millions of rows), both pandas and Polars perform well. Differences become significant with larger datasets or complex operations:\n\nPandas uses single-threaded execution for most operations. Custom apply functions run sequentially in Python, which can be slow for large groups or expensive functions.\nPolars uses parallel execution by default and query optimization. Operations expressed using Polars expressions (rather than Python functions) run much faster, often 10-100x faster than pandas on large datasets.\n\nFor production pipelines or research involving millions of rows, Polars‚Äôs performance advantages justify the learning curve. For interactive analysis and moderate-sized datasets, pandas‚Äôs ecosystem maturity and flexibility often win.\n\n\n14.2.7 Grouped Transformations\nAggregation reduces groups to single values, but sometimes you want to transform data within groups while preserving the original shape. Common examples include:\n\nDemeaning returns within sectors (excess returns over sector average)\nComputing lagged observations in panel data\nNormalizing values within groups\nComputing ranks within categories\nForward-filling missing data within securities\n\nPandas Transform\n# Demean returns within each sector\ndata['excess_return'] = (\n    data.groupby('sector')['return']\n    .transform(lambda x: x - x.mean())\n)\n\n# Standardize returns within each ticker\ndata_panel['standardized_return'] = (\n    data_panel.groupby('ticker')['return']\n    .transform(lambda x: (x - x.mean()) / x.std())\n)\n\n# Rank returns within each date (cross-sectional ranks)\ndata_panel['return_rank'] = (\n    data_panel.groupby('date')['return']\n    .rank(pct=True)\n)\nTransform operations return a Series with the same index as the original data, allowing direct assignment to new columns.\nPolars Window Functions\nPolars uses window functions (similar to SQL) for grouped transformations:\n# Demean returns within each sector\ndata_polars_with_excess = data_polars.with_columns(\n1    (pl.col('return') - pl.col('return').mean().over('sector')).alias('excess_return')\n)\n\n# Standardize returns within each ticker\ndata_panel_polars_standardized = data_panel_polars.with_columns(\n    (\n        (pl.col('return') - pl.col('return').mean().over('ticker')) /\n        pl.col('return').std().over('ticker')\n2    ).alias('standardized_return')\n)\n\n# Rank returns within each date\ndata_panel_polars_ranked = data_panel_polars.with_columns(\n3    pl.col('return').rank().over('date').alias('return_rank')\n)\n\n1\n\nThe .over('sector') computes the mean within each sector group, similar to SQL window functions.\n\n2\n\nZ-score normalization: subtract the group mean and divide by the group standard deviation.\n\n3\n\nCross-sectional ranking: rank returns among all stocks on each date.\n\n\nThe .over() syntax specifies the grouping for the window function. This approach is more explicit than pandas transform and integrates naturally with Polars‚Äôs expression system.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Data Structuring and Aggregation</span>"
    ]
  },
  {
    "objectID": "data-analysis/structuring/index.html#common-pitfalls-in-financial-data",
    "href": "data-analysis/structuring/index.html#common-pitfalls-in-financial-data",
    "title": "14¬† Data Structuring and Aggregation",
    "section": "14.3 Common Pitfalls in Financial Data",
    "text": "14.3 Common Pitfalls in Financial Data\nFinancial data has specific characteristics that create traps for the unwary. Two problems‚Äîlook-ahead bias and survivorship bias‚Äîare particularly insidious because they can produce plausible-looking results that are completely invalid. Understanding these issues and designing your data structures to avoid them is essential for credible empirical work.\n\n14.3.1 Look-Ahead Bias\nLook-ahead bias occurs when your analysis uses information that would not have been available at the time a decision would have been made. This is surprisingly easy to do accidentally, and it typically makes strategies appear more profitable than they actually would have been.\nExample 1: Using Future Data for Current Decisions\nConsider computing a moving average of returns to generate trading signals:\n# WRONG: Look-ahead bias\nreturns = pd.Series(\n    [0.01, -0.02, 0.03, -0.01, 0.02],\n    index=pd.date_range('2024-01-01', periods=5, freq='D')\n)\n\n# This looks innocent but is wrong!\nreturns['ma_5'] = returns.rolling(window=5, center=True).mean()\nThe center=True parameter makes each moving average value include both past and future returns. A trading signal based on the 2024-01-03 moving average would use returns through 2024-01-05, which wouldn‚Äôt be known on 2024-01-03. The correct approach:\n# CORRECT: Only use past data\nreturns_with_ma = pd.DataFrame({\n    'return': returns,\n1    'ma_5': returns.rolling(window=5).mean()\n})\n\n# Generate signal based on past information\nreturns_with_ma['signal'] = (\n2    returns_with_ma['return'].shift(1) &gt; returns_with_ma['ma_5'].shift(1)\n).astype(int)\n\n1\n\nDefault rolling window uses only current and past observations (no future data).\n\n2\n\nThe .shift(1) ensures today‚Äôs signal uses only yesterday‚Äôs values‚Äîinformation available at market open.\n\n\nExample 2: Point-in-Time Data Issues\nFinancial datasets are often revised. A company‚Äôs book value reported in 2024 financial statements might differ from what was reported in earlier vintages due to restatements. Academic databases like Compustat provide ‚Äúpoint-in-time‚Äù datasets that preserve what was known at each historical date.\n# Problematic: Using latest restated values\nfundamentals = pd.DataFrame({\n    'ticker': ['AAPL', 'AAPL', 'AAPL'],\n    'report_date': pd.to_datetime(['2023-09-30', '2023-12-31', '2024-03-31']),\n    'book_value': [150e9, 155e9, 160e9]  # Current restated values\n})\n\n# Computing book-to-market ratio using restated book values\n# would create look-ahead bias if values were adjusted retroactively\n\n# Better: Use point-in-time database or verify no retroactive changes\n# Query data as it existed at each decision point\nThe safest approach is to use datasets explicitly designed to avoid this issue (CRSP/Compustat point-in-time, FactSet‚Äôs point-in-time fundamentals) or carefully verify that your data source doesn‚Äôt include retroactive revisions.\nExample 3: Survivorship Bias in Filtering\nFiltering data before creating historical samples can inadvertently create look-ahead bias:\n# WRONG: Filtering on current characteristics\n# Get list of large-cap stocks as of 2024\nlarge_caps_2024 = get_stocks_by_market_cap(min_cap=10e9, date='2024-01-01')\n\n# Use this list to analyze returns from 2020-2024\nhistorical_returns = get_returns(\n    tickers=large_caps_2024,\n    start_date='2020-01-01',\n    end_date='2024-01-01'\n)\n# This is wrong! Uses 2024 information to select 2020 sample\n\n# CORRECT: Filter at each point in time\ndef get_large_cap_returns(start_date, end_date, min_cap):\n    \"\"\"Get returns for stocks that were large-cap at each date.\"\"\"\n    all_dates = pd.date_range(start_date, end_date, freq='D')\n    results = []\n\n    for date in all_dates:\n        # Get stocks that qualified on this date\n        eligible = get_stocks_by_market_cap(min_cap=min_cap, date=date)\n        # Get returns for these stocks on this date\n        returns = get_returns(tickers=eligible, start_date=date, end_date=date)\n        results.append(returns)\n\n    return pd.concat(results)\nThe correct approach checks eligibility at each date, allowing the composition to change over time as stocks grow or shrink.\n\n\n\n\n\n\nWarningDetecting Look-Ahead Bias\n\n\n\nAsk these questions about any empirical analysis:\n\nCould I have computed this value in real-time? If your calculation requires future data, it‚Äôs look-ahead bias.\nDoes the analysis use the latest version of the data? If you‚Äôre using restated fundamentals or revised economic data, verify those revisions were available when decisions would have been made.\nAre sample filters time-varying? Any filter based on characteristics (market cap, industry, etc.) should be applied using values from that point in time, not current values.\nAre there suspicious shifts? If lagged variables are used to predict returns, ensure lags are implemented correctly with .shift() operations.\n\n\n\n\n\n14.3.2 Survivorship Bias\nSurvivorship bias occurs when your dataset includes only entities that survived until the sample endpoint, excluding those that disappeared (delisted stocks, defunct funds, closed portfolios). This creates artificially optimistic results because you‚Äôre analyzing a selected sample of winners.\nExample 1: Stock Return Analysis\nConsider analyzing stock returns from 2020-2024 using a current stock list:\n# WRONG: Current stock universe\ncurrent_stocks = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA']\nreturns = get_returns(tickers=current_stocks, start='2020-01-01', end='2024-01-01')\n\n# Average return calculation\nmean_return = returns.mean()\n# This is biased upward! Only includes stocks that survived and remained liquid\nThese are all large, successful companies. Hundreds of stocks that existed in 2020 but delisted due to bankruptcy, acquisition, or poor performance are missing. The average return will be substantially higher than the true market average.\nThe solution requires a database that includes delisted securities:\n# CORRECT: Include delisted stocks\n# Using a complete database (e.g., CRSP with delisting codes)\nall_stocks = get_stocks(start='2020-01-01', include_delisted=True)\ncomplete_returns = get_returns(\n    tickers=all_stocks,\n    start='2020-01-01',\n    end='2024-01-01'\n)\n\n# Handle delisting returns\n# CRSP provides delisting returns accounting for final liquidation values\ncomplete_returns['total_return'] = (\n    complete_returns['return'].fillna(0) +\n    complete_returns['delisting_return'].fillna(0)\n)\n\nmean_return_unbiased = complete_returns['total_return'].mean()\nAcademic databases like CRSP explicitly address survivorship bias by maintaining historical records of all listed stocks, including delisting codes and final returns.\nExample 2: Fund Performance Analysis\nMutual fund and hedge fund databases are notorious for survivorship bias. Failed or poorly performing funds often stop reporting, creating databases that over-represent successful funds:\n# Analyzing fund performance with survivorship bias\nfunds_database = get_fund_returns(database='current_funds')  # Only active funds\n\n# Average alpha appears positive\naverage_alpha = compute_alpha(funds_database)\n# Misleading! Unsuccessful funds that closed are missing\n\n# Better approach: Use survivorship-bias-free database\nfunds_complete = get_fund_returns(database='complete_with_defunct')\n\n# Explicitly track fund status\nactive_funds = funds_complete[funds_complete['status'] == 'active']\ndefunct_funds = funds_complete[funds_complete['status'] == 'defunct']\n\n# Compare performance\nprint(f\"Active funds average return: {active_funds['return'].mean():.2%}\")\nprint(f\"Defunct funds average return: {defunct_funds['return'].mean():.2%}\")\nprint(f\"All funds average return: {funds_complete['return'].mean():.2%}\")\nStudies have found that survivorship bias can inflate reported mutual fund returns, a massive distortion in performance measurement.\n\n\n\n\n\n\nWarningDetecting Survivorship Bias\n\n\n\nGuard against survivorship bias by:\n\nUsing complete databases: CRSP, Compustat, or commercial providers that maintain historical records of delisted securities.\nChecking delisting treatment: Verify how your data source handles stocks that stopped trading. Are delisting returns included?\nComparing universe sizes: If analyzing 2020-2024 data in 2024, check how many securities existed in 2020 versus 2024. A large decline might indicate missing delistings.\nExamining data provider methodology: Read documentation about survivorship bias treatment. Reputable providers are explicit about whether datasets include defunct entities.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Data Structuring and Aggregation</span>"
    ]
  },
  {
    "objectID": "data-analysis/structuring/index.html#summary",
    "href": "data-analysis/structuring/index.html#summary",
    "title": "14¬† Data Structuring and Aggregation",
    "section": "14.4 Summary",
    "text": "14.4 Summary\nEffective financial data analysis requires understanding how data structure shapes what questions you can answer and what mistakes you might make. Keys and identifiers determine how observations are uniquely identified; choosing whether to use regular columns or special index structures affects code clarity and operation convenience. Grouping and aggregation operations are central to computing statistics within subsets, whether sector averages, monthly volatility, or portfolio characteristics. Both pandas and Polars provide powerful capabilities, with pandas offering flexibility and ecosystem maturity while Polars delivers better performance through query optimization and parallel execution.\nLook-ahead bias and survivorship bias represent the most serious threats to validity in financial empirical work. Look-ahead bias uses information that wouldn‚Äôt have been available at decision time, making strategies appear more profitable than possible. Survivorship bias analyzes only entities that survived to the sample endpoint, excluding failures and creating upward-biased performance measures. Both require careful attention to data structure, filtering logic, and temporal alignment.\nKey practices for robust financial data analysis include:\n\nExplicitly validate that composite keys uniquely identify observations\nChoose index structures based on clarity and operation convenience, not capability\nUse time-aware operations (resampling, shifting) carefully to maintain temporal integrity\nExpress grouped calculations explicitly, making aggregation logic clear\nUse complete databases that include delisted securities and defunct entities\nDocument assumptions about data availability and point-in-time values\n\nThese skills form the foundation for more advanced topics: joining datasets, reshaping between long and wide formats, and handling complex temporal relationships. The ability to structure data correctly and aggregate it appropriately without introducing bias separates valid empirical research from plausible-looking but incorrect analyses.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Data Structuring and Aggregation</span>"
    ]
  },
  {
    "objectID": "data-analysis/reshaping/index.html",
    "href": "data-analysis/reshaping/index.html",
    "title": "15¬† Reshaping Data",
    "section": "",
    "text": "15.1 Long vs.¬†Wide Formats\nFinancial data rarely arrives in the exact format you need for analysis. Stock returns might come as one row per stock-date combination, but you need a matrix with dates as rows and stocks as columns. Or you might receive a wide table of quarterly earnings across columns that needs to be converted into a tidy long format for regression analysis. These transformations‚Äîcollectively known as ‚Äúreshaping‚Äù‚Äîare fundamental operations in empirical finance.\nThis chapter covers the essential reshaping operations you‚Äôll use repeatedly: converting between long and wide formats, pivoting and melting data, and stacking and unstacking hierarchical indices. We‚Äôll work through practical examples using both pandas and Polars, showing how each library handles these transformations.\nThe distinction between long and wide formats is fundamental to data analysis. Understanding when to use each format, and how to convert between them, will save you countless hours of frustration.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "data-analysis/reshaping/index.html#long-vs.-wide-formats",
    "href": "data-analysis/reshaping/index.html#long-vs.-wide-formats",
    "title": "15¬† Reshaping Data",
    "section": "",
    "text": "15.1.1 What Are Long and Wide Formats?\nWide format stores each entity as a row and each time period (or category) as a separate column. This is how many people naturally think about panel data‚Äîit looks like a spreadsheet:\ndate        AAPL    MSFT    GOOGL\n2024-01-01  0.012   0.008   0.015\n2024-01-02  -0.005  0.003   0.002\n2024-01-03  0.018   0.012   0.010\nLong format (also called ‚Äútidy‚Äù or ‚Äúnarrow‚Äù format) stores each observation as a separate row, with columns indicating the entity, time period, and value:\ndate        ticker  return\n2024-01-01  AAPL    0.012\n2024-01-01  MSFT    0.008\n2024-01-01  GOOGL   0.015\n2024-01-02  AAPL    -0.005\n2024-01-02  MSFT    0.003\n...\nThe same information appears in both formats, but the organization is completely different.\n\n\n15.1.2 When to Use Each Format\nUse wide format when:\n\nPerforming matrix operations (correlation matrices, portfolio optimization)\nCalculating cross-sectional statistics (comparing values across entities at a point in time)\nCreating visualizations that show multiple series over time\nWorking with time series models that expect matrices\n\nUse long format when:\n\nRunning regressions or statistical models (most modeling libraries expect long data)\nCreating grouped or faceted visualizations\nFiltering or aggregating by category\nFollowing ‚Äútidy data‚Äù principles for data analysis\n\nIn empirical finance, you‚Äôll often start with data in one format and need to convert it for your specific analysis. For example, stock return data from CRSP arrives in long format (one row per stock-date), but calculating a covariance matrix requires wide format (dates as rows, stocks as columns).\n\n\n15.1.3 A Simple Example\nLet‚Äôs create a small dataset of daily stock returns in long format:\n\nimport pandas as pd\nimport polars as pl\nfrom datetime import datetime, timedelta\n\n# Create sample return data in long format\ndates = pd.date_range('2024-01-01', periods=5, freq='D')\ntickers = ['AAPL', 'MSFT', 'GOOGL']\n\n# pandas version\ndata_long_pd = pd.DataFrame([\n    {'date': date, 'ticker': ticker, 'return': round(0.01 * (hash(str(date) + ticker) % 100 - 50) / 100, 4)}\n    for date in dates\n    for ticker in tickers\n])\n\ndata_long_pd.head(10)\n\n\n\n\n\n\n\n\ndate\nticker\nreturn\n\n\n\n\n0\n2024-01-01\nAAPL\n0.0047\n\n\n1\n2024-01-01\nMSFT\n0.0023\n\n\n2\n2024-01-01\nGOOGL\n-0.0038\n\n\n3\n2024-01-02\nAAPL\n0.0047\n\n\n4\n2024-01-02\nMSFT\n0.0035\n\n\n5\n2024-01-02\nGOOGL\n-0.0009\n\n\n6\n2024-01-03\nAAPL\n-0.0027\n\n\n7\n2024-01-03\nMSFT\n-0.0035\n\n\n8\n2024-01-03\nGOOGL\n-0.0026\n\n\n9\n2024-01-04\nAAPL\n-0.0017\n\n\n\n\n\n\n\n\n# Polars version\ndata_long_pl = pl.DataFrame([\n    {'date': date, 'ticker': ticker, 'return': round(0.01 * (hash(str(date) + ticker) % 100 - 50) / 100, 4)}\n    for date in dates\n    for ticker in tickers\n])\n\ndata_long_pl.head(10)\n\n\nshape: (10, 3)\n\n\n\ndate\nticker\nreturn\n\n\ndatetime[Œºs]\nstr\nf64\n\n\n\n\n2024-01-01 00:00:00\n\"AAPL\"\n0.0047\n\n\n2024-01-01 00:00:00\n\"MSFT\"\n0.0023\n\n\n2024-01-01 00:00:00\n\"GOOGL\"\n-0.0038\n\n\n2024-01-02 00:00:00\n\"AAPL\"\n0.0047\n\n\n2024-01-02 00:00:00\n\"MSFT\"\n0.0035\n\n\n2024-01-02 00:00:00\n\"GOOGL\"\n-0.0009\n\n\n2024-01-03 00:00:00\n\"AAPL\"\n-0.0027\n\n\n2024-01-03 00:00:00\n\"MSFT\"\n-0.0035\n\n\n2024-01-03 00:00:00\n\"GOOGL\"\n-0.0026\n\n\n2024-01-04 00:00:00\n\"AAPL\"\n-0.0017\n\n\n\n\n\n\nThis long format is ideal for most statistical operations. Each row is an observation: a specific stock on a specific date. You can easily filter to specific stocks, calculate summary statistics by ticker, or run panel regressions.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "data-analysis/reshaping/index.html#pivot-melt-stack-unstack",
    "href": "data-analysis/reshaping/index.html#pivot-melt-stack-unstack",
    "title": "15¬† Reshaping Data",
    "section": "15.2 Pivot, Melt, Stack, Unstack",
    "text": "15.2 Pivot, Melt, Stack, Unstack\nPython‚Äôs data libraries provide several operations for reshaping data. While they might seem redundant at first, each serves a specific purpose and understanding all of them makes you more fluent in data manipulation.\n\n15.2.1 Pivot: Long to Wide\nThe pivot operation transforms long format data into wide format. You specify which column becomes the index (rows), which becomes the columns, and which contains the values to fill the resulting matrix.\npandas syntax:\n\n# Convert long to wide: dates as rows, tickers as columns\ndata_wide_pd = data_long_pd.pivot(\n1    index='date',\n2    columns='ticker',\n3    values='return'\n)\n\ndata_wide_pd\n\n\n1\n\nColumn that becomes the row index\n\n2\n\nColumn whose unique values become new columns\n\n3\n\nColumn containing the values to fill the matrix\n\n\n\n\n\n\n\n\n\n\nticker\nAAPL\nGOOGL\nMSFT\n\n\ndate\n\n\n\n\n\n\n\n2024-01-01\n0.0047\n-0.0038\n0.0023\n\n\n2024-01-02\n0.0047\n-0.0009\n0.0035\n\n\n2024-01-03\n-0.0027\n-0.0026\n-0.0035\n\n\n2024-01-04\n-0.0017\n0.0007\n-0.0033\n\n\n2024-01-05\n-0.0013\n0.0006\n-0.0012\n\n\n\n\n\n\n\nNotice that pivot creates a DataFrame where:\n\nEach unique date becomes a row\nEach unique ticker becomes a column\nReturn values fill the cells\n\nPolars syntax:\n\n# Polars uses pivot differently - need to specify aggregation\ndata_wide_pl = data_long_pl.pivot(\n    index='date',\n    columns='ticker',\n    values='return'\n)\n\ndata_wide_pl\n\n/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_60679/105412148.py:2: DeprecationWarning: the argument `columns` for `DataFrame.pivot` is deprecated. It was renamed to `on` in version 1.0.0.\n  data_wide_pl = data_long_pl.pivot(\n\n\n\nshape: (5, 4)\n\n\n\ndate\nAAPL\nMSFT\nGOOGL\n\n\ndatetime[Œºs]\nf64\nf64\nf64\n\n\n\n\n2024-01-01 00:00:00\n0.0047\n0.0023\n-0.0038\n\n\n2024-01-02 00:00:00\n0.0047\n0.0035\n-0.0009\n\n\n2024-01-03 00:00:00\n-0.0027\n-0.0035\n-0.0026\n\n\n2024-01-04 00:00:00\n-0.0017\n-0.0033\n0.0007\n\n\n2024-01-05 00:00:00\n-0.0013\n-0.0012\n0.0006\n\n\n\n\n\n\nPolars‚Äô pivot is similar but requires you to think about aggregation from the start. If multiple rows could map to the same cell (same date and ticker), you must specify how to combine them (sum, mean, first, etc.).\n\n\n15.2.2 Handling Duplicate Entries\nOne common issue with pivoting is duplicate entries. What happens if your data has multiple return observations for the same stock on the same date?\n\n# Create data with duplicates\ndata_with_dups_pd = pd.DataFrame([\n    {'date': '2024-01-01', 'ticker': 'AAPL', 'return': 0.01},\n    {'date': '2024-01-01', 'ticker': 'AAPL', 'return': 0.02},  # duplicate!\n    {'date': '2024-01-01', 'ticker': 'MSFT', 'return': 0.03},\n])\n\n# pandas pivot will fail with duplicates\ntry:\n    data_with_dups_pd.pivot(index='date', columns='ticker', values='return')\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n\nError: Index contains duplicate entries, cannot reshape\n\n\nWhen pandas encounters duplicates, it raises an error. You have two options:\n\nUse pivot_table with an aggregation function\nClean your data first to remove duplicates\n\n\n# Option 1: Use pivot_table with aggregation\nwide_with_agg_pd = data_with_dups_pd.pivot_table(\n    index='date',\n    columns='ticker',\n    values='return',\n    aggfunc='mean'  # or 'sum', 'first', 'last', etc.\n)\n\nwide_with_agg_pd\n\n\n\n\n\n\n\nticker\nAAPL\nMSFT\n\n\ndate\n\n\n\n\n\n\n2024-01-01\n0.015\n0.03\n\n\n\n\n\n\n\nPolars requires you to specify aggregation from the start, so duplicates are handled automatically:\n\ndata_with_dups_pl = pl.DataFrame([\n    {'date': '2024-01-01', 'ticker': 'AAPL', 'return': 0.01},\n    {'date': '2024-01-01', 'ticker': 'AAPL', 'return': 0.02},\n    {'date': '2024-01-01', 'ticker': 'MSFT', 'return': 0.03},\n])\n\n# Polars pivot with aggregation\nwide_with_agg_pl = data_with_dups_pl.pivot(\n    index='date',\n    columns='ticker',\n    values='return',\n    aggregate_function='mean'\n)\n\nwide_with_agg_pl\n\n/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_60679/556176386.py:8: DeprecationWarning: the argument `columns` for `DataFrame.pivot` is deprecated. It was renamed to `on` in version 1.0.0.\n  wide_with_agg_pl = data_with_dups_pl.pivot(\n\n\n\nshape: (1, 3)\n\n\n\ndate\nAAPL\nMSFT\n\n\nstr\nf64\nf64\n\n\n\n\n\"2024-01-01\"\n0.015\n0.03\n\n\n\n\n\n\n\n\n15.2.3 Melt/Unpivot: Wide to Long\nThe melt operation (called unpivot in Polars) is the inverse of pivot‚Äîit transforms wide format data back into long format. You specify which columns to keep as identifiers and which to ‚Äúmelt‚Äù into a single column.\npandas syntax:\n\n# Reset index to make date a regular column\ndata_wide_pd_reset = data_wide_pd.reset_index()\n\n# Melt back to long format\ndata_melted_pd = data_wide_pd_reset.melt(\n1    id_vars=['date'],\n2    value_vars=['AAPL', 'MSFT', 'GOOGL'],\n3    var_name='ticker',\n4    value_name='return'\n)\n\ndata_melted_pd.head(10)\n\n\n1\n\nColumns to keep as identifiers\n\n2\n\nColumns to melt (optional‚Äîmelts all others if not specified)\n\n3\n\nName for the new column containing original column names\n\n4\n\nName for the new column containing values\n\n\n\n\n\n\n\n\n\n\n\ndate\nticker\nreturn\n\n\n\n\n0\n2024-01-01\nAAPL\n0.0047\n\n\n1\n2024-01-02\nAAPL\n0.0047\n\n\n2\n2024-01-03\nAAPL\n-0.0027\n\n\n3\n2024-01-04\nAAPL\n-0.0017\n\n\n4\n2024-01-05\nAAPL\n-0.0013\n\n\n5\n2024-01-01\nMSFT\n0.0023\n\n\n6\n2024-01-02\nMSFT\n0.0035\n\n\n7\n2024-01-03\nMSFT\n-0.0035\n\n\n8\n2024-01-04\nMSFT\n-0.0033\n\n\n9\n2024-01-05\nMSFT\n-0.0012\n\n\n\n\n\n\n\nPolars syntax:\n\n# Polars uses unpivot (melt is deprecated)\ndata_melted_pl = data_wide_pl.unpivot(\n1    index=['date'],\n2    on=['AAPL', 'MSFT', 'GOOGL'],\n    variable_name='ticker',\n    value_name='return'\n)\n\ndata_melted_pl.head(10)\n\n\n1\n\nColumns to keep as identifiers (called index in Polars)\n\n2\n\nColumns to unpivot (called on in Polars)\n\n\n\n\n\nshape: (10, 3)\n\n\n\ndate\nticker\nreturn\n\n\ndatetime[Œºs]\nstr\nf64\n\n\n\n\n2024-01-01 00:00:00\n\"AAPL\"\n0.0047\n\n\n2024-01-02 00:00:00\n\"AAPL\"\n0.0047\n\n\n2024-01-03 00:00:00\n\"AAPL\"\n-0.0027\n\n\n2024-01-04 00:00:00\n\"AAPL\"\n-0.0017\n\n\n2024-01-05 00:00:00\n\"AAPL\"\n-0.0013\n\n\n2024-01-01 00:00:00\n\"MSFT\"\n0.0023\n\n\n2024-01-02 00:00:00\n\"MSFT\"\n0.0035\n\n\n2024-01-03 00:00:00\n\"MSFT\"\n-0.0035\n\n\n2024-01-04 00:00:00\n\"MSFT\"\n-0.0033\n\n\n2024-01-05 00:00:00\n\"MSFT\"\n-0.0012\n\n\n\n\n\n\n\n\n15.2.4 Stack and Unstack: Index-Based Reshaping\nWhile pivot and melt work with columns, stack and unstack work with index levels. They‚Äôre particularly useful when you have MultiIndex DataFrames (hierarchical indices with multiple levels).\nStack: Moves a column level into the row index (wide to long)\nUnstack: Moves a row index level into the columns (long to wide)\n\n# Create a MultiIndex DataFrame\n# Set both date and ticker as index\ndata_multi_pd = data_long_pd.set_index(['date', 'ticker'])\n\ndata_multi_pd.head()\n\n\n\n\n\n\n\n\n\nreturn\n\n\ndate\nticker\n\n\n\n\n\n2024-01-01\nAAPL\n0.0047\n\n\nMSFT\n0.0023\n\n\nGOOGL\n-0.0038\n\n\n2024-01-02\nAAPL\n0.0047\n\n\nMSFT\n0.0035\n\n\n\n\n\n\n\n\n# Unstack: move ticker from index to columns (long to wide)\ndata_unstacked_pd = data_multi_pd.unstack(level='ticker')\n\ndata_unstacked_pd\n\n\n\n\n\n\n\n\nreturn\n\n\nticker\nAAPL\nGOOGL\nMSFT\n\n\ndate\n\n\n\n\n\n\n\n2024-01-01\n0.0047\n-0.0038\n0.0023\n\n\n2024-01-02\n0.0047\n-0.0009\n0.0035\n\n\n2024-01-03\n-0.0027\n-0.0026\n-0.0035\n\n\n2024-01-04\n-0.0017\n0.0007\n-0.0033\n\n\n2024-01-05\n-0.0013\n0.0006\n-0.0012\n\n\n\n\n\n\n\n\n# Stack: move ticker from columns back to index (wide to long)\ndata_stacked_pd = data_unstacked_pd.stack(level='ticker')\n\ndata_stacked_pd.head()\n\n/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_60679/4172318071.py:2: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n  data_stacked_pd = data_unstacked_pd.stack(level='ticker')\n\n\n\n\n\n\n\n\n\n\nreturn\n\n\ndate\nticker\n\n\n\n\n\n2024-01-01\nAAPL\n0.0047\n\n\nGOOGL\n-0.0038\n\n\nMSFT\n0.0023\n\n\n2024-01-02\nAAPL\n0.0047\n\n\nGOOGL\n-0.0009\n\n\n\n\n\n\n\nThe key difference between pivot/melt and stack/unstack:\n\npivot/melt: Work with regular columns, create new structure from scratch\nstack/unstack: Work with existing MultiIndex structure, move levels between index and columns\n\nPolars doesn‚Äôt have direct equivalents to stack/unstack because it doesn‚Äôt use MultiIndex. Instead, you would use combinations of pivot, melt, group_by, and join to achieve the same results.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "data-analysis/reshaping/index.html#summary",
    "href": "data-analysis/reshaping/index.html#summary",
    "title": "15¬† Reshaping Data",
    "section": "15.3 Summary",
    "text": "15.3 Summary\nReshaping data is a fundamental skill in empirical finance. The key operations are:\n\nLong vs.¬†Wide: Choose the format appropriate for your analysis\n\nLong for modeling and tidy operations\nWide for matrix operations and cross-sectional analysis\n\nPivot/Melt: Convert between long and wide formats\n\npivot: long ‚Üí wide (create matrix from observations)\nmelt/unpivot: wide ‚Üí long (create observations from matrix)\nHandle duplicates carefully with aggregation\n\nStack/Unstack: Work with MultiIndex structures\n\nunstack: move index level to columns\nstack: move column level to index\nPrimarily useful in pandas (Polars doesn‚Äôt use MultiIndex)\n\n\nThe specific syntax differs between pandas and Polars, but the concepts are universal. pandas uses pivot, pivot_table, melt, stack, and unstack. Polars uses pivot and unpivot, relying more on grouped operations for reshaping tasks.\nPractice these operations until they become second nature. You‚Äôll use them constantly in empirical finance, and fluency with reshaping will make your analyses both faster and more reliable.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "data-analysis/joins/index.html",
    "href": "data-analysis/joins/index.html",
    "title": "16¬† Joins and Merges",
    "section": "",
    "text": "16.1 Join Types: Cardinality Matters\nCombining datasets is fundamental to empirical finance. Stock returns live in one database (CRSP), accounting data in another (Compustat), analyst forecasts in a third (I/B/E/S). Real research requires merging these sources, and doing it wrong can silently corrupt your results.\nThis chapter develops the theory and practice of joins. We start with the conceptual framework‚Äîwhat happens when you match rows between datasets‚Äîthen move to specific merge operations. Special attention goes to asof joins, essential for time-series data with mismatched timestamps. We close with diagnostic techniques to catch the merge errors that plague real-world research.\nBefore learning merge syntax, understand what happens to row counts. The relationship between keys in your datasets determines whether you end up with more rows, fewer rows, or the same number. Getting this wrong is catastrophic‚Äîyou might duplicate observations, lose data, or create spurious patterns.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Joins and Merges</span>"
    ]
  },
  {
    "objectID": "data-analysis/joins/index.html#join-types-cardinality-matters",
    "href": "data-analysis/joins/index.html#join-types-cardinality-matters",
    "title": "16¬† Joins and Merges",
    "section": "",
    "text": "16.1.1 One-to-One Joins\nEach key appears at most once in both datasets. This is the simplest case: rows either match or they don‚Äôt.\n\n\n\n\n\ngraph LR\n    subgraph \"Dataset A\"\n        A1[ticker: AAPL&lt;br/&gt;ret: 0.02]\n        A2[ticker: MSFT&lt;br/&gt;ret: -0.01]\n    end\n\n    subgraph \"Dataset B\"\n        B1[ticker: AAPL&lt;br/&gt;mktcap: 2500B]\n        B2[ticker: MSFT&lt;br/&gt;mktcap: 2200B]\n    end\n\n    A1 -.-&gt;|match| B1\n    A2 -.-&gt;|match| B2\n\n    style A1 fill:#e1f5ff\n    style A2 fill:#e1f5ff\n    style B1 fill:#fff4e1\n    style B2 fill:#fff4e1\n\n\n One-to-one join: each key matches at most once \n\n\n\nExample: merging monthly portfolio returns with portfolio characteristics. Each portfolio-month appears once in each dataset.\n\nimport pandas as pd\nimport polars as pl\nfrom datetime import date\n\n# Monthly portfolio returns\nreturns = pd.DataFrame({\n    'portfolio': ['growth', 'value', 'momentum'],\n    'month': ['2024-01', '2024-01', '2024-01'],\n    'return': [0.025, -0.010, 0.035]\n})\n\n# Portfolio characteristics (computed separately)\ncharacteristics = pd.DataFrame({\n    'portfolio': ['growth', 'value', 'momentum'],\n    'month': ['2024-01', '2024-01', '2024-01'],\n    'avg_size': [5000, 3000, 4000],\n    'avg_bm': [0.5, 2.5, 1.2]\n})\n\n# One-to-one merge\nmerged = returns.merge(\n    characteristics,\n1    on=['portfolio', 'month'],\n2    validate='one_to_one'\n)\nmerged\n\n\n1\n\nColumns that must match exactly in both DataFrames.\n\n2\n\nRaises MergeError if duplicate keys exist in either DataFrame.\n\n\n\n\n\n\n\n\n\n\n\nportfolio\nmonth\nreturn\navg_size\navg_bm\n\n\n\n\n0\ngrowth\n2024-01\n0.025\n5000\n0.5\n\n\n1\nvalue\n2024-01\n-0.010\n3000\n2.5\n\n\n2\nmomentum\n2024-01\n0.035\n4000\n1.2\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantAlways Validate Cardinality\n\n\n\nUse validate='one_to_one' in pandas or check row counts before and after. One-to-one merges should never change the number of rows (except for unmatched keys with inner joins). If row count changes unexpectedly, you have duplicate keys somewhere.\n\n\n\n\n16.1.2 One-to-Many Joins\nKeys are unique in one dataset but repeated in the other. This expands rows from the unique side.\n\n\n\n\n\ngraph LR\n    subgraph \"Firms (One)\"\n        F1[permno: 10107&lt;br/&gt;sector: Tech]\n    end\n\n    subgraph \"Returns (Many)\"\n        R1[permno: 10107&lt;br/&gt;date: 2024-01-02&lt;br/&gt;ret: 0.01]\n        R2[permno: 10107&lt;br/&gt;date: 2024-01-03&lt;br/&gt;ret: -0.02]\n        R3[permno: 10107&lt;br/&gt;date: 2024-01-04&lt;br/&gt;ret: 0.03]\n    end\n\n    F1 -.-&gt;|broadcast| R1\n    F1 -.-&gt;|broadcast| R2\n    F1 -.-&gt;|broadcast| R3\n\n    style F1 fill:#e1f5ff\n    style R1 fill:#fff4e1\n    style R2 fill:#fff4e1\n    style R3 fill:#fff4e1\n\n\n One-to-many join: firm-level data matched to daily returns \n\n\n\nExample: merging firm characteristics (one row per firm) with daily returns (many rows per firm).\n\n# Firm characteristics (one row per firm)\nfirms = pd.DataFrame({\n    'permno': [10107, 10107, 14593],\n    'year': [2023, 2024, 2024],\n    'sector': ['Tech', 'Tech', 'Finance'],\n    'headquarters': ['CA', 'CA', 'NY']\n})\n\n# Daily returns (many rows per firm-year)\nreturns = pd.DataFrame({\n    'permno': [10107, 10107, 10107, 14593, 14593],\n    'year': [2024, 2024, 2024, 2024, 2024],\n    'date': ['2024-01-02', '2024-01-03', '2024-01-04',\n             '2024-01-02', '2024-01-03'],\n    'ret': [0.01, -0.02, 0.03, 0.005, -0.001]\n})\n\n# One-to-many merge: firm characteristics broadcast to each return\nmerged = returns.merge(\n    firms,\n    on=['permno', 'year'],\n    validate='many_to_one'  # Ensures firms has unique keys\n)\nmerged\n\n\n\n\n\n\n\n\npermno\nyear\ndate\nret\nsector\nheadquarters\n\n\n\n\n0\n10107\n2024\n2024-01-02\n0.010\nTech\nCA\n\n\n1\n10107\n2024\n2024-01-03\n-0.020\nTech\nCA\n\n\n2\n10107\n2024\n2024-01-04\n0.030\nTech\nCA\n\n\n3\n14593\n2024\n2024-01-02\n0.005\nFinance\nNY\n\n\n4\n14593\n2024\n2024-01-03\n-0.001\nFinance\nNY\n\n\n\n\n\n\n\nNotice how sector and headquarters repeat for each date. This is broadcasting: the single firm row expands to match multiple return rows.\n\n\n\n\n\n\nWarningWatch for Unintended Duplicates\n\n\n\nIf you think you have a one-to-many join but the ‚Äúone‚Äù side has duplicates, you‚Äôll get a many-to-many join instead. This multiplies rows in ways that corrupt analyses. Always validate.\n\n\n\n\n16.1.3 Many-to-Many Joins\nKeys repeat in both datasets. Every combination of matching keys produces a row. This explodes row counts and is rarely what you want.\n\n\n\n\n\ngraph LR\n    subgraph \"Dataset A (Many)\"\n        A1[sector: Tech&lt;br/&gt;year: 2024&lt;br/&gt;firm: AAPL]\n        A2[sector: Tech&lt;br/&gt;year: 2024&lt;br/&gt;firm: MSFT]\n    end\n\n    subgraph \"Dataset B (Many)\"\n        B1[sector: Tech&lt;br/&gt;year: 2024&lt;br/&gt;analyst: Goldman]\n        B2[sector: Tech&lt;br/&gt;year: 2024&lt;br/&gt;analyst: Morgan]\n    end\n\n    A1 -.-&gt;|match| B1\n    A1 -.-&gt;|match| B2\n    A2 -.-&gt;|match| B1\n    A2 -.-&gt;|match| B2\n\n    style A1 fill:#e1f5ff\n    style A2 fill:#e1f5ff\n    style B1 fill:#fff4e1\n    style B2 fill:#fff4e1\n\n\n Many-to-many join: combinatorial explosion \n\n\n\nExample: joining firm-level data to analyst coverage by sector-year creates all combinations.\n\n# Firms in each sector-year\nfirms = pd.DataFrame({\n    'sector': ['Tech', 'Tech', 'Finance'],\n    'year': [2024, 2024, 2024],\n    'ticker': ['AAPL', 'MSFT', 'JPM'],\n    'return': [0.25, 0.30, 0.15]\n})\n\n# Analyst reports by sector-year (multiple analysts per sector)\nanalysts = pd.DataFrame({\n    'sector': ['Tech', 'Tech', 'Finance'],\n    'year': [2024, 2024, 2024],\n    'analyst': ['Goldman', 'Morgan', 'Citi'],\n    'recommendation': ['Buy', 'Hold', 'Buy']\n})\n\n# Many-to-many merge: DANGER!\nmerged = firms.merge(analysts, on=['sector', 'year'])\nmerged\n\n\n\n\n\n\n\n\nsector\nyear\nticker\nreturn\nanalyst\nrecommendation\n\n\n\n\n0\nTech\n2024\nAAPL\n0.25\nGoldman\nBuy\n\n\n1\nTech\n2024\nAAPL\n0.25\nMorgan\nHold\n\n\n2\nTech\n2024\nMSFT\n0.30\nGoldman\nBuy\n\n\n3\nTech\n2024\nMSFT\n0.30\nMorgan\nHold\n\n\n4\nFinance\n2024\nJPM\n0.15\nCiti\nBuy\n\n\n\n\n\n\n\nNotice how the row count exploded:\n\nprint(f\"Original firms: {len(firms)}, analysts: {len(analysts)}, merged: {len(merged)}\")\n\nOriginal firms: 3, analysts: 3, merged: 5\n\n\n\n\n\n\n\n\nCautionMany-to-Many is Usually Wrong\n\n\n\nIn finance, many-to-many joins typically indicate a modeling error. You probably need to aggregate one dataset first (e.g., average analyst recommendations by sector-year) or use a more specific join key. Pandas allows many-to-many by default; consider this a bug, not a feature.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Joins and Merges</span>"
    ]
  },
  {
    "objectID": "data-analysis/joins/index.html#merge-operations-which-rows-survive",
    "href": "data-analysis/joins/index.html#merge-operations-which-rows-survive",
    "title": "16¬† Joins and Merges",
    "section": "16.2 Merge Operations: Which Rows Survive?",
    "text": "16.2 Merge Operations: Which Rows Survive?\nJoin type (one-to-one, etc.) describes cardinality. Merge operation (inner, outer, left, right) describes which rows to keep when keys don‚Äôt match perfectly.\n\n16.2.1 Inner Join: Intersection Only\nKeep only rows where keys match in both datasets. This is conservative‚Äîyou lose any observation that doesn‚Äôt have a partner‚Äîbut ensures every row has complete data.\n\n\n\n\n\ngraph TD\n    subgraph \"Dataset A\"\n        A1[AAPL]\n        A2[MSFT]\n        A3[TSLA]\n    end\n\n    subgraph \"Dataset B\"\n        B1[AAPL]\n        B2[GOOGL]\n        B3[TSLA]\n    end\n\n    subgraph \"Result\"\n        R1[AAPL]\n        R2[TSLA]\n    end\n\n    A1 --&gt; R1\n    B1 --&gt; R1\n    A3 --&gt; R2\n    B3 --&gt; R2\n\n    style A2 fill:#ffcccc\n    style B2 fill:#ffcccc\n    style R1 fill:#ccffcc\n    style R2 fill:#ccffcc\n\n\n Inner join: only matching keys survive \n\n\n\nExample: merging returns with accounting data. Only firm-years with both survive.\n\n# Stock returns\nreturns = pd.DataFrame({\n    'ticker': ['AAPL', 'MSFT', 'TSLA'],\n    'year': [2023, 2023, 2023],\n    'return': [0.45, 0.35, 0.10]\n})\n\n# Accounting data (missing MSFT)\naccounting = pd.DataFrame({\n    'ticker': ['AAPL', 'TSLA', 'GOOGL'],\n    'year': [2023, 2023, 2023],\n    'roa': [0.20, 0.05, 0.15],\n    'leverage': [0.30, 0.10, 0.05]\n})\n\n# Inner join: only AAPL and TSLA have both\ninner = returns.merge(accounting, on=['ticker', 'year'], how='inner')\ninner\n\n\n\n\n\n\n\n\nticker\nyear\nreturn\nroa\nleverage\n\n\n\n\n0\nAAPL\n2023\n0.45\n0.20\n0.3\n\n\n1\nTSLA\n2023\n0.10\n0.05\n0.1\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipWhen to Use Inner Joins\n\n\n\nUse inner joins when you need data from both sources for your analysis. For example, computing correlations between returns and accounting ratios requires both variables. The sample restriction is a feature, not a bug‚Äîyou‚Äôre explicitly limiting to observations where analysis is possible.\n\n\n\n\n16.2.2 Left Join: Keep All from Left Dataset\nKeep every row from the left dataset, adding data from the right where available. Unmatched right rows disappear. Missing values (NaN) fill columns from the right dataset where no match exists.\n\n\n\n\n\ngraph TD\n    subgraph \"Left Dataset\"\n        L1[AAPL]\n        L2[MSFT]\n        L3[TSLA]\n    end\n\n    subgraph \"Right Dataset\"\n        R1[AAPL]\n        R2[GOOGL]\n        R3[TSLA]\n    end\n\n    subgraph \"Result\"\n        RES1[AAPL + match]\n        RES2[MSFT + NaN]\n        RES3[TSLA + match]\n    end\n\n    L1 --&gt; RES1\n    L2 --&gt; RES2\n    L3 --&gt; RES3\n    R1 --&gt; RES1\n    R3 --&gt; RES3\n\n    style R2 fill:#ffcccc\n    style RES1 fill:#ccffcc\n    style RES2 fill:#ffffcc\n    style RES3 fill:#ccffcc\n\n\n Left join: all left-side rows survive, right-side data added where available \n\n\n\nExample: keeping all returns, adding accounting data where available.\n\n# Left join: keep all returns\nleft = returns.merge(accounting, on=['ticker', 'year'], how='left')\nleft\n\n\n\n\n\n\n\n\nticker\nyear\nreturn\nroa\nleverage\n\n\n\n\n0\nAAPL\n2023\n0.45\n0.20\n0.3\n\n\n1\nMSFT\n2023\n0.35\nNaN\nNaN\n\n\n2\nTSLA\n2023\n0.10\n0.05\n0.1\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteLeft Joins Preserve Sample\n\n\n\nUse left joins when the left dataset defines your sample. For example, if you‚Äôve carefully constructed a set of stocks for analysis, use those as the left dataset and merge in additional characteristics. This ensures you keep your full sample even if some characteristics are missing.\n\n\n\n\n16.2.3 Right Join: Keep All from Right Dataset\nThe mirror of left join. Keep every row from the right dataset, add data from left where available. In practice, just swap your datasets and use a left join‚Äîright joins exist for symmetry but rarely clarify code.\n\n# Right join: keep all accounting observations\nright = returns.merge(accounting, on=['ticker', 'year'], how='right')\nright\n\n\n\n\n\n\n\n\nticker\nyear\nreturn\nroa\nleverage\n\n\n\n\n0\nAAPL\n2023\n0.45\n0.20\n0.30\n\n\n1\nTSLA\n2023\n0.10\n0.05\n0.10\n\n\n2\nGOOGL\n2023\nNaN\n0.15\n0.05\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipPrefer Left Joins for Clarity\n\n\n\nWrite accounting.merge(returns, how='left') instead of returns.merge(accounting, how='right'). Reading left-to-right matches execution order and makes the priority explicit: you‚Äôre starting with accounting data and adding returns.\n\n\n\n\n16.2.4 Outer Join: Keep Everything\nKeep all rows from both datasets. This maximizes sample size but can create large numbers of missing values.\n\n\n\n\n\ngraph TD\n    subgraph \"Dataset A\"\n        A1[AAPL]\n        A2[MSFT]\n        A3[TSLA]\n    end\n\n    subgraph \"Dataset B\"\n        B1[AAPL]\n        B2[GOOGL]\n        B3[TSLA]\n    end\n\n    subgraph \"Result\"\n        R1[AAPL + match]\n        R2[MSFT + NaN]\n        R3[TSLA + match]\n        R4[GOOGL + NaN]\n    end\n\n    A1 --&gt; R1\n    A2 --&gt; R2\n    A3 --&gt; R3\n    B1 --&gt; R1\n    B2 --&gt; R4\n    B3 --&gt; R3\n\n    style R1 fill:#ccffcc\n    style R2 fill:#ffffcc\n    style R3 fill:#ccffcc\n    style R4 fill:#ffffcc\n\n\n Outer join: all rows from both datasets survive \n\n\n\nExample: keeping all tickers from either dataset.\n\n# Outer join: keep everything\nouter = returns.merge(accounting, on=['ticker', 'year'], how='outer')\nouter\n\n\n\n\n\n\n\n\nticker\nyear\nreturn\nroa\nleverage\n\n\n\n\n0\nAAPL\n2023\n0.45\n0.20\n0.30\n\n\n1\nGOOGL\n2023\nNaN\n0.15\n0.05\n\n\n2\nMSFT\n2023\n0.35\nNaN\nNaN\n\n\n3\nTSLA\n2023\n0.10\n0.05\n0.10\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningOuter Joins Hide Problems\n\n\n\nOuter joins are dangerous because they succeed even when keys match poorly. You might merge stock returns with bond yields on date, get mostly NaN, but never realize the datasets use different date formats. Always inspect outer join results carefully and report match rates.\n\n\n\n\n16.2.5 Polars Merge Syntax\nPolars uses similar concepts but different method names. The key difference: Polars is more explicit about join types and discourages many-to-many joins by default.\n\n# Same data in Polars\nreturns_pl = pl.DataFrame({\n    'ticker': ['AAPL', 'MSFT', 'TSLA'],\n    'year': [2023, 2023, 2023],\n    'return': [0.45, 0.35, 0.10]\n})\n\naccounting_pl = pl.DataFrame({\n    'ticker': ['AAPL', 'TSLA', 'GOOGL'],\n    'year': [2023, 2023, 2023],\n    'roa': [0.20, 0.05, 0.15],\n    'leverage': [0.30, 0.10, 0.05]\n})\n\n# Inner join\ninner_pl = returns_pl.join(\n    accounting_pl,\n    on=['ticker', 'year'],\n    how='inner'\n)\ninner_pl\n\n\nshape: (2, 5)\n\n\n\nticker\nyear\nreturn\nroa\nleverage\n\n\nstr\ni64\nf64\nf64\nf64\n\n\n\n\n\"AAPL\"\n2023\n0.45\n0.2\n0.3\n\n\n\"TSLA\"\n2023\n0.1\n0.05\n0.1\n\n\n\n\n\n\n\n# Left join\nleft_pl = returns_pl.join(\n    accounting_pl,\n    on=['ticker', 'year'],\n    how='left'\n)\nleft_pl\n\n\nshape: (3, 5)\n\n\n\nticker\nyear\nreturn\nroa\nleverage\n\n\nstr\ni64\nf64\nf64\nf64\n\n\n\n\n\"AAPL\"\n2023\n0.45\n0.2\n0.3\n\n\n\"MSFT\"\n2023\n0.35\nnull\nnull\n\n\n\"TSLA\"\n2023\n0.1\n0.05\n0.1\n\n\n\n\n\n\n\n# Outer join (called 'full' in Polars)\nouter_pl = returns_pl.join(\n    accounting_pl,\n    on=['ticker', 'year'],\n    how='full'\n)\nouter_pl\n\n\nshape: (4, 7)\n\n\n\nticker\nyear\nreturn\nticker_right\nyear_right\nroa\nleverage\n\n\nstr\ni64\nf64\nstr\ni64\nf64\nf64\n\n\n\n\n\"AAPL\"\n2023\n0.45\n\"AAPL\"\n2023\n0.2\n0.3\n\n\n\"TSLA\"\n2023\n0.1\n\"TSLA\"\n2023\n0.05\n0.1\n\n\nnull\nnull\nnull\n\"GOOGL\"\n2023\n0.15\n0.05\n\n\n\"MSFT\"\n2023\n0.35\nnull\nnull\nnull\nnull\n\n\n\n\n\n\nPolars also provides a validate parameter similar to pandas:\n\n# Validate one-to-one\ninner_pl = returns_pl.join(\n    accounting_pl,\n    on=['ticker', 'year'],\n    how='inner',\n    validate='1:1'  # Raises error if violated\n)",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Joins and Merges</span>"
    ]
  },
  {
    "objectID": "data-analysis/joins/index.html#asof-joins-for-time-series-data",
    "href": "data-analysis/joins/index.html#asof-joins-for-time-series-data",
    "title": "16¬† Joins and Merges",
    "section": "16.3 Asof Joins for Time-Series Data",
    "text": "16.3 Asof Joins for Time-Series Data\nFinancial data rarely aligns perfectly in time. You have daily stock prices, quarterly earnings announcements, irregular analyst forecast updates. Matching these requires asof joins: find the most recent value as of each timestamp, without requiring exact matches.\n\n16.3.1 The Asof Join Problem\nSuppose you want to merge earnings announcement dates with stock returns. Earnings come out on irregular dates (whenever the company reports). You need to tag each daily return with the most recent earnings announcement as of that date. A standard merge fails because announcement dates don‚Äôt match return dates. An asof join solves this: for each return date, find the most recent announcement date on or before that date.\n\n\n16.3.2 Asof Join Mechanics\nAsof joins require sorted data and directional matching. The syntax varies between pandas and Polars, but the logic is identical:\n\nSort both datasets by time\nFor each row in the left dataset, find the row in the right dataset with the closest timestamp that doesn‚Äôt exceed the left timestamp (backward-looking)\nOptionally match exactly on other columns (e.g., ticker)\n\nExample: merging daily returns with quarterly earnings.\n\nimport numpy as np\n\n# Daily returns (many observations)\nreturns = pd.DataFrame({\n    'ticker': ['AAPL'] * 10,\n    'date': pd.date_range('2024-01-01', periods=10, freq='D'),\n    'return': np.random.randn(10) * 0.02\n})\n\n# Earnings announcements (sparse, irregular)\nearnings = pd.DataFrame({\n    'ticker': ['AAPL', 'AAPL', 'AAPL'],\n    'announce_date': pd.to_datetime(['2023-12-28', '2024-01-03', '2024-01-08']),\n    'eps': [1.25, 1.30, 1.35],\n    'surprise': [0.02, -0.01, 0.03]\n})\n\n# CRITICAL: Sort by time\n1returns = returns.sort_values('date')\nearnings = earnings.sort_values('announce_date')\n\n# Asof join: match each return to most recent earnings\nmerged = pd.merge_asof(\n    returns,\n    earnings,\n2    left_on='date',\n    right_on='announce_date',\n3    by='ticker',\n4    direction='backward'\n)\n\nmerged[['date', 'return', 'announce_date', 'eps', 'surprise']]\n\n\n1\n\nBoth DataFrames must be sorted by their time columns before asof join.\n\n2\n\nTime columns to match on (can have different names).\n\n3\n\nAdditional columns that must match exactly.\n\n4\n\nUse most recent prior value (avoids look-ahead bias).\n\n\n\n\n\n\n\n\n\n\n\ndate\nreturn\nannounce_date\neps\nsurprise\n\n\n\n\n0\n2024-01-01\n-0.032470\n2023-12-28\n1.25\n0.02\n\n\n1\n2024-01-02\n0.023386\n2023-12-28\n1.25\n0.02\n\n\n2\n2024-01-03\n-0.040501\n2024-01-03\n1.30\n-0.01\n\n\n3\n2024-01-04\n-0.017916\n2024-01-03\n1.30\n-0.01\n\n\n4\n2024-01-05\n-0.004434\n2024-01-03\n1.30\n-0.01\n\n\n5\n2024-01-06\n-0.020436\n2024-01-03\n1.30\n-0.01\n\n\n6\n2024-01-07\n-0.024500\n2024-01-03\n1.30\n-0.01\n\n\n7\n2024-01-08\n0.046251\n2024-01-08\n1.35\n0.03\n\n\n8\n2024-01-09\n-0.013361\n2024-01-08\n1.35\n0.03\n\n\n9\n2024-01-10\n0.024632\n2024-01-08\n1.35\n0.03\n\n\n\n\n\n\n\nNotice how the earnings data ‚Äúfills forward‚Äù:\n\nJan 1-2 use the Dec 28 announcement\nJan 3-7 use the Jan 3 announcement\nJan 8+ use the Jan 8 announcement\n\nThis is exactly what you want for event studies: tag each return with the most recent earnings information available at that time.\n\n\n\n\n\n\nImportantAsof Joins Require Sorted Data\n\n\n\nBoth datasets must be sorted by the time column. Pandas doesn‚Äôt always check this, and unsorted data produces wrong results silently. Always sort explicitly before asof joins.\n\n\n\n\n16.3.3 Asof Join Directions\nThe direction parameter controls which timestamp to match:\n\ndirection='backward' (default): Use most recent prior value (as of semantics)\ndirection='forward': Use next future value (rare in finance)\ndirection='nearest': Use closest value in either direction (dangerous for event studies‚Äîcan introduce look-ahead bias)\n\n\n# Forward-looking (CAREFUL: creates look-ahead bias)\nforward = pd.merge_asof(\n    returns,\n    earnings,\n    left_on='date',\n    right_on='announce_date',\n    by='ticker',\n    direction='forward'  # Use next announcement\n)\n\n# Nearest (AVOID for event studies)\nnearest = pd.merge_asof(\n    returns,\n    earnings,\n    left_on='date',\n    right_on='announce_date',\n    by='ticker',\n    direction='nearest'  # Use closest announcement\n)\n\n\n\n\n\n\n\nWarningBeware Look-Ahead Bias\n\n\n\nUsing direction='forward' or direction='nearest' can introduce look-ahead bias: your analysis uses information that didn‚Äôt exist at the time. For example, tagging a January return with a February earnings announcement. This inflates backtested strategy returns and invalidates empirical tests. Stick with direction='backward' unless you have a specific reason to do otherwise.\n\n\n\n\n16.3.4 Asof Joins in Polars\nPolars provides asof joins through the join_asof method. The syntax is cleaner and performance is often better for large datasets.\n\n# Same data in Polars\nreturns_pl = pl.DataFrame({\n    'ticker': ['AAPL'] * 10,\n    'date': pl.date_range(\n        date(2024, 1, 1),\n        date(2024, 1, 10),\n        interval='1d',\n        eager=True\n    ),\n    'return': np.random.randn(10) * 0.02\n})\n\nearnings_pl = pl.DataFrame({\n    'ticker': ['AAPL', 'AAPL', 'AAPL'],\n    'announce_date': [\n        date(2023, 12, 28),\n        date(2024, 1, 3),\n        date(2024, 1, 8)\n    ],\n    'eps': [1.25, 1.30, 1.35],\n    'surprise': [0.02, -0.01, 0.03]\n})\n\n# Asof join in Polars\nmerged_pl = returns_pl.sort('date').join_asof(\n    earnings_pl.sort('announce_date'),\n    left_on='date',\n    right_on='announce_date',\n    by='ticker',\n    strategy='backward'  # Equivalent to pandas direction='backward'\n)\n\nmerged_pl.select(['date', 'return', 'announce_date', 'eps', 'surprise'])\n\n/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_65713/4247637574.py:25: UserWarning: Sortedness of columns cannot be checked when 'by' groups provided\n  merged_pl = returns_pl.sort('date').join_asof(\n\n\n\nshape: (10, 5)\n\n\n\ndate\nreturn\nannounce_date\neps\nsurprise\n\n\ndate\nf64\ndate\nf64\nf64\n\n\n\n\n2024-01-01\n0.01453\n2023-12-28\n1.25\n0.02\n\n\n2024-01-02\n0.001239\n2023-12-28\n1.25\n0.02\n\n\n2024-01-03\n0.016048\n2024-01-03\n1.3\n-0.01\n\n\n2024-01-04\n-0.02854\n2024-01-03\n1.3\n-0.01\n\n\n2024-01-05\n0.01889\n2024-01-03\n1.3\n-0.01\n\n\n2024-01-06\n-0.005555\n2024-01-03\n1.3\n-0.01\n\n\n2024-01-07\n0.045201\n2024-01-03\n1.3\n-0.01\n\n\n2024-01-08\n-0.029402\n2024-01-08\n1.35\n0.03\n\n\n2024-01-09\n-0.021957\n2024-01-08\n1.35\n0.03\n\n\n2024-01-10\n0.026204\n2024-01-08\n1.35\n0.03\n\n\n\n\n\n\nPolars also supports tolerance (maximum time difference) and handles missing values more explicitly:\n\n# Only match if announcement is within 90 days\nmerged_pl = returns_pl.sort('date').join_asof(\n    earnings_pl.sort('announce_date'),\n    left_on='date',\n    right_on='announce_date',\n    by='ticker',\n    strategy='backward',\n    tolerance='90d'  # NaN if no announcement within 90 days\n)\n\n\n\n16.3.5 Common Asof Join Patterns in Finance\nPattern 1: Point-in-time accounting data\nMerge daily returns with quarterly accounting variables. Each return should use the most recent financial statement available at that date.\n\n# Daily returns\nreturns = pd.DataFrame({\n    'permno': [10107] * 100,\n    'date': pd.date_range('2024-01-01', periods=100, freq='D'),\n    'ret': np.random.randn(100) * 0.02\n})\n\n# Quarterly accounting (fiscal quarter end dates)\naccounting = pd.DataFrame({\n    'permno': [10107, 10107, 10107, 10107],\n    'datadate': pd.to_datetime(['2023-09-30', '2023-12-31',\n                                '2024-03-31', '2024-06-30']),\n    'book_equity': [1000, 1100, 1150, 1200],\n    'total_assets': [5000, 5200, 5300, 5400]\n})\n\n# Asof join: each return uses most recent accounting data\nmerged = pd.merge_asof(\n    returns.sort_values('date'),\n    accounting.sort_values('datadate'),\n    left_on='date',\n    right_on='datadate',\n    by='permno',\n    direction='backward'\n)\n\nPattern 2: Analyst forecast updates\nAnalyst forecasts arrive irregularly. Tag each return with the current consensus forecast.\n\n# Daily returns\nreturns = pd.DataFrame({\n    'ticker': ['AAPL'] * 30,\n    'date': pd.date_range('2024-01-01', periods=30, freq='D'),\n    'ret': np.random.randn(30) * 0.02\n})\n\n# Analyst forecast updates (irregular)\nforecasts = pd.DataFrame({\n    'ticker': ['AAPL'] * 5,\n    'forecast_date': pd.to_datetime([\n        '2023-12-15', '2024-01-05', '2024-01-12',\n        '2024-01-20', '2024-01-28'\n    ]),\n    'consensus_eps': [5.25, 5.30, 5.28, 5.35, 5.40],\n    'num_analysts': [25, 26, 27, 28, 29]\n})\n\n# Asof join: current forecast as of each date\nmerged = pd.merge_asof(\n    returns.sort_values('date'),\n    forecasts.sort_values('forecast_date'),\n    left_on='date',\n    right_on='forecast_date',\n    by='ticker',\n    direction='backward'\n)\n\nPattern 3: Bid-ask spreads and trades\nMatch each trade to the prevailing bid-ask spread (within milliseconds).\n\n# Trades (exact timestamps)\ntrades = pd.DataFrame({\n    'symbol': ['AAPL'] * 5,\n    'trade_time': pd.to_datetime([\n        '2024-01-02 09:30:00.123',\n        '2024-01-02 09:30:00.456',\n        '2024-01-02 09:30:01.789',\n        '2024-01-02 09:30:02.012',\n        '2024-01-02 09:30:03.345'\n    ]),\n    'trade_price': [150.25, 150.26, 150.24, 150.25, 150.27],\n    'trade_size': [100, 200, 150, 300, 250]\n})\n\n# Quote updates (irregular, microsecond timestamps)\nquotes = pd.DataFrame({\n    'symbol': ['AAPL'] * 4,\n    'quote_time': pd.to_datetime([\n        '2024-01-02 09:30:00.100',\n        '2024-01-02 09:30:00.500',\n        '2024-01-02 09:30:02.000',\n        '2024-01-02 09:30:03.000'\n    ]),\n    'bid': [150.24, 150.25, 150.23, 150.26],\n    'ask': [150.26, 150.27, 150.25, 150.28]\n})\n\n# Asof join: prevailing quote at each trade\nmerged = pd.merge_asof(\n    trades.sort_values('trade_time'),\n    quotes.sort_values('quote_time'),\n    left_on='trade_time',\n    right_on='quote_time',\n    by='symbol',\n    direction='backward',\n    tolerance=pd.Timedelta('1s'),  # Only match within 1 second\n    allow_exact_matches=False  # Use quote *before* the trade, not at same instant\n)\n\n# Calculate effective spread\nmerged['spread'] = merged['ask'] - merged['bid']\nmerged['effective_spread'] = abs(merged['trade_price'] -\n                                  (merged['bid'] + merged['ask']) / 2)",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Joins and Merges</span>"
    ]
  },
  {
    "objectID": "data-analysis/joins/index.html#diagnosing-and-correcting-merge-errors",
    "href": "data-analysis/joins/index.html#diagnosing-and-correcting-merge-errors",
    "title": "16¬† Joins and Merges",
    "section": "16.4 Diagnosing and Correcting Merge Errors",
    "text": "16.4 Diagnosing and Correcting Merge Errors\nMerges fail silently. You run the code, get a DataFrame back, and assume it worked. But maybe 80% of rows didn‚Äôt match. Maybe you have duplicate keys creating a many-to-many join. Maybe your date formats don‚Äôt align. The code runs, but your results are garbage.\nRun the following checks every time until they become automatic.\n\n16.4.1 Essential checks before and after merging\nBefore merging:\n\nPredict row counts: Inner join should give ‚â§ min(left, right) rows; left join should preserve left row count for one-to-one or one-to-many; outer join gives ‚â• max(left, right) rows\nCheck for duplicate keys: Use df.duplicated(key_cols).sum() or validate parameter to enforce cardinality\nVerify data types match: Use df[key_cols].dtypes to ensure join keys have compatible types\nFor string keys: Normalize with .str.strip().str.upper() to handle whitespace and case differences\nFor dates: Ensure both use datetime64, same time zone, and same granularity\n\nAfter merging:\n\nCheck row count changes: Unexpected changes indicate duplicate keys or wrong join type\nCheck match rates: Use indicator=True to see how many rows matched (use merged['_merge'].value_counts())\nInspect missing values: NaN in right-side columns after left join indicates failed matches\n\n\n\n16.4.2 Using the indicator parameter\nThe indicator=True parameter adds a _merge column showing match status:\n\n# Re-create sample data for merge diagnostics\nreturns = pd.DataFrame({\n    'ticker': ['AAPL', 'MSFT', 'TSLA'],\n    'year': [2023, 2023, 2023],\n    'return': [0.45, 0.35, 0.10]\n})\n\naccounting = pd.DataFrame({\n    'ticker': ['AAPL', 'TSLA', 'GOOGL'],\n    'year': [2023, 2023, 2023],\n    'roa': [0.20, 0.05, 0.15],\n    'leverage': [0.30, 0.10, 0.05]\n})\n\n# Left join with indicator\nmerged = returns.merge(\n    accounting,\n    on=['ticker', 'year'],\n    how='left',\n1    indicator=True\n)\n\n# Check match rates\nmerged['_merge'].value_counts()\n\n\n1\n\nAdds a _merge column showing whether each row matched: left_only, right_only, or both.\n\n\n\n\n_merge\nboth          2\nleft_only     1\nright_only    0\nName: count, dtype: int64\n\n\n\nprint(f\"Match rate: {(merged['_merge'] == 'both').mean():.1%}\")\n\nMatch rate: 66.7%\n\n\n\n\n\n\n\n\nImportantInvestigate low match rates\n\n\n\nA match rate below 90% usually indicates problems: incorrect join keys, different identifier schemes (CUSIP vs PERMNO), date format mismatches, inconsistent capitalization, or datasets filtered to different subsets. Don‚Äôt just accept low match rates‚Äîunderstand why they occurred.\n\n\n\n\n16.4.3 Common merge error patterns\n\n\n\n\n\n\n\n\n\nError\nSymptom\nCause\nSolution\n\n\n\n\nCartesian explosion\nRow count explodes (10,000x+ more rows)\nDuplicate keys in both datasets\nAggregate one dataset or use more specific join keys\n\n\nZero matches\nAll rows are left_only\nKeys don‚Äôt overlap at all\nCheck identifier mapping (CUSIP vs PERMNO), date formats\n\n\nType mismatch\nLow or zero matches despite overlapping keys\nDifferent data types (int vs float, str vs object)\nConvert both to same type before merge\n\n\nFloat precision\nValues that should match don‚Äôt\n150.25 vs 150.25000001\nRound floats or use integer keys\n\n\nTime zone mismatch\nAsof join produces all NaN\nOne dataset in UTC, other in local time\nConvert both to same time zone\n\n\nCase/whitespace\nLow matches on string keys\n‚ÄúAAPL‚Äù vs ‚Äúaapl‚Äù vs ‚Äù AAPL‚Äù\nUse .str.strip().str.upper()\n\n\nUnsorted asof join\nWrong matches or NaN\nData not sorted by time column\nAlways sort_values() before asof joins",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Joins and Merges</span>"
    ]
  },
  {
    "objectID": "data-analysis/joins/index.html#summary-and-best-practices",
    "href": "data-analysis/joins/index.html#summary-and-best-practices",
    "title": "16¬† Joins and Merges",
    "section": "16.5 Summary and Best Practices",
    "text": "16.5 Summary and Best Practices\nMerging is where most data errors occur in empirical finance. Follow these practices to catch problems early:\n\nUnderstand cardinality before merging. Is this one-to-one, one-to-many, or many-to-many? Use validate to enforce expectations.\nChoose the right merge type. Inner for complete cases only, left to preserve your sample, outer rarely (and carefully).\nUse asof joins for time-series data. Don‚Äôt try to match timestamps exactly‚Äîuse asof joins with direction='backward' to avoid look-ahead bias.\nValidate every merge. Check row counts, match rates, duplicates, and data types until these checks become automatic.\nSort before asof joins. Unsorted data produces silent errors.\nDocument match rates. Report how many observations matched in your analysis. Low match rates indicate problems.\nNormalize strings. Strip whitespace, standardize case, remove special characters before merging on text.\nUse indicator columns. Add indicator=True to understand what matched and what didn‚Äôt.\nPrefer left joins for clarity. Write base.merge(extra, how='left') instead of extra.merge(base, how='right').\nConsider Polars for large datasets. Polars‚Äô lazy evaluation and better error messages can save time on big merges.\n\nThe datasets in empirical finance are messy. Companies change tickers, databases use different identifiers, timestamps don‚Äôt align. Merging correctly requires skepticism, validation, and careful attention to detail. The techniques in this chapter won‚Äôt eliminate merge errors, but they‚Äôll help you catch them before they corrupt your research.",
    "crumbs": [
      "Working with Data",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Joins and Merges</span>"
    ]
  }
]