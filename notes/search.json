[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Empirical Finance",
    "section": "",
    "text": "About This Book\nThis book is aimed at students in the MSc in Finance program at HEC Montr√©al taking the course MATH 60230 Empirical Finance. I make it publicly available for anyone interested in learning Python for finance, but some examples and explanations are tailored to the HEC Montr√©al context.\nThe main objective of this book is to offer a thorough and accessible practical guide to empirical finance research using Python. To achieve this goal, I cover statistical and econometric techniques used in empirical finance, with a focus on practical applications using Python. I believe that learning by doing is the most effective way to master new skills. Thus, I present real-world scenarios and datasets, enabling you to see the power and efficacy of these techniques in action.\nNo prior programming experience is required. If you are already familiar with Python, I still recommend you at least skim Part I because I propose not only an introduction to Python, but an introduction to a modern workflow for data analysis with Python.\nMy goal is that by the end of this book, you will have an advanced understanding of the main econometric techniques used in empirical finance and a solid grounding in modern Python programming for data analysis. I look forward to guiding you through this exciting journey into the world of empirical finance, statistics, econometrics, and Python programming.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Empirical Finance",
    "section": "",
    "text": "Tip\n\n\n\nThis book is also available as a PDF file.\n\n\n\n\n\n\n\n\nNoteWork in Progress\n\n\n\nThis book is a work in progress. I am constantly adding new content and refining existing content. If you have any suggestions or feedback, please reach out at vincent.gregoire@hec.ca.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "Empirical Finance",
    "section": "Structure",
    "text": "Structure\nThis book is organized into seven parts plus appendices, each designed to build upon the knowledge from the previous section, ultimately guiding you to a robust understanding of empirical finance using Python.\nPart I: Python Fundamentals, Environment, and Best Practices\nThe first part of the book is devoted to familiarizing you with Python and setting up the necessary coding environment. We begin with instructions on installing Python and understanding its basic syntax. We then introduce various tools that are part of the programmer‚Äôs toolbox, including the terminal, VS Code, Git, and GitHub. You will learn about managing Python environments with uv, writing clean and well-documented code, testing your code, and object-oriented programming concepts.\nPart II: Working with Data\nIn the second part, we focus on data manipulation and analysis. You will learn how to load data from various sources and formats, work with DataFrames using pandas and Polars, clean and transform data, structure datasets for analysis, merge multiple data sources, and reshape data between wide and long formats.\nPart III: Visualization and Research Output\nThe third part covers how to communicate your findings effectively. We explore data visualization with matplotlib and seaborn, creating publication-quality tables for regression results and summary statistics, and using Quarto to create reproducible research documents that combine code, text, and results.\nPart IV: Statistical Foundations\nIn the fourth part, we dive into the statistical foundations needed for empirical finance. We cover numerical computing with NumPy, probability distributions and random number generation, and descriptive statistics for financial data.\nPart V: Regression Methods\nThe fifth part focuses on econometric techniques central to empirical finance research. We cover linear regression with statsmodels, panel data methods for working with firm-time observations, and instrumental variables estimation for addressing endogeneity.\nPart VI: Machine Learning and Artificial Intelligence in Research\nThe sixth part introduces modern AI and machine learning tools relevant to finance research. We discuss how to use AI assistants effectively for coding and research, machine learning techniques for prediction and classification, and natural language processing methods for analyzing textual data.\nPart VII: Reproducibility and Replication\nThe final part addresses the critical importance of reproducibility in research. We cover best practices for organizing research projects, managing dependencies, and ensuring that your results can be replicated by others.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#learning-approach",
    "href": "index.html#learning-approach",
    "title": "Empirical Finance",
    "section": "Learning Approach",
    "text": "Learning Approach\nThe learning approach adopted in this book is designed to be practical and closely linked with the real-world challenges encountered in empirical finance. My philosophy is grounded in the belief that the best way to learn is by doing, especially when it comes to mastering complex concepts like econometrics and programming.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#youtube-video-tutorials",
    "href": "index.html#youtube-video-tutorials",
    "title": "Empirical Finance",
    "section": "YouTube Video Tutorials",
    "text": "YouTube Video Tutorials\nThroughout the book, I provide links to YouTube videos that offer alternative explanations of the concepts covered in the chapters. These videos are not meant to replace the book, but rather to provide additional perspectives and clarifications. Some of these videos are created by me and are available on my YouTube channel, Vincent Codes Finance.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#tech-stack",
    "href": "index.html#tech-stack",
    "title": "Empirical Finance",
    "section": "Tech Stack",
    "text": "Tech Stack\nThis book is structured around a tech stack formed by a specific set of tools that has been carefully chosen based on their wide adoption, robustness, versatility, and compatibility with each other. While alternative tools exist and may be equally capable, the book takes an opinionated approach, focusing on this particular stack for clarity and consistency. It‚Äôs worth noting that the concepts and techniques covered in this book can be applied with other tools as well, but the specific examples and code use the following:\n\nPython 3.14 (released in October 2025)\nuv for managing Python versions and environments\nVisual Studio Code for writing code\nGit and GitHub for version control and collaboration\nClaude, ChatGPT, and Microsoft Copilot\nClaude Code, OpenAI Codex, and GitHub Copilot for coding assistance\nQuarto for writing technical content",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#use-of-ai",
    "href": "index.html#use-of-ai",
    "title": "Empirical Finance",
    "section": "Use of AI",
    "text": "Use of AI\nThis book was written with substantial assistance from AI tools, primarily ChatGPT and Claude Code. AI was used for all aspects of the book‚Äôs creation, including idea generation, creating outlines, drafting content, proofreading, and generating code examples. This reflects the modern reality of software development and technical writing, where AI assistants have become valuable collaborators.\nHowever, all content has been reviewed and edited by a human. I take full responsibility for the accuracy and quality of the material presented. Any errors or omissions remain my responsibility.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Empirical Finance",
    "section": "About the Author",
    "text": "About the Author\nI‚Äôm Vincent Gr√©goire, CFA, a Professor of Finance at HEC Montr√©al and the Canada Research Chair in Finance and Technology. I teach empirical finance with a strong emphasis on Python-based data analysis. I earned a Ph.D.¬†in Finance from the University of British Columbia, along with degrees in Computer Engineering and Financial Engineering from Universit√© Laval, and previously served as Chief Data Scientist at Berkindale Analytics, a fintech startup.\nMy work focuses on how information is produced, processed, and priced in financial markets. I study market structure through the lens of big data, machine learning, algorithmic trading, and cybersecurity, with an emphasis on methods that actually scale outside toy examples.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Empirical Finance",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI am grateful to Charles Martineau and Saad Ali Khan for their feedback and suggestions on the book.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "python/index.html",
    "href": "python/index.html",
    "title": "Python",
    "section": "",
    "text": "What is Python?\nThis part introduces Python, an open-source, high-level programming language that has become indispensable for empirical finance. I aim to provide readers with a foundational understanding of Python‚Äôs capabilities and how to leverage it for financial research. Starting from the basics, we will explore why Python is the go-to tool for researchers in finance.\nPython is a versatile and user-friendly programming language that emphasizes readability and simplicity. Its design philosophy promotes code that is easy to write and understand, making it an excellent choice for both beginners and experienced programmers. The name Python also refers to the interpreter, the program that runs Python code.",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "python/index.html#why-python-for-finance",
    "href": "python/index.html#why-python-for-finance",
    "title": "Python",
    "section": "Why Python for Finance?",
    "text": "Why Python for Finance?\nPython has been widely adopted by the finance industry and finance researchers for several compelling reasons:\nOpen source\nPython is free and open-source. Yes, that means you can use it for free, but open-source is much more than that. Python is distributed under the Python Software Foundation License, which is a permissive license that allows you to use, modify, and distribute the code and derived works based on it. This has led to a vibrant ecosystem of libraries and tools that are freely available to all, and to private forks of Python that are used internally by large financial institutions know collectively as bank Python.\nEase of learning\nPython‚Äôs syntax is designed to be intuitive and human-readable, making it an accessible language for beginners and experts alike. This simplicity allows finance professionals‚Äîmany of whom may not have a computer science background‚Äîto quickly learn Python and apply it to their work. Python code often reads like plain English, reducing the learning curve and enabling users to focus on problem-solving rather than struggling with the syntax.\nFor academic researchers, this ease of learning means that Python can be introduced in undergraduate or graduate programs with minimal friction. Students can rapidly transition from learning the basics of the language to applying it in real-world financial scenarios, such as data analysis, statistical modeling, and portfolio optimization.\nPowerful\nPython is exceptionally powerful due to its extensive library ecosystem. Libraries like NumPy, SciPy, and statsmodels provide robust tools for numerical and statistical computations, while pandas and polars facilitate data manipulation and analysis. These capabilities make Python an ideal choice for tasks ranging from simple data cleaning to complex econometric modeling.\nIn addition to its computational capabilities, Python integrates seamlessly with other programming languages and platforms, enabling finance practitioners to incorporate Python into larger, multi-language workflows. For example, it can call high-performance code written in C++ or Rust, interact with databases through SQL, or interface with scentific languages like R or Julia. This power and flexibility ensure that Python remains suitable for both small-scale analyses and enterprise-level financial systems.\nVersatile\nPython‚Äôs versatility allows it to handle a wide range of tasks, making it a one-stop solution for financial workflows. Analysts can use Python for tasks such as data acquisition from APIs or through web scraping, performing statistical analyses, creating visualizations, and even building predictive models using machine learning libraries like scikit-learn.\nWidely-used\nIn 2024, Python surpassed Javascript to become the most widely used programming language in the world according to GitHub‚Äôs Octoverse. This rise in use is attributed to the growing importance of AI and data science, for which Python is the most popular language.\nThis popularity ensures that Python skills are highly transferable and in demand, making it a valuable asset for finance professionals. Large financial institutions, such as JPMorgan Chase and Goldman Sachs, use Python extensively for data analysis, trading algorithms, and risk modeling. Financial data providers such as LSEG (formerly Refinitiv and Thomson Reuters) and WRDS, an academinc data provider, offer Python-based platforms for accessing and analyzing financial data. Python skills are now a must-have for finance professionals, so much so that the CFA Institute has added Python practical skills to its 2024 curriculum.\n\nOther languages\nPython is not the only game in town. R and Stata offer better capabilities for econometric and statistical modeling, and are also widely used in academic research. Julia and Matlab offer better performance for numerical computing, and C++ and Rust are the languages of choice for performance-critical parts of financial applications. Finally, SAS and many database softwares use a variant of SQL, while some use their own proprietary language like q for kdb+ which is a staple of high-frequency trading firms. Finally, OCaml is the language of choice at Jane Street, a large quantitative trading firm. Overall, each language has its strengths and weaknesses, and the choice of language depends on the specific task at hand, but Python is a very good choice for most tasks.",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "python/index.html#components-of-the-python-ecosystem",
    "href": "python/index.html#components-of-the-python-ecosystem",
    "title": "Python",
    "section": "Components of the Python Ecosystem",
    "text": "Components of the Python Ecosystem\nThe Python ecosystem consists of various tools and components that make it a powerful platform for data analysis. This section introduces the key elements of the ecosystem and their roles in creating efficient workflows.\n\nPython interpreter\nPython is an interpreted language, meaning that your code is executed by an interpreter when you run it rather than compiled ahead of time to an executable. The Python interpreter is responsiable for executing your Python code. It comes in various implementations, with the most common being CPython, the default implementation distributed with official Python releases, which is the one we will use in this book. Other variants include PyPy, a just-in-time (JIT) compiled interpreter that enhances performance for specific tasks, and Pyodide, a port of CPython to WebAssembly that allows Python to run in web browsers.\n\n\nPython libraries\nThe base Python language is simple, but it is extended through a large ecosystem of libraries. Python comes with a large standard library, that includes many features such as file input/output, basic data structures, and mathematical functions. However, most Python programs will leverage additional libraries. These libraries are pre-written modules that extend Python‚Äôs functionality. For empirical finance, some key libraries include:\n\npandas and polars: For data manipulation and analysis.\nNumPy and SciPy: For numerical computations.\nmatplotlib and seaborn: For data visualization.\nstatsmodels and linearmodels: For econometric modeling.\nscikit-learn: For machine learning.\n\nThese libraries form the backbone of financial analysis in Python, enabling everything from basic calculations to complex statistical modeling.\n\n\n\n\n\n\nWarning\n\n\n\nPython libraries are published on PyPI, the Python Package Index. Anyone can publish a library to PyPI, so it is important to check the library‚Äôs reputation before using it. Always keep in mind that librairies contain code that will be executed on your computer, so they can contain malware. Well-known libraries are less likely to contain vulnerabilities, but, as with any unreviewed software, there is always a risk. We will discuss security best practices in more detail in future chapters.\n\n\n\n\nEnvironment and package management\nPython versions are updated frequently.1 Libraries follow their own release cycles and are not always updated at the same time as the Python interpreter. Some libraries depend on specific versions of other libraries, so a chain of dependencies may need to be updated. To complicate matters, updates to libraries may break your code. This means that code that worked last month may not work this month if you always use the latest version of everything.\nTo minimize these issues, the best practice is to create a virtual environment for each project. This ensures that the versions of the Python interpreter and libraries are fixed and consistent for each project, and that you can easily update the libraries for a project without affecting your other projects.\nThere are several tools for managing Python environments. Python comes with venv, a built-in module for creating virtual environments, and pip, a package manager for installing and updating libraries. A popular alternative for data science projects is conda, part of the Anaconda distribution.2 Another popular tool is poetry. Conda and poetry act as both environment and package manager.\nIn this book, we will use uv, a tool that replaces venv, pip, and a suite of other tools. It brings a lot of modern features to the table, such as a lockfile to ensure reproducibility, ability to install and manage Python versions, and more. For me, its main advantage is its increadible speed which is in part due to an advanced caching mechanism, the use of hardlinks to avoid keeping multiple copies of each library, and a very fast dependancy resolver.3\n\n\nIntegrated Development Environments (IDEs)\nIDEs streamline coding by providing features such as syntax highlighting, debugging tools, and other tools to make your life easier. In this book, we will use Visual Studio Code (VS Code), an open-source code editor by Microsoft that is very popular in the data science community. It is not Python-specific, but it integrates seamlessly with Python through extensions. Because it is open-source, many forks have been created, such as Cursor, which adds a powerful AI engine to the editor, and Positron, a data science-oriented fork by Posit, the company behind RStudio and Quarto (still in beta at the time of writing).\nOther popular IDEs include PyCharm by JetBrains (commercial, but free for students and academics) and neovim, a terminal-based text editor with a steep learning curve that is popular among developers for its extensibility. Finally, Spyder, is an open-source IDE that was very popular in the Python scientific community, but has since been eclipsed by VS Code.\n\n\nNotebooks\nNotebooks, such as Jupyter Notebooks, are interactive environments where code, text, and visualizations can coexist. They have significant drawbacks for their use in robust, replicable research, but are nonetheless very popular in data science because of their simplicity. VS Code supports Jupyter notebooks natively with an extension. We will notebooks them in more details in this book, along with their shortcomings and ways to mitigate them. marimo is a new Python notebook interface that aims to address some of the issues with Jupyter Notebooks. While it has a long way to go to overtake Jupyter in the data science community, it shows a lot of promise.",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "python/index.html#footnotes",
    "href": "python/index.html#footnotes",
    "title": "Python",
    "section": "",
    "text": "The latest version at the time of writing is 3.14. Since 2018, Python releases have been annual in October.‚Ü©Ô∏é\nWhile conda is open-source, it uses by default the Anaconda Repository, which requires a paid subscription under some conditions. At the time of writing, it is free for academic use.‚Ü©Ô∏é\nThe main task of a package manager is to resolve dependencies, i.e.¬†to figure out which versions of libraries need to be installed to satisfy the dependencies of a project. This is a very complex task (NP-hard) that requires a lot of computation and heuristics.‚Ü©Ô∏é",
    "crumbs": [
      "Python"
    ]
  },
  {
    "objectID": "python/install/index.html",
    "href": "python/install/index.html",
    "title": "1¬† Installing Python",
    "section": "",
    "text": "1.1 What you need for a complete Python environment\nIn this chapter, I cover installing Python 3.14 and the related tools for a complete coding environment.\nThe most common way to use Python is to install it locally on your computer. The instructions below will guide you through the process of installing the following tools:\nWe will also install the following tools that are not required to run Python code, but are useful when working on projects with code:",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#what-you-need-for-a-complete-python-environment",
    "href": "python/install/index.html#what-you-need-for-a-complete-python-environment",
    "title": "1¬† Installing Python",
    "section": "",
    "text": "uv: A package manager for Python. I use it to manage the external libraries used in projects. uv makes it easy to install and update libraries on a per-project basis, and to make sure all collaborators use the same library version.\nPython: The Python interpreter, which allows you to run Python code. We will install multiple versions using uv.\nVisual Studio Code: Visual Studio Code is a free source code editor made by Microsoft. Features include support for debugging, syntax highlighting and intelligent code completion. Users can install extensions that add additional functionality.\n\n\n\nGit and GitHub: I use Git to manage my code and GitHub to host my code online and collaborate with others. Git is a version control system that tracks code changes and keeps a full history of changes. GitHub is a website that hosts Git repositories and provides additional features for collaboration such as issue tracking and pull requests.\n\n\n\n\n\n\n\nNoteuv vs Poetry and Anaconda\n\n\n\n\n\nMost Python projects use external libraries. For example, we use the pandas library for data analysis. To manage these libraries, we need a package manager. I now recommend using uv instead of Poetry (also very good) and Anaconda. Anaconda was my package manager of choice for many years and it remains very popular, but like many I eventually switched to poetry and more recently to uv. Overall, uv brings many nice features, but the two main reasons why I settled on uv is that it is very fast and it lets you easily install and use multiple Python versions. While speed might seem a minor concern for a package manager, anyone who has spent minutes (plural) waiting for Anaconda to create a virtual environment and install all the dependencies will understand.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#installation",
    "href": "python/install/index.html#installation",
    "title": "1¬† Installing Python",
    "section": "1.2 Installation",
    "text": "1.2 Installation\n\n macOS Linux Windows\n\n\nuv\nThe simplest way to install uv on macOS is with their install script.1 First, you need to open the Terminal app. You can find it in the Applications/Utilities folder, or by using Spotlight (press Cmd+Space and type Terminal). Then run the following script:2\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nAlternatively, you can also install it using Homebrew3:\nbrew install uv\nTo check that uv is properly installed, run the following command in the Terminal app:\nuv --version\nVisual Studio Code\nDownload Visual Studio Code from code.visualstudio.com.\nGit and GitHub\nGit might already be installed on your Mac as a command-line tool if you have installed the Xcode tools. If not, you can get the official installer. You can also use Git directly in VS Code, or using a GUI client such as GitHub Desktop. I prefer to use the VS Code integration or the command-line tool, but many beginners prefer to use GitHub Desktop.\n\n\nTo be honest, if you‚Äôre using Linux, you probably already know how to install Python and other tools. The instructions below are for manual installation, but you probably want to use your distribution‚Äôs package manager instead.\nuv\nInstallation instructions can be found here.\nVisual Studio Code\nDownload Visual Studio Code from code.visualstudio.com.\nGit and GitHub\nGit is probably already installed on Linux as a command-line tool. You can also use Git directly in VS Code, or using a GUI client such as GitHub Desktop. I prefer to use the VS Code integration or the command-line tool, but many beginners prefer to use GitHub Desktop.\n\n\nuv\nThe simplest way to install uv on Windows is with their install script.4 First, you need to open Powershell. Then, run the following script:\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\nAlternatively, you can also install it using WinGet5:\nwinget install --id=astral-sh.uv  -e\nTo check that uv is properly installed, run the following command in Powershell (you may have to close and re-open Powershell):\nuv --version\nNote: If this does not work but you had a successful installation messsage, you may have to restart your computer for the uv command to be available. You may have to manually add the path to uv in your environment variables. Do to so, you first need to figure out where uv was installed on your computer. It will tell you after installing, but the default is C:\\Users\\YOURUSERNAME\\.local\\bin. Once you have that path, add it to enviroment variables (Control panel-&gt;Edit Environment variables).\nVisual Studio Code\nDownload Visual Studio Code from code.visualstudio.com.\nGit and GitHub\nTo use Git on Windows, you need to install the Git client, which is a command-line tool.\nYou can also use Git directly in VS Code, or using a GUI client such as GitHub Desktop, but you need to first install the Git client. I prefer to use the VS Code integration or the command-line tool, but many beginners prefer to use GitHub Desktop.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#creating-a-sandbox-environment",
    "href": "python/install/index.html#creating-a-sandbox-environment",
    "title": "1¬† Installing Python",
    "section": "1.3 Creating a sandbox environment",
    "text": "1.3 Creating a sandbox environment\nThe recommended way to work with environments in Python is to have unique enviromnents for each project. However, not everything is a project, so I like to have a ‚Äúsandbox‚Äù environment with all the libraries I use regularly. That way, when I want to try something quickly like reading a CSV file to explore it, I have this sandbox project ready to go. It used to be common to install these libraries in the default (or base) environment, but it can lead to issues when updating packages, so many Python distribution now lock the default environment to prevent you from installing packages.\nFor my sandbox environment, I will want at least the following libraries:\n\npandas: Data analysis library\nnumpy: Numerical computing library\nscipy: Scientific computing library\nmatplotlib: Plotting library\nseaborn: Plotting library\nstatsmodels: Statistical models\nscikit-learn: Machine learning library\nlinearmodels: Linear models for Python\npyarrow: Library for working with parquet files\njupyter: for Jupyter notebooks and the VS Code Python interactive window\npytest: Testing framework\n\nTo create this sandbox environment, I will use uv. First, I need to create a new directory for the environment. I will call it sandbox, but you can name name it whatever you want.6 Then, I need to create a new project in this directory:\n\n macOS Linux Windows\n\n\nmkdir ~/Documents/sandbox\ncd ~/Documents/sandbox\nuv init\n\n\nmkdir ~/sandbox\ncd ~/sandbox\nuv init\n\n\nFirst create a folder named sandbox where you want on your computer. Then, from Windows Explorer, open the folder in PowerShell using File-&gt;Open Windows PowerShell. You can then initialize your environment using uv:\nuv init\n\n\n\nThis creates a pyproject.toml file in the sandbox directory. This file contains the list of dependencies for the project (which will be empty for now).\nOnce the project is created, you can add the dependencies:\nuv add pandas numpy scipy matplotlib seaborn statsmodels scikit-learn linearmodels pyarrow jupyter pytest\nThis step updates the pyproject.toml file and creates a uv.lock file, which contains the exact version of each dependency. This file is used to make sure that all collaborators use the same version of each library. Note that because our dependencies are built on top of other libraries, uv will also install the dependencies of our dependencies.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#configuring-visual-studio-code",
    "href": "python/install/index.html#configuring-visual-studio-code",
    "title": "1¬† Installing Python",
    "section": "1.4 Configuring Visual Studio Code",
    "text": "1.4 Configuring Visual Studio Code\nVisual Studio Code is a free source code editor made by Microsoft. Features include support for debugging, syntax highlighting and intelligent code completion. While there are some built-in features for Python, most of the functionality comes from extensions. What I recommend is to use the profile feature of VS Code, which lets you define a set of extensions for each use case. For example, you can have a profile for Python development, another for R development, and another for LaTeX editing. This way, you can have a clean installation of VS Code and only install the extensions you need for each profile. Furthermore, each profile can have its specific settings and theming options.\nTo create a profile, click on the profile icon in the bottom left corner of the VS Code window. Then, under the Profiles section, click on Create Profile.\n\n\n\n\n\nGive the profile a name and select a distinctive icon. Make sure to copy from the Data Science template, which will install all the extensions you need for data analysis with Python.\n\n\n\n\n\nVS Code works best when you have a project (directory) open. To open a project, select Open Folder from the File menu and select the folder you want to open, for example, the sandbox folder we created earlier.\nTo open an interactive window, bring up the command palette by pressing Cmd+Shift+P (or Ctrl+Shift+P on Windows and Linux) and type Python: Create Interactive Window.\nAt this point, VS Code should have detected the virtual environment created by uv and should have asked you if you want to use it. If not, you can select it manually by clicking on the Python version in the top right corner of the interactive window.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#github.com-optional",
    "href": "python/install/index.html#github.com-optional",
    "title": "1¬† Installing Python",
    "section": "1.5 GitHub.com (optional)",
    "text": "1.5 GitHub.com (optional)\nYou do not need a GitHub account to have a complete Python environment. However, I recommend creating one because it will be useful later when we start working on projects.\nTo follow the some examples, you will need a GitHub account. You can create one for free at https://github.com/. GitHub offers many benefits to students and educators, including free access to GitHub Copilot and extra free hours for GitHub Codespaces. I highly recommend applying at GitHub Education if you are eligible.\nWhile GitHub is the leader in the space, GitLab is their main competitor offering similar features. Gitea is a fully open-source solution for those who prefer to self-host.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#github-codespaces",
    "href": "python/install/index.html#github-codespaces",
    "title": "1¬† Installing Python",
    "section": "1.6 Python in the cloud using Github Codespaces",
    "text": "1.6 Python in the cloud using Github Codespaces\nMany online platforms allow you to develop and run Python code without installing anything on your computer. If you want to use a cloud-based solution, I recommend using GitHub Codespaces.\nAll you need is a GitHub account. However, note that GitHub Codespaces is not free. At the time of this writing, you get 60 hours per month for free, or 90 hours if you signed up for the GitHub Student Developer Pack (this is for a 2-core machine, which is the smallest machine available). After that, you have to pay for it (the current rate is USD 0.18 per hour).\nMake sure to shut down your Codespace when you are not using it, otherwise you will run out of free hours very quickly.\n\n1.6.1 Other cloud alternatives\nThere are many other cloud-based alternatives. However, most are based on Jupyter notebooks, which can be interesting when you are learning Python, but are not ideal for robust, replicable research. Some of the most popular alternatives are:\n\nGoogle Colab\nCocalc\nWRDS Jupyter Hub (requires a WRDS subscription through your institution)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#whats-next",
    "href": "python/install/index.html#whats-next",
    "title": "1¬† Installing Python",
    "section": "1.7 What‚Äôs next?",
    "text": "1.7 What‚Äôs next?\nNow that you have a complete Python environment, you can start learning Python. The next chapter introduces the basic Python syntax.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/install/index.html#footnotes",
    "href": "python/install/index.html#footnotes",
    "title": "1¬† Installing Python",
    "section": "",
    "text": "See the uv website for more details and troubleshooting advice.‚Ü©Ô∏é\ncurl is a program that will download the script, and |, the pipe operator, will take the result (the downloaded script) and pass it as input to sh, which will execute the script.‚Ü©Ô∏é\nHomebrew is a package manager for macOS that allows you to install and update software from the command line. It simplifies the installation process and makes it easy to keep your software up-to-date.‚Ü©Ô∏é\nSee the uv website for more details and troubleshooting advice.‚Ü©Ô∏é\nWinGet is a package manager for Windows that allows you to install and update software from the command line. It simplifies the installation process and makes it easy to keep your software up-to-date.‚Ü©Ô∏é\nI avoid spaces and special characters as they can sometimes cause trouble.‚Ü©Ô∏é",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html",
    "href": "python/python-basics/index.html",
    "title": "2¬† Python Syntax",
    "section": "",
    "text": "2.1 Introduction\nIn this chapter, we lay the foundation for your programming skills by exploring the basic syntax of Python 3.14.\nMy aim is to make this process as accessible as possible for non-programmers while giving you the necessary tools to excel in the world of empirical finance research.\nThe objectives of this chapter are to:\nBy the end of this chapter, you will have a solid grasp of Python‚Äôs basic syntax, empowering you to use it as a versatile tool for finance-related tasks. Remember, the key to success in learning any programming language is practice. As you work through this tutorial, be sure to experiment with the examples provided and try writing your own code to reinforce your understanding.\nI am purposefully leaving out of this chapter more advanced topics such as object-oriented programming, testing, and logging. These topics are important, but they are not necessary to get started with Python, so will come later in the book.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#introduction",
    "href": "python/python-basics/index.html#introduction",
    "title": "2¬† Python Syntax",
    "section": "",
    "text": "Provide a gentle introduction to the basic syntax of Python, allowing you to read and understand Python code.\nEnable you to write simple programs that will serve as building blocks for more advanced applications.\nEquip you with the knowledge and confidence to further explore advanced topics in Python and its applications in finance.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#data-types",
    "href": "python/python-basics/index.html#data-types",
    "title": "2¬† Python Syntax",
    "section": "2.2 Data types",
    "text": "2.2 Data types\nThe Python language offers many built-in fundamental data types. These data types serve as the building blocks for working with different kinds of data, which is critical in many applications. The basic data types you should be familiar with are presented in Table¬†2.1.\n\n\n\nTable¬†2.1: Main data types in Python\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nExample\n\n\n\n\nInteger\nint\nIntegers represent whole positive and negative numbers. They are used for counting, indexing, and various arithmetic operations.\n1\n\n\nFloat-Point Number\nfloat\nFloats represent real numbers with decimals. They are used for working with financial data that require precision, such as interest rates, stock prices, and percentages.\n1.0\n\n\nComplex\ncomplex\nComplex numbers consist of real and imaginary parts, represented as a + bj. While less commonly used in finance, they may be relevant in specific advanced applications, such as signal processing or quantitative finance.\n1.0 + 2.0j\n\n\nBoolean\nbool\nBooleans represent the truth values of True and False. They are used in conditional statements, comparisons, and other logical operations.\nTrue\n\n\nText String\nstr\nStrings are sequences of characters used for storing and manipulating text data, such as stock symbols, company names, or descriptions.\n\"Hello\"\n\n\nBytes\nbytes\nBytes are sequences of integers in the range of 0-255, often used for representing binary data or encoding text. Bytes may be used when working with binary file formats or network communication.\nb\"Hello\"\n\n\nNone\nNone\nNone is a special data type representing the absence of a value or a null value. It is used to signify that a variable has not been assigned a value or that a function returns no value.\nNone\n\n\n\n\n\n\n\n2.2.1 Literals\nA literal is a notation for representing a fixed value in source code. For example, 42 is a literal for the integer value of forty-two. The following are examples of literals in Python. Each code block contains code and is followed by the output of the code.\n\n2.2.1.1 int\nint literals are written as positive and negative whole numbers.\n\n42\n\n42\n\n\n\n-99\n\n-99\n\n\nThey can also include underscores to make them more readable.\n\n1_000_000\n\n1000000\n\n\n\n\n2.2.1.2 float\nfloat literals are written as decimal numbers.\n\n2.25\n\n2.25\n\n\nThey can be written in scientific notation by using e to indicate the exponent.\n\n2.25e8\n\n225000000.0\n\n\nTo define a whole number literal as a float instead of an int, you can append a decimal point to the number.\n\n2.0\n\n2.0\n\n\n\n\n2.2.1.3 complex\nComplex numbers consist of a real part and an imaginary part, represented as a + bj.\n\n2.3 + 4.5j\n\n(2.3+4.5j)\n\n\n\n\n2.2.1.4 None\nNone is a special data type that represents the absence of a value or a null value. It is used to signify that a variable has not been assigned a value or that a function returns no value.\n\nNone\n\n\n\n2.2.1.5 bool\nbool is a data type that represents the truth values of True and False. They are used in conditional statements, comparisons, and other logical operations.\n\nTrue\n\nTrue\n\n\n\n\n\n2.2.2 str\nStrings are sequences of characters. Strings literals are written by enclosing a sequence of characters in single or double quotes. Note that double quotes are preferred by the ruff formatter, which is used in this book, but most Python environments will use single quotes by default when displaying strings.\n\n\"USD\"\n\n'USD'\n\n\nStrings are sequences of Unicode characters, which means they can represent any character in any language, including emojis.\n\n\"Bitcoin  üöÄ\"\n\n'Bitcoin  üöÄ'\n\n\nString literals can span multiple lines by enclosing them in triple quotes or triple double quotes. This is useful for writing multiline strings.\n\n# Multiline strings\n\n\"\"\"GAFA is a group of companies:\n\n- Google\n- Apple\n- Facebook\n- Amazon\n\n\"\"\"\n\n'GAFA is a group of companies:\\n\\n- Google\\n- Apple\\n- Facebook\\n- Amazon\\n\\n'\n\n\nMultiline strings, or any strings with special characters, can be displayed using the print function.\n\nprint(\n    \"\"\"GAFA is a group of companies:\n\n- Google\n- Apple\n- Facebook\n- Amazon\n\n\"\"\"\n)\n\nGAFA is a group of companies:\n\n- Google\n- Apple\n- Facebook\n- Amazon\n\n\n\n\n\n\n2.2.3 bytes\nbytes are sequences of integers in the range of 0-255. They are often used for representing binary data or encoding text. Bytes literals are written by prepending a string literal with b.\n\nb\"Hello\"\n\nb'Hello'\n\n\n\n\n\n\n\n\nCautionBytes vs strings\n\n\n\nBytes can be confused with strings, but they are not the same. Strings are sequences of Unicode characters, while bytes are sequences of integers in the range of 0-255. Bytes are often used for representing binary data or encoding text. In most cases, you will be working with strings, but you may encounter bytes when working with binary file formats or network communication.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#variables",
    "href": "python/python-basics/index.html#variables",
    "title": "2¬† Python Syntax",
    "section": "2.3 Variables",
    "text": "2.3 Variables\nA variable in Python is a named location in the computer‚Äôs memory that holds a value. It serves as a container for data, allowing you to reference and manipulate the data stored within it. Variables are created by assigning a value to a name using the assignment operator (=). They can store data of various types, such as integers, floats, strings, or even more complex data structures like lists.\nUnderstanding the concept of variables and their naming conventions will help you write clean, readable, and maintainable code. An overview of variable naming rules in Python is presented in Table¬†2.2, and Table¬†2.3 presents some examples of valid and invalid variable names.\n\n\n\nTable¬†2.2: Variable naming rules\n\n\n\n\n\n\n\n\n\nRule\nDescription\n\n\n\n\nCan contain letters, numbers, and underscores\nVariable names can include any combination of letters (both uppercase and lowercase), numbers, and underscores (_). Python variable names support Unicode characters, enabling you to use non-English characters in your variable names. However, they must follow the other rules mentioned below.\n\n\nCannot start with a number\nAlthough variable names can contain numbers, they must not begin with a number. For example, 1_stock is an invalid variable name, whereas stock_1 is valid.\n\n\nCannot be a reserved word\nPython has a set of reserved words (e.g., if, else, while) that have special meanings within the language. You should not use these words as variable names.\n\n\n\n\n\n\n\n\n\nTable¬†2.3: Variable naming examples\n\n\n\n\n\nValid\nInvalid\n\n\n\n\nticker\n1ceo\n\n\nfirm_size\n@price\n\n\ntotal_sum_2023\nclass\n\n\n_tmp_buffer\nfor\n\n\n\n\n\n\n\n\n\n\n\n\nCautionCase-sensitive\n\n\n\nPython is case-sensitive, so ret and RET are two different variables.\n\n\nBeyond the rules mentioned above, there are also some conventions that you should follow when naming variables. These conventions are not enforced by Python, but they are widely adopted by the Python community. Table¬†2.4 summarizes the most common conventions.\n\n\n\nTable¬†2.4: Variable naming conventions\n\n\n\n\n\n\n\n\n\nConvention\nDescription\n\n\n\n\nUse lowercase letters and underscores for variable names\nTo enhance code readability, use lowercase letters for variable names and separate words with underscores. For example, market_cap is a recommended variable name, whereas MarketCap or marketCap are not. This naming convention is known as snake case.\n\n\nUse uppercase letters for constants\nConstants are values that do not change throughout the program. Use uppercase letters and separate words with underscores to differentiate them from regular variables. For example, INFLATION_TARGET is a suitable constant name. Note that Python does not support constants like other languages, so this is just a convention, but Python won‚Äôt stop you from changing the value of a constant.\n\n\n\n\n\n\nBy adhering to these guidelines, you will improve your coding style and ensure that your code is easier to understand, maintain, and collaborate on with your peers.\n\n\n\n\n\n\nTipReserved keywords\n\n\n\nReserved keywords cannot be used as variable names. You can check the complete list of reserved keywords by running the following command in the Python console:\n\nhelp(\"keywords\")\n\n\nHere is a list of the Python keywords.  Enter any keyword to get more help.\n\nFalse               class               from                or\nNone                continue            global              pass\nTrue                def                 if                  raise\nand                 del                 import              return\nas                  elif                in                  try\nassert              else                is                  while\nasync               except              lambda              with\nawait               finally             nonlocal            yield\nbreak               for                 not                 \n\n\n\nNote that some reserved keywords may be confusing when thinking about finance problems. For example, return, yield, raise, global, class, and lambda are all reserved keywords, so you cannot use them as variable names. Most modern IDEs, such as Visual Studio Code, will highlight reserved keywords in a different color to help you avoid using them as variable names.\n\n\n\n2.3.1 Declaring variables\nA simple way to think about variables is to consider them labels that you can use to refer to values. For example, you can create a variable x and assign it a value of 42 using the assignment operator (=). You can then use the variable x to refer to the value 42 in your code.\n\nx = 42\nx\n\n42\n\n\n\n\n\n\n\n\nNoteThe walrus operator\n\n\n\nIn the previous example, we added x to the last line of the code to display the value of x. This is necessary in the interactive window and in Jupyer Notebooks, as they automatically display the result of the last line of the code. However, the assignment operator (=) does not return a value, so the value of x is not displayed without that last line.\n\nx = 42\n\nIntroduced in Python 3.8, the := operator, also known as the walrus operator, allows you to assign a value to a variable and return that value in a single expression. For example, you can use the walrus operator to assign a value of 10 to a variable z and use that variable in the same expression, assigning the result to y.\n\ny = (z := 10) * 2\n\nNote, however, that the walrus operator cannot be used to assign a value to a variable without using it in an expression. For example, the following code will raise an error.\n\nx := 42\n\n\n  Cell In[19], line 1\n    x := 42\n      ^\nSyntaxError: invalid syntax\n\n\n\n\n\n\nYou can reassign the value of a variable by assigning a new value to it. Once you reassign the value of a variable, the old value is lost. For example, you can reassign the value of x to 32 by running the following code.\n\nx = 32\nx\n\n32\n\n\nYou can perform operations on variables, just like you would on values. For example, you can add 10 to x.\n\nx + 10\n\n42\n\n\nYou can assign the result of an operation to a new variable. For example, you can assign the result of 2 * 10 to a new variable y.\n\ny = 2 * x\ny\n\n64\n\n\n\nz = x + y\nz\n\n96\n\n\nIf you try to use a variable name that is invalid, Python will raise an error. For example, if you try to assign a variable 1ceo, Python will raise an error because variable names cannot start with a number.\n\n1ceo = 2\n\n\n  Cell In[24], line 1\n    1ceo = 2\n    ^\nSyntaxError: invalid decimal literal\n\n\n\n\nYou can, however, use Unicode characters in variable names. For example, you can use accents such as √© in a variable name.\n\ncote_de_cr√©dit = \"AAA\"\n\nA leading underscore in a variable name indicates that the variable is private, which means that it should not be accessed outside of the module or scope in which it is defined. For example, you can use a leading underscore in a variable name to indicate that the variable is private. This is a convention that is widely adopted by the Python community, but it is not enforced by Python.\n\n_hidden = 30_000\n\nAnother convention is to use all caps for constants. For example, you can use all caps to indicate that INFLATION_TARGET is a constant.\n\nINFLATION_TARGET = 0.02\n\nPython will raise an error if you attempt to use a variable that has not been declared. For instance, if you try to use the variable inflation_target instead of INFLATION_TARGET, Python will generate an error. It‚Äôs important to note that Python is case-sensitive, so variables must be referenced with the exact casing as their declaration.\n\ninflation_target\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[28], line 1\n----&gt; 1 inflation_target\n\nNameError: name 'inflation_target' is not defined\n\n\n\n\n\n\n2.3.2 Variable types\nPython is a dynamically typed language, meaning you do not need to specify the variable type when you declare it. Instead, Python will automatically infer the type of a variable based on the value you assign to it. For example, if you assign an integer value to a variable, Python will infer that the variable is an integer. Similarly, if you assign a string value to a variable, Python will infer that the variable is a string. You can use the type() function to check the type of a variable. For example, you can check the type of a by running the following code.\n\na = 3.3\ntype(a)\n\nfloat\n\n\n\nb = 2\ntype(b)\n\nint\n\n\n\nmarket_open = True\ntype(market_open)\n\nbool\n\n\n\ncurrency = \"CAD\"\ntype(currency)\n\nstr\n\n\n\n\n\n\n\n\nTipVariables explorer in Visual Studio Code\n\n\n\nVS Code has a built-in variable explorer that allows you to view the variables in your workspace when using the interactive window or a Jupyer Notebook. You can open the Variables View by clicking on the Variables button in the top toolbar of the editor:\n\n\n\nVariable View button\n\n\nThe Variables View will appear at the bottom of the window, showing the variables in your workspace, along with their values, types, and size for collections. For example, the following screenshot shows the variables in the workspace after running the code in this section:\n\n\n\nVariable View\n\n\n\n\n\n2.3.2.1 Converting between types\nYou can convert a variable from one type to another using the built-in functions float(), int(), str(), and bool(). For example, you can convert the variable x, which is currently an int, to a float by running the following code.\n\nfloat(x)\n\n32.0\n\n\nThe same way, you can convert the variable y, which is currently a float, to an int by running the following code. Note that the int() function will round down the value of y to the nearest integer.\n\nint(a)\n\n3\n\n\nSimilarly, you can convert the variable x to a string by running the following code.\n\nstr(x)\n\n'32'\n\n\nYou can convert a string to an integer or a float if the string contains a valid representation of a number. For example, you can convert the string \"42\" to an integer by running the following code.\n\nint('42')\n\n42\n\n\nHowever, you cannot convert a string that does not contain a valid representation of a number to an integer. For example, you cannot convert the string \"42.5\" to an integer.\n\nint('42.5')\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[37], line 1\n----&gt; 1 int('42.5')\n\nValueError: invalid literal for int() with base 10: '42.5'\n\n\n\n\nWhen converting to a boolean value, most values will be converted to True, except for 0, 0.0, and \"\", which will be converted to False.\n\nbool(0)\n\nFalse\n\n\n\nbool(1)\n\nTrue\n\n\n\nbool(\"\")\n\nFalse\n\n\n\nbool(\"33\")\n\nTrue\n\n\nThe None value is a special type in Python that represents the absence of a value. You can use the None value to initialize a variable without assigning it a value. For example, you can initialize a variable problem to None by running the following code.\n\nproblem = None\ntype(problem)\n\nNoneType",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#comments",
    "href": "python/python-basics/index.html#comments",
    "title": "2¬† Python Syntax",
    "section": "2.4 Comments",
    "text": "2.4 Comments\nComments are an essential part of writing clear, maintainable code. They help explain the purpose, logic, or any specific details of the code that might not be obvious at first glance. However, excessive or unnecessary commenting can clutter your code and make it harder to read. To strike the right balance, consider the guidelines listed in Table¬†2.5 when deciding when to use comments and when to avoid them:\n\n\n\nTable¬†2.5: Guidelines for comments\n\n\n\n\n\n\n\n\n\nGuideline\nDescription\n\n\n\n\nUse comments when the code is complex or non-obvious\nWhen your code involves complex algorithms, calculations, or logic that may be difficult for others (or yourself) to understand at a glance, use comments to explain the reasoning behind the code or to provide additional context.\n\n\nAvoid comments for simple or self-explanatory code\nFor code that is simple, clear, and easy to understand, avoid adding comments. Instead, use descriptive variable and function names that convey the purpose of the code.\n\n\nUse comments to explain the ‚Äòwhy‚Äô, not the ‚Äòhow‚Äô\nGood comments explain the purpose of a piece of code or the reasoning behind a decision. Focus on providing context and insight that isn‚Äôt immediately apparent from reading the code. Avoid repeating what the code is doing, as this can be redundant and clutter the code.\n\n\nAvoid commenting out large blocks of code\nInstead of leaving large blocks of commented-out code in your final version, remove them. It‚Äôs better to use version control systems like Git to keep track of previous versions of your code.\n\n\nKeep comments up-to-date\nEnsure that your comments are always up-to-date with the code they describe. Outdated comments can be confusing and misleading, making it harder to understand the code.\n\n\nUse comments to provide additional information\nUse comments to provide references to external resources, such as links to relevant documentation, papers, or articles. This can be helpful for providing additional context or background information related to the code.\n\n\nUse consistent commenting style\nFollow a consistent commenting style throughout your codebase. This makes it easier for others to read and understand your comments.\n\n\n\n\n\n\n\n2.4.1 Writing comments\nIn Python, comments are created using the # symbol. Any text that follows the # symbol on the same line is ignored by the Python interpreter. Comments can be placed on a separate line or at the end of a line of code.:\n\n# This is a single-line comment\n\nprice = 150  # This is an inline comment\n\nYou can also create multi-line comments by enclosing the text in triple quotes (\"\"\" or '''). Multi-line comments are often used to provide docstrings (documentation strings) for functions and classes. We‚Äôll learn more about functions and classes in a later section. Note that multi-line comments are technically strings, but the Python interpreter ignores them and does not store them in memory because they are not assigned to a variable.\n\n\"\"\"\nThis is a multi-line comment.\nYou can write your comments across multiple lines.\n\"\"\"\n\n'\\nThis is a multi-line comment.\\nYou can write your comments across multiple lines.\\n'\n\n\nComments can occur alongside code to document its purpose or explain the logic.\n\n# Calculate compound interest\nprincipal = 1000  # Principal amount\nrate = 0.05  # Annual interest rate\ntime = 5  # Time in years\n\n# Future value with compound interest formula\nfuture_value = principal * (1 + rate) ** time\n\n# Display the result\nprint(f\"Future value: {future_value:.2f}\")\n\nFuture value: 1276.28\n\n\n\n\n\n\n\n\nTipDon‚Äôt overdo it\n\n\n\nComments are useful for providing additional context or explanation, but they can also be overdone. Avoid adding comments for trivial or self-explanatory code. For example, the code above is simple and clear enough to understand without comments, so adding comments is decreasing readability instead of improving it.\n\n\nComments are usually written in English, but you can use any language as long as the file is UTF-8 encoded. You can also use emojis in comments if you like üòä.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#numbers",
    "href": "python/python-basics/index.html#numbers",
    "title": "2¬† Python Syntax",
    "section": "2.5 Numbers",
    "text": "2.5 Numbers\nPython provides built-in functions and operators to perform mathematical operations on numbers. Some commonly used mathematical functions include abs(), round(), min(), max(), and pow(). Additionally, Python‚Äôs math library offers more advanced functions like trigonometry and logarithms.\n\n\n\n\n\n\nWarningRounding errors\n\n\n\nFloating-point numbers may be subject to rounding errors due to the limitations of their binary representation. Keep this in mind when comparing or performing calculations with floats. Consider using the Decimal data type from Python‚Äôs decimal library to avoid floating-point inaccuracies when dealing with high-precision financial data.\n\n\n\n\n\n\n\n\nNotePerformance\n\n\n\nWhen working with large datasets or performing complex calculations, consider using third-party libraries like NumPy and pandas, which are covered in later chapters, for improved performance and additional functionality.\n\n\n\n2.5.1 Operations\nThe Python language supports many mathematical operations. Table¬†2.6 lists some of the most commonly used operators.\n\n\n\nTable¬†2.6: Basic Arithmetic Operations\n\n\n\n\n\nOperator\nName\nExample\nResult\n\n\n\n\n+\nAddition\n1 + 2\n3\n\n\n-\nSubtraction\n1 - 2\n-1\n\n\n*\nMultiplication\n3 * 4\n12\n\n\n/\nDivision\n1 / 2\n0.5\n\n\n**\nExponentiation\n2 ** 3\n8\n\n\n//\nFloor division\n14 // 3\n4\n\n\n%\nModulo (remainder)\n14 % 3\n2\n\n\n\n\n\n\n\na = 5\nb = 3\n\nprint(f\"Addition: a + b = {a + b}\")\nprint(f\"Subtraction: a - b = {a - b}\")\nprint(f\"Multiplication: a * b = {a * b}\")\nprint(f\"Division: a / b = {a / b}\")\nprint(f\"Exponentiation: a ** b = {a ** b}\")\nprint(f\"Floor Division: a // b = {a // b}\")\nprint(f\"Modulo: a % b = {a % b}\")\n\nAddition: a + b = 8\nSubtraction: a - b = 2\nMultiplication: a * b = 15\nDivision: a / b = 1.6666666666666667\nExponentiation: a ** b = 125\nFloor Division: a // b = 1\nModulo: a % b = 2\n\n\n\n\n\n\n\n\nNotef-strings\n\n\n\nThe previous examples use a special type of strings called f-strings to format the output. f-strings are a convenient way to embed variables and expressions inside strings. They are denoted by the f prefix and curly braces ({}) containing the variable or expression to be evaluated.\nWe cover f-strings in more details in Section 2.7.2.\n\n\n\n\n2.5.2 Common mathematical functions\nTo round numbers, use the round() function. The round() function takes two arguments: the number to be rounded and the number of decimal places to round to. The number is rounded to the nearest integer if the second argument is omitted.\n\nrounded_num = round(5.67, 1)\n\nprint(rounded_num)\nprint(type(rounded_num))\n\n\nrounded_to_int = round(5.67)\n\nprint(rounded_to_int)\nprint(type(rounded_to_int))\n\n5.7\n&lt;class 'float'&gt;\n6\n&lt;class 'int'&gt;\n\n\nSome mathematical functions will require the use of the math module from the Python Standard Library. The standard library is a collection of modules included with every Python installation. You can use the functions and types in these modules by importing them into your code using the import statement.\nFor example, to calculate the square root of a number, you can use the sqrt() function from the math module:\n\n1import math\n\nmath.sqrt(25)\n\n\n1\n\nImports the math module, making its functions available in the current code.\n\n\n\n\n5.0\n\n\nThis is only one of the many functions in the math module. You can view the complete list of functions in the module documentation. The math module also contains constants like pi and e, which you can access using the dot notation.\n\nmath.pi\n\n3.141592653589793\n\n\n\n\n2.5.3 Random numbers\nIt is often useful to generate random numbers for simulations and other applications. Python‚Äôs random module provides functions for generating pseudo-random1 numbers from different distributions.\n\n\n\n\n\n\nImportantPseudo-random number generator\n\n\n\nThe random module uses the Mersenne Twister algorithm to generate pseudo-random numbers. This algorithm is deterministic, meaning that given the same seed value, it will produce the same sequence of numbers every time. This is useful for debugging and testing but not for security purposes. If you need a cryptographically secure random number generator, use the secrets module instead.\n\n\nThe random.seed() function initializes the pseudo-random number generator. If you do not call this function, Python will automatically call it the first time you generate a random number. The random.seed() function takes an optional argument that can be used to set the seed value. This can be useful for debugging and testing, allowing you to generate the same sequence of random numbers every time. If you do not specify a seed, Python will use the system time as the seed value, so you will get a different sequence of random numbers every time.\n\nimport random\n\n1random.seed(42)\n\n\n1\n\nSets the seed value to 42. Why 42? Because it‚Äôs the answer to life, the universe, and everything.\n\n\n\n\nrandom.random() generates a random float between 0 and 1 (exclusive).\n\nrand_num = random.random()\n\nrand_num\n\n0.6394267984578837\n\n\nrandom.randint(a, b) generates a random integer between a and b (inclusive).\n\nrand_int = random.randint(1, 10)\n\nrand_int\n\n1\n\n\nrandom.uniform(a, b) generates a random float between a and b (exclusive).\n\nrand_float = random.uniform(0, 1)\n\nrand_float\n\n0.7415504997598329\n\n\nrandom.normalvariate(mu, sigma) generates a random float from a normal distribution with mean mu and standard deviation sigma.\n\nrand_norm = random.normalvariate(0, 1)\n\nrand_norm\n\n-0.508616386057752\n\n\nThe full list of functions in the random module can be found in the module documentation.\n\n\n2.5.4 Floats and decimals\nBecause of the way computers store numbers, floating-point numbers are not exact. This can lead to unexpected results when performing arithmetic operations on floats.\n\n2.33 + 4.44\n\n6.7700000000000005\n\n\nTo avoid this problem when exact results are needed, use the Decimal type from the decimal module to perform arithmetic operations on decimal numbers. You could import the module using import decimal but this would require you to prefix all the functions and types in the module with decimal. To avoid this, you can directly import the Decimal type from the decimal module using from decimal import Decimal.\n\n1from decimal import Decimal\n\nDecimal(\"2.33\") + Decimal(\"4.44\")\n\n\n1\n\nImports the Decimal type from the decimal module. You can now refer to the Decimal type directly without having to prefix it with decimal.\n\n\n\n\nDecimal('6.77')\n\n\n\n\n\n\n\n\nNoteDecimals vs.¬†floats\n\n\n\nUsing the Decimal type in Python provides precise decimal arithmetic and avoids rounding errors, making it suitable for financial and monetary calculations, while floats offer faster computation and are more memory-efficient but can introduce small inaccuracies due to limited precision and binary representation.\n\n\n\n\n2.5.5 Financial formulas\nMany financial calculations involve performing arithmetic operations on financial data. Here are two examples of common calculations in finance and how they can be implemented in Python.\n\n2.5.5.1 Calculating the present value of a future cash flow\nThe formula for calculating the present value of a future cash flow is: \\[ PV = \\frac{FV_t}{(1 + r)^t}, \\] where \\(FV_t\\) is the future value of the cash flow at time \\(t\\), \\(r\\) is the discount rate, and \\(t\\) is the number of periods.\n\nfuture_value = 1000\ndiscount_rate = 0.05\nperiods = 5\n\npresent_value = future_value / (1 + discount_rate) ** periods\n\npresent_value\n\n783.5261664684588\n\n\n\n\n2.5.5.2 Calculating the future value of an annuity\nThe formula for calculating the future value of an annuity is: \\[ FV = PMT \\frac{(1 + r)^t - 1}{r}, \\] where \\(PMT\\) is the payment, \\(r\\) is the interest rate, and \\(t\\) is the number of periods.\nIt can be written in Python as:\n\npayment = 100\nrate = 0.05\nperiods = 5\n\nfuture_value_annuity = payment * ((1 + rate) ** periods - 1) / rate\n\nfuture_value_annuity\n\n552.5631250000007\n\n\n\n\n\n\n\n\nTipParentheses and operator precedence\n\n\n\nPython, just like mathematics, follows a specific order of operations when evaluating expressions. The complete list of precedence rules can be found in the Python documentation.\nWhen in doubt, use parentheses to make the order of operations explicit.\nFor arithmetic operations, the order of operations is as follows:\n\nExponents\nNegative (-)\nMultiplication and division\nAddition and subtraction",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#sec-defining-functions",
    "href": "python/python-basics/index.html#sec-defining-functions",
    "title": "2¬† Python Syntax",
    "section": "2.6 Defining functions",
    "text": "2.6 Defining functions\nFunctions are blocks of organized and reusable code that perform a specific action. They allow you to encapsulate a set of instructions, making your code modular and easier to maintain. Functions can take input parameters, perform operations on those inputs, and return a result.\nDefining a function in Python involves the following steps:\n\nUse the def keyword: Start by using the def keyword, followed by the function name and parentheses that enclose any input parameters.\nAdd input parameters: Specify any input parameters within the parentheses, separated by commas. These parameters allow you to pass values to the function, which it can then use in its calculations or operations.\nWrite the function body: After the parentheses, add a colon (:) and indent the following lines to create the function body. This block of code contains the instructions that the function will execute when called.\nReturn a result (optional): If your function produces a result, use the return statement to send the result back to the caller. If no return statement is specified, the function will return None by default.\n\n\n\n\n\n\n\nTipBest practices\n\n\n\nWhen defining functions, keep the following best practices in mind:\n\nChoose descriptive function names: Use meaningful names that reflect the purpose of the function, making your code more readable and easier to understand.\nKeep functions small and focused: Each function should have a single responsibility, making it easier to test, debug, and maintain.\n\n\n\nWe can define functions to perform a wide variety of tasks. For example, we can define a function to calculate the present value of a future cash flow:\n\n1def present_value(future_value, discount_rate, periods):\n2    return future_value / (1 + discount_rate) ** periods\n\n\n# Example usage:\nfuture_value = 1000\ndiscount_rate = 0.05\nperiods = 5\n3result = present_value(future_value, discount_rate, periods)\nprint(f\"Present value: {result:.2f}\")\n\n\n1\n\nDefines a function called present_value that takes three input parameters: future_value, discount_rate, and periods.\n\n2\n\nCalculates the present value of a future cash flow using the formula from the previous section and returns the result to the caller. The code in the function body is indented to indicate that it is part of the function.\n\n3\n\nCalls the present_value function with the specified input values and stores the returned value in a variable called result. When the function is called, the input values are passed to the function as arguments in the same order as the parameters were defined. The function body is then executed, and the result is returned to the caller.\n\n\n\n\nPresent value: 783.53\n\n\n\n\n\n\n\n\nNoteIndentation\n\n\n\nIndentation refers to the spaces or tabs used at the beginning of a line to organize code. It helps Python understand the program‚Äôs structure and which lines of code are grouped together.\nThe Python language specification mandates the use of consistent indentation for code readability and proper execution. Indentation is typically achieved using four spaces per level. It plays a crucial role in determining the scope and hierarchy of statements within control structures, such as loops and conditional statements. For example, the statements that are part of a function body must be indented to indicate that they are part of the function. The Python interpreter knows that the function body ends when the indentation level returns to the previous level.\n\n\nWe will learn more about functions in Section 2.11.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#sec-strings",
    "href": "python/python-basics/index.html#sec-strings",
    "title": "2¬† Python Syntax",
    "section": "2.7 Strings",
    "text": "2.7 Strings\nText data is often encountered in finance in the form of stock symbols, company names, descriptions, or financial reports. Understanding how to work with strings is essential for processing and manipulating text data effectively.\nStrings are sequences of characters, and they can be created using single quotes (' '), double quotes (\" \"), or triple quotes (''' ''' or \"\"\" \"\"\") for multi-line strings.\n\n\n\n\n\n\nNoteSpecial characters\n\n\n\nSome characters have special meanings in Python strings. The backslash (\\) is used to escape characters that have special meaning, such as newline (\\n) or tab (\\t). To include a backslash in a string, you need to escape it by adding another backslash before it (\\\\). Alternatively, you can use raw strings by prefixing the strings with r or R, which will treat backslashes as literal characters. For example, these two strings are equivalent:\nstr1 = \"C:\\\\Users\\\\John\"\nstr2 = r\"C:\\Users\\John\"\n\n\n\n2.7.1 String operations\nThe Python language provides many common string operations. Table¬†2.7 lists some of the most commonly used operations.\n\n\n\nTable¬†2.7: Common string operations\n\n\n\n\n\n\n\n\n\n\nOperation\nExample\nDescription\n\n\n\n\nConcatenate strings\nresult = str1 + \" \" + str2\nCombines two or more strings together\n\n\nRepeat strings\nresult = repeat_str * 3\nRepeats a string a specified number of times\n\n\nLength of a string\nlength = len(text)\nGets the length (number of characters) of a string\n\n\nAccess characters in a string\nfirst_char = text[0]\nRetrieves a specific character in a string\n\n\nSlice a string\nslice_text = text[0:12]\nExtracts a part of a string\n\n\nConvert case\nupper_text = text.upper()\nConverts a string to uppercase\n\n\n\nlower_text = text.lower()\nConverts a string to lowercase\n\n\nJoin a list of strings\ntext = \", \".join(companies)\nJoins a list of strings using a delimiter\n\n\nSplit a string\ncompanies = text.split(\", \")\nSplits a string into a list based on a delimiter\n\n\nReplace a substring\nnew_text = text.replace(\"Finance\", \"Python\")\nReplaces a specified substring in a string\n\n\nCheck substring existence\nresult = substring in text\nChecks if a substring exists in a string\n\n\n\n\n\n\nWe can concatenate (combine) two or more strings into a single string using the + operator.\n\nstr1 = \"Hello\"\nstr2 = \"World\"\nresult = str1 + \" \" + str2\nprint(result)\n\nHello World\n\n\nThe * operator repeats a string multiple times.\n\nrepeat_str = \"Python \"\nresult = repeat_str * 3\nprint(result)\n\nPython Python Python \n\n\nThe len() function returns the string‚Äôs length (number of characters).\n\ntext = \"Finance\"\nlength = len(text)\nprint(length)\n\n7\n\n\nSingle characters in a string can be accessed using the index of the character within square brackets ([]). Python uses zero-based indexing, so the first character in a string has index 0, the second character has index 1, and so on. You can also use negative indices to access characters from the end of a string, with the last character having index -1, the second last character having index -2, and so on.\n\ntext = \"Python\"\nfirst_char = text[0]\nlast_char = text[-1]\nprint(first_char)\nprint(last_char)\n\nP\nn\n\n\nExtracting a portion of a string by specifying a start and end index is called slicing. In Python, you can slice a string using the following syntax: text[start:end]. The start index is inclusive, while the end index is exclusive. If the start index is omitted, it defaults to 0. If the end index is omitted, it defaults to the length of the string.\n\ntext = \"empirical finance Python\"\nslice_text = text[0:7]\nprint(slice_text)\n\nempiric\n\n\nThe upper() and lower() methods convert a string to uppercase or lowercase, respectively.\n\ntext = \"Finance\"\nupper_text = text.upper()\nlower_text = text.lower()\nprint(upper_text)\nprint(lower_text)\n\nFINANCE\nfinance\n\n\n\n\n\n\n\n\nNoteMethods vs functions\n\n\n\nA method is similar to a function but associated with a specific object or data type. In this case, upper() and lower() are methods specific to the str (string) data type. When we call the upper method on the text object using the dot notation (text.upper()), Python knows to transform the string stored in the text variable. Methods are particularly useful because they allow us to perform actions or operations specific to the object or data type they belong to, and they improve code readability by making it clear what object the method is being called on.\n\n\nThe join() method joins a list of strings into a single string using a delimiter. The delimiter can be specified as an argument to the join() method. Lists are introduced in the next section.\n\ncompanies = [\"Apple\", \"Microsoft\", \"Google\"]\ntext = \" | \".join(companies)\nprint(text)\n\nApple | Microsoft | Google\n\n\nThe split() method splits a string into a list of substrings based on a delimiter. The delimiter can be specified as an argument to the split() method. If no delimiter is specified, the string is split on whitespace characters.\n\ntext = \"Apple, Microsoft, Google\"\ncompanies = text.split(\", \")\nprint(companies)\n\n['Apple', 'Microsoft', 'Google']\n\n\nThe replace() method replaces a substring in a string with another string. It takes two arguments: the substring to replace and the string to replace it with.\n\ntext = \"Introduction to Finance\"\nnew_text = text.replace(\"Finance\", \"Python\")\nprint(new_text)\n\nIntroduction to Python\n\n\nThe in operator checks if a substring exists in a string. It returns a boolean value, True if the substring exists in the string, and False otherwise.\n\ntext = \"Introduction to Python\"\nsubstring = \"Python\"\nresult = substring in text\nprint(result)\n\nTrue\n\n\nThe Python documentation provides a complete list of string methods that you can refer to for more details.\n\n\n2.7.2 Formatting strings\nYou will often encounter situations where you must present or display data in a formatted, human-readable manner. F-strings are a powerful tool for formatting strings and embedding expressions or variables directly within the string. They provide a concise and easy-to-read way of formatting strings, making them an essential tool for working with text data.\nF-strings, also known as ‚Äúformatted string literals,‚Äù allow you to embed expressions, variables, or even basic arithmetic directly into a string by enclosing them in curly braces {} within the string. The expressions inside the curly braces are evaluated at runtime and then formatted according to the specified format options.\nSome key features of f-strings that are useful include:\n\nExpression Evaluation: You can embed any valid Python expression within the curly braces, including variables, arithmetic operations, or function calls. This feature enables you to generate formatted strings based on your data dynamically.\nFormatting Options: F-strings support various formatting options, such as alignment, width, precision, and thousand separators. These options can be specified within the curly braces after the expression, separated by a colon (:).\nFormat Specifiers: You can use format specifiers to control the display of numbers, such as specifying the number of decimal places, using scientific notation, or adding a percentage sign. Format specifiers are especially useful in finance when working with currency, percentages, or large numbers.\n\nTo create an f-string, prefix the string with an f character, followed by single or double quotes. You can then embed expressions or variables within the string by enclosing them in curly braces ({}). For example, this lets you concatenate strings and variables together in a single statement:\n\nticker = \"AAPL\"\nexchange = \"NASDAQ\"\ncompany_name = \"Apple, Inc.\"\nfull_name = f\"{company_name} ({exchange}:{ticker})\"\nprint(full_name)\n\nApple, Inc. (NASDAQ:AAPL)\n\n\nPython will convert the expression within the curly braces to a string, which can be used to convert numbers to strings.\n\nnum = 42\nnum_str = f\"{num}\"\nprint(num_str)\n\n42\n\n\nPython evaluates the expression within the curly braces at runtime and then formats the string according to the specified format options. For example, you can use the :,.2f format option to display a number with a thousand separator and two decimal places.\n\namount = 12345.6789\nformatted_amount = f\"${amount:,.2f}\"\nprint(formatted_amount) \n\n$12,345.68\n\n\nYou can also use the :.2% format option to display a number as a percentage with two decimal places.\n\nrate = 0.05\nformatted_rate = f\"{rate:.2%}\"\nprint(formatted_rate) \n\n5.00%\n\n\nThe datetime module provides a datetime class to represent dates and times. The datetime class has a now() method that returns the current date and time. You can use the :%Y-%m-%d format option to display the date in YYYY-MM-DD format.\n\nfrom datetime import datetime\n\ncurrent_date = datetime.now()\nformatted_date = f\"{current_date:%Y-%m-%d}\"\nprint(formatted_date)\n\n2025-12-31\n\n\nYou can also use f-strings to align text to the left (&lt;), right (&gt;), or center (^) within a fixed-width column:\n\nticker = \"AAPL\"\nprice = 150.25\nchange = -1.25\n\n1formatted_string = f\"|{ticker:&lt;10}|{price:^10.2f}|{change:&gt;10.2f}|\"\nprint(formatted_string)\n\n\n1\n\nThe :&lt;10 format option aligns the text to the left within a 10-character column. The :^10.2f format option aligns the number to the center within a 10-character column and displays it with two decimal places. The :&gt;10.2f format option aligns the number to the right within a 10-character column and displays it with two decimal places.\n\n\n\n\n|AAPL      |  150.25  |     -1.25|\n\n\nMultiline f-strings work the same way as multiline strings, except that they are prefixed with an f character. You can use multiline f-strings to create formatted strings that span multiple lines.\n\nstock = \"AAPL\"\nprice = 150.25\nchange = -1.25\n\nformatted_string = f\"\"\"\nStock:  \\t{stock}\nPrice:  \\t${price:.2f}\nChange: \\t${change:.2f}\n\"\"\"\nprint(formatted_string)\n\n\nStock:      AAPL\nPrice:      $150.25\nChange:     $-1.25\n\n\n\n\n\n\n\n\n\nNoteAlternative formatting methods\n\n\n\nWhen reading code or answers on websites such as Stack Overflow or receiving suggestions from AI-assisted coding assistant, you may encounter other string formatting methods. Before f-strings, the two primary string formatting methods in Python were %-formatting and str.format().\n%-formatting\nAlso known as printf-style formatting, %-formatting uses the % operator to replace placeholders with values. Inspired by the printf function in C, it has been available since early versions of Python. It is less readable and more error-prone than other methods.\nExample:\nformatted_string = \"%s has a balance of $%.2f\" % (name, balance)\nstr.format()\nThe str.format() method embeds placeholders using curly braces {} and replaces them with the format() method. Introduced in Python 2.6, it offers improved readability and more advanced formatting options compared to %-formatting.\nExample:\nformatted_string = \"{} has a balance of ${:,.2f}\".format(name, balance)\nAdvantages of f-strings\nI recommend using f-strings instead of %-formatting or str.format() for string formatting for the following reasons:\n\nReadability: Concise syntax with expressions and variables embedded directly.\nFlexibility: Supports any valid Python expression within curly braces.\nPerformance: Faster than other methods, evaluated at runtime.\nSimplicity: No need to specify variable order or maintain separate lists.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#sec-collections",
    "href": "python/python-basics/index.html#sec-collections",
    "title": "2¬† Python Syntax",
    "section": "2.8 Collections",
    "text": "2.8 Collections\nSequences and collections are fundamental data structures in Python that allow you to store and manipulate multiple elements in an organized manner. They differ along three dimensions: order, mutability, and indexability. An ordered collection is one where the elements are stored in a particular order, the order of the elements is important, and you can iterate over the elements in that order. A collection is mutable if you can add, remove, or modify elements, after it is created. A collection is indexable if you can refer to its elements by their index (position or key).\nTable¬†2.8 presents the main types of sequences and collections in Python. You are already familiar with the string type, an ordered, immutable, and indexable sequence of characters.\n\n\n\nTable¬†2.8: Sequences and collections in Python\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nExample\n\n\n\n\nList\nlist\nOrdered, mutable, and indexed. Allows duplicate members.\n[1, 2, 3]\n\n\nTuple\ntuple\nOrdered, immutable, and indexed. Allows duplicate members.\n(1, 2, 3)\n\n\nSet\nset\nUnordered, mutable, and unindexed. No duplicate members.\n{1, 2, 3}\n\n\nDictionary\ndict\nUnordered, mutable, and indexed. No duplicate index entries. Elements are indexed according to a key.\n{\"a\": 1, \"b\": 4}\n\n\nString\nstring\nOrdered, immutable, and indexed. Allows duplicate characters.\n\"abc\"\n\n\n\n\n\n\n\n2.8.1 Lists\nLists in Python are ordered collections of items that can hold different data types. They are mutable, meaning that elements can be added, removed, or modified. Lists are versatile and commonly used to store and manipulate sets of related data. The elements within a list are accessed using indexes, which allow for easy retrieval and modification. Lists also support various built-in methods and operations for efficient data manipulation, such as appending, extending, sorting, and slicing.\nA list is created by enclosing a comma-separated sequence of elements within square brackets ([ ]). The elements can be of any data type, including other lists. The following code snippet creates a list of strings and a list of integers.\n\nstocks = [\"AAPL\", \"GOOG\", \"MSFT\"]\nprices = [150.25, 1200.50, 250.00]\n\nYou can access the elements of a list using their index. The index of the first element is 0, the index of the second element is 1, and so on. You can also use negative indexes to access elements from the end of the list. The index of the last element is -1, the index of the second to last element is -2, and so on. The following code snippet illustrates how to access the elements of the stocks and prices lists.\n\nfirst_stock = stocks[0]\nprint(first_stock)\n\nlast_price = prices[-1]\nprint(last_price)\n\nAAPL\n250.0\n\n\nYou can replace the elements of a list by assigning new values to their indexes, add new elements to the list using the append() method, or delete elements from the list using the remove() method.\n\n# Replace an element\nstocks[1] = \"GOOGL\"\nprint(stocks)\n\n# Adding an element to the list\nstocks.append(\"AMZN\")\nprint(stocks)\n\n# Removing an element from the list\nstocks.remove(\"MSFT\")\nprint(stocks)\n\n['AAPL', 'GOOGL', 'MSFT']\n['AAPL', 'GOOGL', 'MSFT', 'AMZN']\n['AAPL', 'GOOGL', 'AMZN']\n\n\nYou can also use the len() function to get the length of a list, and the in operator to check if an element is present in a list.\n\n# Length of the list\nlist_length = len(stocks)\nprint(f\"Length: {list_length}\")\n\n# Checking if an element is in the list\nis_present = \"AAPL\" in stocks\nprint(f\"Is AAPL in the list? {is_present}\")\n\nLength: 3\nIs AAPL in the list? True\n\n\n\n\n2.8.2 Tuples\nTuples are ordered collections of elements that are immutable, meaning they cannot be modified after creation. They are typically used to store related pieces of data as a single entity, and their immutability provides benefits such as ensuring data integrity and enabling safe data sharing across different parts of a program.\n\n\n\n\n\n\nNoteTuples vs lists\n\n\n\nTuples and lists in Python differ in mutability, syntax, and use cases. Tuples are commonly used for fixed data, have a slight performance advantage over lists and can be more memory-efficient. Lists are commonly used for variable data, and provide more flexibility in terms of operations and methods.\n\n\nA tuple is created by enclosing a comma-separated sequence of elements within parentheses (( )). The elements can be of any data type, including other tuples. The following code snippet creates a tuple of integers and a tuple of strings.\n\nmu = 0.1\nsigma = 0.2\ntheta = 0.5\n\nparameters = (mu, sigma, theta)\nprint(parameters)\n\n(0.1, 0.2, 0.5)\n\n\nYou can access the elements of a tuple using their index, find their length using len(), just like with lists.\n\n# Accessing elements in a tuple\nsigma0 = parameters[1]\nprint(sigma0)\n\n# Length of the tuple\ntuple_length = len(parameters)\nprint(f\"Length: {tuple_length}\")\n\n0.2\nLength: 3\n\n\nTuples are immutable, so you cannot add, remove, or replace their elements directly. You can, however, create a new tuple with the modified elements. Also note that you can modify mutable elements within a tuple, such as a list.\n\na = [1, 2, 3]\nb = (\"c\", a, 2)\nprint(f\"Before appending to list a: {b}\")\n\na.append(4)\nprint(f\"After appending to list a: {b}\")\n\nBefore appending to list a: ('c', [1, 2, 3], 2)\nAfter appending to list a: ('c', [1, 2, 3, 4], 2)\n\n\nb still contains the same elements, but the list a within the tuple has been modified.\n\n2.8.2.1 Tuple unpacking\nTuple unpacking is a powerful feature of Python that allows you to assign multiple variables from the elements of a tuple in a single line of code. It is a form of ‚Äúdestructuring assignment‚Äù that provides a concise way to extract the elements of a tuple into individual variables.\nTo perform tuple unpacking, you use a sequence of variables on the left side of an assignment statement, followed by a tuple on the right side. When the assignment is made, each variable on the left side will be assigned the corresponding value from the tuple on the right side.\nHere is an example:\n\n# Create a tuple\nt = (1, 2, 3)\n\n# Unpack the tuple into three variables\na, b, c = t\n\n# Display the values of the variables\nprint(f\"a: {a}, b: {b}, c: {c}\")\n\na: 1, b: 2, c: 3\n\n\nIn this example, the tuple t contains three elements: 1, 2, and 3. When the tuple is unpacked into the variables a, b, and c, each variable gets assigned the corresponding value from the tuple: a gets 1, b gets 2, and c gets 3.\nTuple unpacking can be useful in various situations. For example, when working with functions that return multiple values as a tuple, you can use tuple unpacking to assign the return values to individual variables. Here‚Äôs an example:\n\n# Define a function that returns a tuple\ndef get_top3_stocks():\n    return (\"AAPL\", \"MSFT\", \"AMZN\")\n\n# Unpack the returned tuple into three variables\nstock1, stock2, stock3 = get_top3_stocks()\n\n# Display the values of the variables\nprint(f\"Largest: {stock1}, 1nd: {stock2}, 3rd: {stock3}\")\n\nLargest: AAPL, 1nd: MSFT, 3rd: AMZN\n\n\nNote that the number of variables on the left side of the assignment must match the number of elements in the tuple being unpacked.\n\n\n\n2.8.3 Sets\nSets are unordered collections of unique elements. They are defined using curly braces { } or the set() constructor. Sets do not allow duplicate values and support various operations such as intersection, union, and difference. Sets are commonly used for tasks like removing duplicates from a list, membership testing, and mathematical operations on distinct elements.\n\n# Creating a set\nunique_numbers = {1, 2, 3, 2, 1}\nprint(unique_numbers)\n\n# Adding an element to the set\nunique_numbers.add(4)\nprint(f\"Added 4: {unique_numbers}\")\n\n# Removing an element from the set\nunique_numbers.remove(1)\nprint(f\"Removed 1: {unique_numbers}\")\n\n# Checking if an element is in the set\nis_present = 2 in unique_numbers\nprint(f\"Is 2 in the set? {is_present}\")\n\n# Length of the set\nset_length = len(unique_numbers)\nprint(f\"Length: {set_length}\")\n\n{1, 2, 3}\nAdded 4: {1, 2, 3, 4}\nRemoved 1: {2, 3, 4}\nIs 2 in the set? True\nLength: 3\n\n\nSets support operations such as intersection, union, and difference, which are performed using the &, |, and - operators respectively.\n\nset1 = {1, 2, 3, 4}\nset2 = set([3, 4, 5, 6])\n\n# Intersection\nprint(f\"Intersection: {set1 & set2}\")\n\n# Union\nprint(f\"Union: {set1 | set2}\")\n\n# Difference\nprint(f\"Difference: {set1 - set2}\")\n\nIntersection: {3, 4}\nUnion: {1, 2, 3, 4, 5, 6}\nDifference: {1, 2}\n\n\n\n\n\n\n\n\nNoteSets and data types\n\n\n\nSets can contain elements of different data types, including numbers, strings, and tuples. However, sets only support immutable elements, so you cannot add lists or dictionaries to a set.\n\n\n\n\n2.8.4 Dictionaries\nDictionaries are key-value pairs that provide a way to store and retrieve data using unique keys. They are defined with curly braces { } like sets, but contain pairs of elements called items, where each item is a key-value pair separated by a colon (:).\nDictionaries are unordered and mutable, allowing for efficient data lookup and modification. They are commonly used for mapping and associating values with specific keys, making them useful for tasks like storing settings, organizing data, or building lookup tables.\n\nstock_prices = {\"AAPL\": 150.25, \"GOOGL\": 1200.50, \"MSFT\": 250.00}\nprint(stock_prices)\n\n{'AAPL': 150.25, 'GOOGL': 1200.5, 'MSFT': 250.0}\n\n\nYou access the value for a specific key using square brackets [ ], and modify the value for a key using the assignment operator =. You add new key-value pairs to a dictionary using a new key and assignment operator and remove a key-value pair using the del keyword. The len() function returns the number of key-value pairs in a dictionary.\n\n# Accessing elements in a dictionary\nprice_aapl = stock_prices[\"AAPL\"]\nprint(f\"Price for AAPL: {price_aapl:0.2f}\")\n\n# Modifying an element\nstock_prices[\"GOOGL\"] = 1205.00\nprint(f\"Modified GOOGL: {stock_prices}\")\n\n# Adding a new element to the dictionary\nstock_prices[\"AMZN\"] = 3300.00\nprint(f\"Added AMZN: {stock_prices}\")\n\n# Removing an element from the dictionary\ndel stock_prices[\"MSFT\"]\nprint(f\"Removed MSFT: {stock_prices}\")\n\n# Length of the dictionary\ndict_length = len(stock_prices)\nprint(f\"Length: {dict_length}\")\n\nPrice for AAPL: 150.25\nModified GOOGL: {'AAPL': 150.25, 'GOOGL': 1205.0, 'MSFT': 250.0}\nAdded AMZN: {'AAPL': 150.25, 'GOOGL': 1205.0, 'MSFT': 250.0, 'AMZN': 3300.0}\nRemoved MSFT: {'AAPL': 150.25, 'GOOGL': 1205.0, 'AMZN': 3300.0}\nLength: 3\n\n\nThe collection module from Python‚Äôs standard library provides many other data structures such as defaultdict, OrderedDict, Counter, and deque. You can learn more about these data structures in the Python documentation.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#comparison-operators-and-branching",
    "href": "python/python-basics/index.html#comparison-operators-and-branching",
    "title": "2¬† Python Syntax",
    "section": "2.9 Comparison operators and branching",
    "text": "2.9 Comparison operators and branching\n\n2.9.1 Comparison operators\nPython provides several comparison operators that allow you to compare values and evaluate expressions. Comparison operators can be used with various data types, such as numbers, strings, or even complex data structures, and return a boolean value (True or False). Table¬†2.9 lists the comparison operators available in Python.\n\n\n\nTable¬†2.9: Comparison operators in Python\n\n\n\n\n\nOperator\nName\nExample\nResult\n\n\n\n\n==\nEqual\n1 == 2\nFalse\n\n\n!=\nNot equal\n1 != 2\nTrue\n\n\n&gt;\nGreater than\n1 &gt; 2\nFalse\n\n\n&lt;\nLess than\n1 &lt; 2\nTrue\n\n\n&gt;=\nGreater or equal\n1 &gt;= 2\nFalse\n\n\n&lt;=\nLess or equal\n1 &lt;= 2\nTrue\n\n\nin\nMembership\n1 in [1, 2, 3]\nTrue\n\n\nis\nIdentity comparison\n1 is None\nFalse\n\n\n\n\n\n\nTo create more complex conditions, you can chain multiple comparisons in a single expression using logical operators like and, or, or not. The result of a logical operator is a boolean value (True or False). Table¬†2.10 lists the logical operators available in Python.\n\n\n\nTable¬†2.10: Logical operators in Python\n\n\n\n\n\na\nb\na and b\na or b\nnot a\n\n\n\n\nTrue\nTrue\nTrue\nTrue\nFalse\n\n\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\nFalse\nTrue\nFalse\nTrue\nTrue\n\n\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\nWarning& and | are bitwise operators, not logical operators\n\n\n\nPython also provides bitwise operators that perform bitwise operations on integers. These operators are & (bitwise AND), | (bitwise OR), ^ (bitwise XOR), ~ (bitwise NOT), &lt;&lt; (bitwise left shift), and &gt;&gt; (bitwise right shift). Most casual Python users will not need to use these operators, but they can be confusing for new users due to their similar syntax to logical operators in other programming languages. To add to the confusion, popular Python libraries like NumPy and Pandas overload the bitwise operators to perform logical operations on arrays.\nIt is crucial for beginners to understand the distinction between logical and bitwise operators and to use the appropriate operators (which are usually and, or, or not) based on their intended purpose to ensure the desired logical evaluations are achieved.\n\n\nLonger expressions can be grouped using parentheses to ensure the desired order of operations. For example, a and b or c is equivalent to (a and b) or c, whereas a and (b or c) is different. Python does not offer a built-in exclusive or (XOR) operator, but it can be achieved using a combination of other operators.\n\ndef xor(a, b):\n    return (a and not b) or (not a and b)\n\n\nprint(f\"xor(True, True)) = {xor(True, True)}\")\nprint(f\"xor(True, False)) = {xor(True, False)}\")\nprint(f\"xor(False, True)) = {xor(False, True)}\")\nprint(f\"xor(False, False)) = {xor(False, False)}\")\n\nxor(True, True)) = False\nxor(True, False)) = True\nxor(False, True)) = True\nxor(False, False)) = False\n\n\n\n\n2.9.2 Branching\nBranching allows your code to execute different actions based on specific conditions. The primary branching construct in Python is the if statement, which can be combined with elif (short for ‚Äúelse if‚Äù) and else clauses to create more complex decision-making structures.\nLike other compound statements in Python, the if statement uses indentation to group statements together. The general syntax for an if statement is to start with the if keyword followed by a condition, then a colon (:), and, finally, an indented block of code that will be executed if the condition evaluates to True. The elif and else clauses are optional and can be used to specify additional conditions and code blocks to execute if the initial condition evaluates to False. The elif clause is used to chain multiple conditions together, whereas the else clause is used to specify a default code block to execute if none of the previous conditions evaluate to True.\n\nprice = 150\n\nif price &gt; 100:\n    print(\"The stock price is high.\")\n\nThe stock price is high.\n\n\nIn the previous example, the code block is executed because the price = 150, therefore the condition price &gt; 100 evaluates to True.\nWe can add an else clause to specify a default code block to execute if the condition evaluates to False.\n\nprice = 50\n\nif price &gt; 100:\n    print(\"The stock price is high.\")\nelse:\n    print(\"The stock price is low.\")\n\nThe stock price is low.\n\n\nWe can add an elif clause to specify additional conditions to evaluate if the initial condition evaluates to False. The elif clause can be used multiple times to chain multiple conditions together. The elif clause is optional, but if it is used, it must come before the else clause. The else clause is also optional, but if it is used, it must come last.\nIn all cases, the code block associated with the first condition that evaluates to True will be executed, and the remaining conditions will be skipped. If none of the conditions evaluate to True, then the code block associated with the else clause will be executed. If there is no else clause, then nothing will be executed.\n\nprice = 75\n\nif price &gt; 100:\n    print(\"The stock price is high.\")\nelif price &gt; 50:\n    print(\"The stock price is moderate.\")\nelse:\n    print(\"The stock price is low.\")\n\nThe stock price is moderate.\n\n\nYou can nest if statements inside other if statements to create more complex branching structures. The code block associated with the nested if statement must be indented further than the outer if statement. The nested if statement will only be evaluated if the condition associated with the outer if statement evaluates to True. When reading nested if statements, it is helpful to read from the top down and to keep track of the indentation level to understand which code blocks are associated with which conditions.\n\nprice = 150\nvolume = 1000000\n\nif price &gt; 100:\n    if volume &gt; 500000:\n        print(\"The stock price is high and has high volume.\")\n    else:\n        print(\"The stock price is high but has low volume.\")\nelse:\n    print(\"The stock price is not high.\")\n\nThe stock price is high and has high volume.\n\n\nConditions can be combined using the logical operators and, or, and not to create more complex conditions.\n\nprice = 150\nvolume = 1000000\n\nif price &gt; 100 and volume &gt; 500000:\n    print(\"The stock price is high and has high volume.\")\nelif price &gt; 100 or volume &gt; 500000:\n    print(\"The stock price is high or has high volume.\")\nelse:\n    print(\"The stock price is not high and has low volume.\")\n\nThe stock price is high and has high volume.\n\n\n\n\n2.9.3 Conditional assignment\nPython provides a convenient shorthand for assigning a value to a variable based on a condition. This is known as conditional assignment. The syntax for conditional assignment is variable = value1 if condition else value2. If the condition evaluates to True, the variable is assigned value1; otherwise, it is assigned value2.\n\nprice = 150\n\nmessage = \"The stock price is high.\" if price &gt; 100 else \"The stock price is low.\"\nprint(message)\n\nThe stock price is high.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#typing",
    "href": "python/python-basics/index.html#typing",
    "title": "2¬† Python Syntax",
    "section": "2.10 Typing",
    "text": "2.10 Typing\nPython is a dynamically typed language. This means that you don‚Äôt have to specify the type of a variable when you define it. The Python interpreter will automatically infer the type based on the value assigned to the variable.\nPython also supports optional type annotations, also called type hints, since version 3.5. This allows you to specify the types of variables, function arguments, and return values to improve code readability and catch potential errors early. The Python interpreter will ignore the type annotations and run the code normally. However, you can use external tools like mypy to analyze the code and check for type errors, and modern IDEs like VS Code provide built-in support for type checking.\n\nticker: str = \"AAPL\"\n\n\nstock_prices: list[float] = [150.25, 1200.50, 250.00]\n\n\n# Old version, not needed since Python 3.9\n\nfrom typing import List\n\nstock_prices: List[float] = [150.25, 1200.50, 250.00]\n\n\n# You can also specify function parameters and return types:\n\n\ndef calculate_profit(revenue: float, expenses: float) -&gt; float:\n    return revenue - expenses\n\n\nrevenue = 1000.00\nexpenses = 500.00\nprofit = calculate_profit(revenue, expenses)\n\n\n\n\n\n\n\nTipType hints in Visual Studio Code\n\n\n\nType hints are not required to run Python code, but they can be very useful to improve code readability and catch potential errors early. Modern IDEs like VS Code provide built-in support for type checking that you can enable.\nI find this quite overwhelming, so I prefer to enable type-checking only when needed. However, VS Code still uses type hints to provide useful features like hover info.\n\n\n\nTooltip when hovering over variable.\n\n\n\n\n\nTooltip when hovering over function.\n\n\n\n\nThe typing module provides a set of special types that can be used in type hints. Here are some of the most commonly used ones:\n\nAny: Any type\nOptional: An optional value (can be None)\nCallable: A function\nIterable: An iterable object (e.g., list, tuple, set)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#sec-functions-parameters-return-values",
    "href": "python/python-basics/index.html#sec-functions-parameters-return-values",
    "title": "2¬† Python Syntax",
    "section": "2.11 Functions: parameters and return values",
    "text": "2.11 Functions: parameters and return values\nFunctions help you organize and structure your code by encapsulating specific tasks or calculations. They allow you to define input parameters, perform operations, and return the results, making your code more flexible and maintainable. We have already written simple functions in Section 2.6; we will now look in more detail at how to define parameters and return values.\n\n\n\n\n\n\nNoteType hints in examples\n\n\n\nIn the examples below, I use type hints to indicate the type of the function parameters and return values. Type hints are not required to run Python code, but using them is a good practice as they provide helpful information to other developers (including your future self!) and tools such as linters and type checkers.\n\n\n\n2.11.1 Parameters\nParameters are variables defined within the function signature, enabling you to pass input values to the function when it is called. Parameters can have a default value assigned to them when no value is provided during the function call. Default values can make your functions more flexible and easy to use. Using the *args and **kwargs syntax, you can pass a variable number of positional or keyword arguments to a function, providing greater flexibility for handling different input scenarios.2\n\n\n\n\n\n\nNoteParameter vs.¬†argument\n\n\n\nThe terms function parameter and argument refer to different concepts related to functions.\nFunction parameter: A function parameter is a variable defined in the function‚Äôs definition or signature. It represents a value that the function expects to receive when it is called. Parameters act as placeholders for the actual values that will be passed as arguments when the function is invoked.\nArgument: An argument is the actual value that is passed to a function when it is called. It corresponds to a specific function parameter and provides the actual data or input that the function operates on. Arguments are supplied in the function call, within parentheses, and are passed to the corresponding function parameters based on their position or using keyword arguments.\n\n\n\ndef calculate_roi(investment: float, profit: float) -&gt; float:\n    return (profit / investment) * 100.0\n\n\ninvestment = 2000.00\nprofit = 500.00\nroi = calculate_roi(investment, profit)\nprint(roi)\n\n25.0\n\n\nIn the previous example, variables investment and profit are passed to the function calculate_roi() in the same order as they are defined in the function definition. This is called positional arguments. The names of the variables do not matter, only the order in which they are passed to the function. If we invert the order of the variables, the result will be different.\n\nbad_roi = calculate_roi(profit, investment)\nprint(bad_roi)\n\n400.0\n\n\nPositional arguments can be confusing when the function has many parameters or when the order of the arguments is not obvious. To avoid this, you can use keyword arguments.\n\nroi1 = calculate_roi(investment=2000.00, profit=500.00)\nprint(roi1)\n\nroi2 = calculate_roi(profit=500.00, investment=2000.00)\nprint(roi2)\n\nroi3 = calculate_roi(profit=profit, investment=investment)\nprint(roi3)\n\n25.0\n25.0\n25.0\n\n\nNote that the name of the original variables does not matter, only the name of the parameters in the function definition. In the last example, we use the same names for the variables and the parameters, but the Python interpreter does not care about that. The following code is equivalent to the previous one:\n\nx = 2000.00\ny = 500.00\n\nroi4 = calculate_roi(profit=y, investment=x)\nprint(roi4)\n\n25.0\n\n\nYou can also mix positional and keyword arguments. However, positional arguments must always come before keyword arguments.\n\nroi4 = calculate_roi(investment, profit=profit)\nprint(roi4)\n\n25.0\n\n\nDefault parameters are useful when you want to provide a default value for a parameter, which is used when no value is provided during the function call. This makes your functions more flexible and easy to use, as you can omit parameters that have a default value.\nTo define a default parameter, assign a value to the parameter in the function definition using =. When the function is called, the default value will be used if no value is provided for that parameter. If a value is provided, it will override the default value.\n\ndef calculate_present_value(cashflow: float, discount_rate: float = 0.1) -&gt; float:\n    return cashflow / (1 + discount_rate)\n\n\n# Uses default discount_rate of 10%\npv = calculate_present_value(cashflow=100.00)\nprint(f\"Present Value: {pv}\")\n\n# Uses discount_rate of 5%\npv2 = calculate_present_value(cashflow=100.00, discount_rate=0.05)\nprint(f\"Present Value: {pv2}\")\n\nPresent Value: 90.9090909090909\nPresent Value: 95.23809523809524\n\n\nParameters with default values must come after parameters without default values. Otherwise, the function call will raise a SyntaxError. When you call a function with default parameters, you can omit any parameters that have a default value. However, when you omit a parameter, you must use keyword parameters to specify the values for the parameters that follow it.\n\n\n2.11.2 Passing arguments: peek under the hood\nPython uses a mechanism called ‚Äúpassing arguments by assignment.‚Äù In simple terms, this means that when you pass an argument to a function, a copy of the reference to the object is made and assigned to the function parameter.\nWhen an immutable object (like a number, string, or tuple) is passed as an argument, it is effectively passed by value. Any modifications made to the parameter within the function do not affect the original object outside the function. Changes to the parameter create a new object rather than modifying the original one.\nOn the other hand, when a mutable object (like a list or dictionary) is passed as an argument, it is effectively passed by reference. Any modifications made to the parameter within the function will affect the original object outside the function. This is because both the parameter and the original object refer to the same memory location, so changes are reflected in both.\n\n\n2.11.3 Return values\nFunctions can return a value, multiple values, or no value at all. To return a value, use the return keyword followed by the value or expression you want to return. If a function doesn‚Äôt include a return statement, it will implicitly return None. A function can contain multiple return statements, but the execution of the function will stop as soon as any of them is reached.\nA function can return a single value, such as a number, string, or a more complex data structure. A function can also return multiple values, typically in the form of a tuple. This is useful when you need to return several related results from a single function call. If a function doesn‚Äôt explicitly return a value using the return keyword, it will implicitly return None when it reaches the end of the function body.\n\ndef calculate_mean_and_median(numbers: list[float]) -&gt; tuple[float, float]:\n    mean = sum(numbers) / len(numbers)\n\n    # Sort the numbers in ascending order using the sorted() function\n    sorted_numbers = sorted(numbers)\n    length = len(sorted_numbers)\n\n    if length % 2 == 0:\n        median = (sorted_numbers[length // 2 - 1] + sorted_numbers[length // 2]) / 2\n    else:\n        median = sorted_numbers[length // 2]\n\n    return mean, median\n\n\nprices = [150.25, 1200.50, 250.00]\nmean, median = calculate_mean_and_median(prices)\nprint(f\"Mean: {mean}, Median: {median}\")\n\nMean: 533.5833333333334, Median: 250.0\n\n\n\n\n2.11.4 Scope\nIn Python, the scope of a variable refers to the region of a program where the variable is accessible and can be referenced. The scope determines the visibility and lifetime of a variable, including where it can be accessed and modified.\nWhen using Python functions, there are two main scopes to consider:\n\nLocal scope (function scope): Variables defined within a function have local scope. They are accessible only within the function where they are defined. Local variables are created when the function is called and destroyed when the function execution completes or reaches a return statement. They are not accessible outside the function.\nGlobal scope (module scope): Variables defined outside of any function in the interactive window or in a Python script, have global scope. They are accessible from anywhere within the program, including all functions.\n\nWhen a function is called, it creates a new local scope, which is independent of the global scope. Inside the function, the local scope takes precedence over the global scope. If a variable is referenced within a function, Python first checks the local scope for its existence. If not found, it then searches the global scope.\n\n# Global variable\nmessage = \"Hello\"\nx = 123\n\n\ndef say_hello(m: str):\n    # Local variable\n    message = \"Hello, World!\"\n    print(f\"local message = {message}\")\n\n    # Local variable, copied from the argument\n    print(f\"local m = {m}\")\n\n    # print(f\"global x inside function = {x}\") \n1\n\n    # Local variable\n    x = len(m)\n    print(f\"local x = {x}\")\n\n    return x\n\n\ny = say_hello(message)\n\nprint(f\"global message = {message}\")\nprint(f\"global x after function = {x}\")\nprint(f\"global y = {y}\")\n\n\n1\n\nThis will access the global x if there is no local variable with the same name. In this specific case, it will cause an error because x is actually defined later in the function.\n\n\n\n\nlocal message = Hello, World!\nlocal m = Hello\nlocal x = 5\nglobal message = Hello\nglobal x after function = 123\nglobal y = 5\n\n\nIf you want to modify a global variable from within a function, you can use the global keyword to indicate that the variable being referred to is a global variable rather than creating a new local variable. However, this is generally not recommended, as it can lead to unexpected side effects and make the code difficult to understand and debug.\nIt is important to carefully manage variable scope to avoid naming conflicts and unintended side effects. Understanding the scope of variables helps in organizing and managing data within functions and modules effectively.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#loops",
    "href": "python/python-basics/index.html#loops",
    "title": "2¬† Python Syntax",
    "section": "2.12 Loops",
    "text": "2.12 Loops\nLoops enable you to easily perform repetitive tasks or iterate through data structures, such as sequences and collections. Python provides two primary loop constructs: the for loop and the while loop.\n\n2.12.1 for loops\nFor loops in Python are used to iterate over a sequence (e.g., a list, tuple, or string) or other iterable objects. The loop iterates through each item in the sequence, executing a block of code for each item. The ‚Äòfor‚Äô loop has the following syntax:\nfor item in sequence:\n    # code to execute for each item in the sequence\nAs for function bodies and conditional statements, the code block in a loop must be indented.\n\n\n2.12.2 range() function\nThe built-in range() function in Python is often used in conjunction with for loops to generate a sequence of numbers. This function can be used to create a range of numbers with a specified start, end, and step size. The syntax for the range function is:\nrange(start, stop, step)\nThe ‚Äòstart‚Äô and ‚Äòstep‚Äô arguments are optional, with default values of 0 and 1, respectively. The ‚Äòstop‚Äô argument is required and defines the upper limit of the range (exclusive).\n\n1for i in range(5):\n    print(i)\n\n\n1\n\nThe range(5) function generates a sequence of numbers from 0 to 4 (inclusive) with a step of 1.\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n1for i in range(2, 7):\n    print(i)\n\n\n1\n\nThe range(2, 7) function generates a sequence of numbers from 2 to 6 (inclusive) with a step of 1.\n\n\n\n\n2\n3\n4\n5\n6\n\n\n\n1for i in range(1, 11, 2):\n    print(i)\n\n\n1\n\nThe range(1, 11, 2) function generates a sequence of numbers from 1 to 10 (inclusive) with a step of 2.\n\n\n\n\n1\n3\n5\n7\n9\n\n\nYou can use a negative step size to generate a sequence of numbers in reverse order.\n\n1for i in range(5, 0, -1):\n    print(i)\n\n\n1\n\nThe range(5, 0, -1) function generates a sequence of numbers from 5 to 1 (inclusive) with a step of -1 (decreasing).\n\n\n\n\n5\n4\n3\n2\n1\n\n\nYou can use the range() function to generate a sequence of numbers and iterate through them using a for loop to execute some code for each number in the sequence. In this example, we use the range() function to generate a sequence of numbers from 1 to 5 (inclusive) and calculate the compound interest for each year of an investment.\n\nprincipal = 1000\nrate = 0.05\n\nfor year in range(1, 6):\n    interest = principal * ((1 + rate) ** year - 1)\n    print(f\"Year {year}: Interest = {interest:.2f}\")\n\nYear 1: Interest = 50.00\nYear 2: Interest = 102.50\nYear 3: Interest = 157.63\nYear 4: Interest = 215.51\nYear 5: Interest = 276.28\n\n\n\n\n2.12.3 continue and break statements\nYou can use the continue and break statements to control the flow of a for loop. The continue statement skips the current iteration and continues with the next one. The break statement terminates the loop and transfers execution to the statement immediately following the loop.\n\nfor i in range(10):\n    if i == 3:\n1        continue\n    elif i == 5:\n2        break\n    print(i)\n\n\n1\n\nSkip the rest of the code in the loop and go to the next iteration\n\n2\n\nExit the loop\n\n\n\n\n0\n1\n2\n4\n\n\n\n\n2.12.4 for loops with lists\nYou can also loop over a collection of items using the for loop. For example, you can loop over a list of numbers to calculate the sum of all numbers in the list.\n\ndaily_profit_losses = [1500, 1200, 1800, 2300, 900]\n\ntotal_pl = 0\nfor pl in daily_profit_losses:\n    total_pl += pl\n\nprint(f\"Total P&L for the period: {total_pl}\")\n\nTotal P&L for the period: 7700\n\n\n\n\n\n\n\n\nTipBuilt-in functions\n\n\n\nPython provides several built-in functions that can be used to perform common tasks. For example, the sum() function can be used to calculate the sum of all numbers in a list.\n\ndaily_profit_losses = [1500, 1200, 1800, 2300, 900]\ntotal_pl = sum(daily_profit_losses)\nprint(f\"Total P&L for the period: {total_pl}\")\n\nTotal P&L for the period: 7700\n\n\n\n\nYou can combine two lists of the same length using the zip() built-in function to loop over both lists at the same time.\n\nstock_prices = [150.25, 1200.50, 250.00]\nquantities = [10, 5, 20]\n\ntotal_value = 0\nfor price, quantity in zip(stock_prices, quantities):\n    total_value += price * quantity\n\nprint(f\"Total value of the portfolio: {total_value:.2f}\")\n\nTotal value of the portfolio: 12505.00\n\n\nYou can use the enumerate() function to loop over a list and get the index of each item in the list.\n\ncash_flows = [100, 200, 300, 400, 500]\n\ndiscount_rate = 0.1\n\npresent_values = []\nfor year, cash_flow in enumerate(cash_flows):\n    present_value = cash_flow / (1 + discount_rate) ** year\n    print(f\"Year {year}: Present Value = {present_value:.2f}\")\n\nYear 0: Present Value = 100.00\nYear 1: Present Value = 181.82\nYear 2: Present Value = 247.93\nYear 3: Present Value = 300.53\nYear 4: Present Value = 341.51\n\n\n\n\n\n\n\n\nNoteIterables and iterators\n\n\n\nIn Python, an iterable is an object capable of returning its elements one at a time, such as a list, tuple, or string. An iterator is an object that keeps track of its current position within an iterable and provides a way to access the next element when required.\nMany built-in data types and functions return values in Python are iterables or iterators. For example, the range() function returns an iterator that produces a sequence of numbers. The zip() function returns an iterator that produces tuples containing elements from the input iterables. The enumerate() function returns an iterator that produces tuples containing the index and value of each item in the input iterable.\nThere are many benefits to iterators, such as better memory efficiency and allowing you to work with infinite sequences, such as the sequence of all prime numbers. However, you can‚Äôt print the result of calling an iterator like zip() directly. Instead, you must convert the iterator to a list or another collection using the list() function.\n\nstock_prices = [150.25, 1200.50, 250.00]\nquantities = [10, 5, 20]\n\nzipped = zip(stock_prices, quantities)\n\nprint(f\"zipped: {zipped}\")\nprint(f\"list(zipped): {list(zipped)}\")\n\nzipped: &lt;zip object at 0x124798080&gt;\nlist(zipped): [(150.25, 10), (1200.5, 5), (250.0, 20)]\n\n\n\n\n\n\n\n2.12.5 Nested for loops\nYou can nest loops. The inner loop will be executed one time for each iteration of the outer loop.\nIn this example, we have a list of products, each with a name, per-item profit margin, and a list of quantities sold at different times. The outer loop iterates through each product, while the inner loop iterates through the quantities for each product. The product‚Äôs profit is calculated by multiplying its margin by the quantity sold and adding it to the product_profit variable. The total_profit variable accumulates the profit for all products.\n\nproducts = [\n    {\"name\": \"Product A\", \"margin\": 10, \"quantities\": [5, 10, 15]},\n    {\"name\": \"Product B\", \"margin\": 20, \"quantities\": [2, 4, 6]},\n    {\"name\": \"Product C\", \"margin\": 30, \"quantities\": [1, 3, 5]},\n]\n\ntotal_profit = 0\n\nfor product in products:\n    product_profit = 0\n    for quantity in product[\"quantities\"]:\n        product_profit += product[\"margin\"] * quantity\n    total_profit += product_profit\n    print(f\"Profit for {product['name']}: {product_profit}\")\n\nprint(f\"Total profit: {total_profit}\")\n\nProfit for Product A: 300\nProfit for Product B: 240\nProfit for Product C: 270\nTotal profit: 810\n\n\n\n\n2.12.6 while loops\nWhile loops are used to repeatedly execute a block of code as long as a specified condition is True. The while loop has the following syntax:\nwhile condition:\n    # code to execute while the condition is True\nIn this example, we use a while loop to calculate the number of years it takes for an investment to double at a given interest rate.\n\nprincipal = 1000\nrate = 0.05\nbalance = principal\ntarget = principal * 2\nyears = 0\n\nwhile balance &lt; target:\n    interest = balance * rate\n    balance += interest\n    years += 1\n\nprint(f\"It takes {years} years for the investment to double.\")\n\nIt takes 15 years for the investment to double.\n\n\nYou can use the continue statement to skip the rest of the code in the current iteration and continue with the next one and the break statement to exit a while loop before the condition becomes False.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#list-and-dictionary-comprehensions",
    "href": "python/python-basics/index.html#list-and-dictionary-comprehensions",
    "title": "2¬† Python Syntax",
    "section": "2.13 List and dictionary comprehensions",
    "text": "2.13 List and dictionary comprehensions\nPython supports list and dictionary comprehensions, which allow you to create lists and dictionaries in a concise and efficient manner by transforming or filtering items from another iterable, such as a range, a tuple or another list. Comprehensions can be confusing at first, but they are a powerful tool worth learning.\n\n2.13.1 List comprehensions\nList comprehension syntax consists of square brackets containing an expression followed by a for clause, then zero or more for or if clauses. The expression can be anything, meaning you can put in all kinds of objects in lists.\n\n# This is perfectly valid:\nsquared1 = []\nfor x in range(10):\n    squared1.append(x * x)\n\nprint(squared1)\n\n# This is much shorter!\nsquared2 = [x * x for x in range(10)]\n\nprint(squared2)\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nYou can use list comprehensions to transform items from a list into a new list using complex expressions.\n\n# Calculate the percentage change for a list of stock prices\nstock_prices = [150.25, 1200.50, 250.00, 175.00, 305.75]\npercentage_changes = [\n    (stock_prices[i + 1] - stock_prices[i]) / stock_prices[i] * 100\n    for i in range(len(stock_prices) - 1)\n]\n\nprint(\"Percentage changes:\", percentage_changes)\n\nPercentage changes: [699.0016638935108, -79.17534360683048, -30.0, 74.71428571428571]\n\n\n\n\n2.13.2 Dictionary comprehensions\nSimilar to list comprehensions, dictionary comprehensions use a single line of code to define the structure of the new dictionary.\n\n# Create a dictionary mapping numbers to their squares\nsquares = {i: i**2 for i in range(1, 6)}\n\nprint(squares)\n\n{1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n\n\n\n\n2.13.3 Filtering and transforming\nYou can use conditional statements in list and dictionary comprehensions to filter items from the source iterable.\n\n# Create a dictionary mapping even numbers to their cubes\neven_cubes = {i: i**3 for i in range(1, 6) if i % 2 == 0}\n\nprint(even_cubes)\n\n{2: 8, 4: 64}\n\n\nYou can transform the items in the source iterable before adding them to the new list or dictionary.\n\n# List of stock symbols and prices\nstock_data = [(\"AAPL\", 150.25), (\"GOOG\", 1200.50), (\"MSFT\", 250.00)]\n\n# Create a dictionary mapping lowercase stock symbols to their prices\nstocks = {symbol.lower(): price for symbol, price in stock_data}\n\nprint(stocks)\n\n{'aapl': 150.25, 'goog': 1200.5, 'msft': 250.0}\n\n\n\n\n2.13.4 Nested comprehensions\nYou can nest comprehensions inside other comprehensions to create complex data structures.\n\n# Create a list of (stock, prices) tuples\nstock_data = [\n    (\"AAPL\", [150.25, 150.50, 150.75]),\n    (\"GOOG\", [1200.50, 1201.00]),\n    (\"MSFT\", [250.00, 250.25, 250.50, 250.75]),\n]\n\n1stocks = [(symbol, price) for symbol, prices in stock_data for price in prices]\nprint(stocks)\n\n\n1\n\nThe comprehension is evaluated from left to right, so the prices variable is available in the second for clause.\n\n\n\n\n[('AAPL', 150.25), ('AAPL', 150.5), ('AAPL', 150.75), ('GOOG', 1200.5), ('GOOG', 1201.0), ('MSFT', 250.0), ('MSFT', 250.25), ('MSFT', 250.5), ('MSFT', 250.75)]\n\n\n\n\n\n\n\n\nWarningNested comprehensions vs readability\n\n\n\nNested comprehensions with more than two levels can be difficult to read, so you should avoid them if possible. If you find yourself nesting comprehensions, it‚Äôs probably a good idea to use a regular for loop instead. Remember, code readability is more important than brevity.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#working-with-files-and-the-with-statement",
    "href": "python/python-basics/index.html#working-with-files-and-the-with-statement",
    "title": "2¬† Python Syntax",
    "section": "2.14 Working with files and the with statement",
    "text": "2.14 Working with files and the with statement\nSo far, we‚Äôve focused on working with data in memory. But in practice, you‚Äôll often need to read data from files or save results to disk. Python provides powerful tools for working with files, and understanding them requires introducing an important concept: context managers and the with statement.\n\n2.14.1 The with statement\nWhen you open a file, you acquire a resource that needs to be properly released when you‚Äôre done. If you forget to close a file, you can run into problems: data might not be written to disk, you could run out of file handles, or other programs might not be able to access the file.\nThe with statement solves this problem elegantly. It ensures that cleanup happens automatically, even if an error occurs while you‚Äôre working with the resource. Here‚Äôs the basic pattern:\nwith some_resource as name:\n    # work with the resource\n    ...\n# resource is automatically cleaned up here\nThe with statement works with any object that implements the context manager protocol. These objects define what should happen when entering the with block (acquiring the resource) and when exiting it (releasing the resource). Many built-in Python types support this, including files, database connections, and locks.\n\n\n\n\n\n\nTipWhy use with?\n\n\n\nYou might be tempted to open and close files manually:\nf = open(\"data.txt\")\ncontent = f.read()\nf.close()  # Easy to forget!\nThis works, but it‚Äôs risky. If an error occurs before close() is called, the file might remain open. The with statement guarantees cleanup:\nwith open(\"data.txt\") as f:\n    content = f.read()\n# File is automatically closed, even if an error occurred\nAlways prefer with when working with files and other resources.\n\n\n\n\n2.14.2 Reading and writing files with open()\nThe built-in open() function is Python‚Äôs standard way to work with files. It takes a file path and a mode, and returns a file object that you can read from or write to.\nCommon modes include:\n\n\"r\" - read mode (default)\n\"w\" - write mode (creates a new file or overwrites existing)\n\"a\" - append mode (adds to the end of an existing file)\n\"x\" - exclusive creation (fails if the file already exists)\n\nFor text files, you can read the entire contents at once or process line by line:\n\n# First, let's create a sample file to work with\nsample_data = \"\"\"Date,Ticker,Price\n2024-01-02,AAPL,185.64\n2024-01-02,MSFT,374.58\n2024-01-03,AAPL,184.25\"\"\"\n\nwith open(\"sample_prices.csv\", \"w\") as f:\n    f.write(sample_data)\n\n# Now read it back\nwith open(\"sample_prices.csv\", \"r\") as f:\n    content = f.read()\n\nprint(content)\n\nDate,Ticker,Price\n2024-01-02,AAPL,185.64\n2024-01-02,MSFT,374.58\n2024-01-03,AAPL,184.25\n\n\nFor processing files line by line, you can iterate directly over the file object:\n\nwith open(\"sample_prices.csv\", \"r\") as f:\n    for line in f:\n        print(f\"Line: {line.strip()}\")  # strip() removes the newline character\n\nLine: Date,Ticker,Price\nLine: 2024-01-02,AAPL,185.64\nLine: 2024-01-02,MSFT,374.58\nLine: 2024-01-03,AAPL,184.25\n\n\nThis approach is memory-efficient for large files because it only loads one line at a time.\n\n\n\n\n\n\nWarningFile encoding\n\n\n\nWhen working with text files, be aware of character encoding. The default encoding depends on your system, which can cause problems when sharing files. It‚Äôs good practice to specify the encoding explicitly:\nwith open(\"data.txt\", \"r\", encoding=\"utf-8\") as f:\n    content = f.read()\nUTF-8 is the most common encoding for modern text files and handles international characters well.\n\n\n\n\n2.14.3 Working with paths using pathlib\nPython‚Äôs pathlib module provides a modern, object-oriented approach to working with file paths. Instead of manipulating path strings directly, you work with Path objects that understand the structure of file paths on your operating system.\n\nfrom pathlib import Path\n\n# Create a Path object\ndata_dir = Path(\"data\")\nfile_path = data_dir / \"prices.csv\"  # Use / to join paths\n\nprint(f\"Full path: {file_path}\")\nprint(f\"Parent directory: {file_path.parent}\")\nprint(f\"File name: {file_path.name}\")\nprint(f\"File extension: {file_path.suffix}\")\n\nFull path: data/prices.csv\nParent directory: data\nFile name: prices.csv\nFile extension: .csv\n\n\nThe / operator for joining paths is particularly elegant. It works correctly across different operating systems, so you don‚Äôt need to worry about whether to use forward slashes or backslashes.\nPath objects have convenient methods for common file operations:\n\nfrom pathlib import Path\n\n# Check if a file exists\nsample_file = Path(\"sample_prices.csv\")\nprint(f\"File exists: {sample_file.exists()}\")\n\n# Read text directly (no need for open!)\nif sample_file.exists():\n    content = sample_file.read_text()\n    print(f\"First 50 characters: {content[:50]}...\")\n\nFile exists: True\nFirst 50 characters: Date,Ticker,Price\n2024-01-02,AAPL,185.64\n2024-01-0...\n\n\nFor simple read and write operations, Path methods like read_text() and write_text() are often more concise than using open():\n\nfrom pathlib import Path\n\n# Write text to a file\noutput_file = Path(\"output.txt\")\noutput_file.write_text(\"Hello from pathlib!\")\n\n# Read it back\nprint(output_file.read_text())\n\n# Clean up\noutput_file.unlink()  # Delete the file\n\nHello from pathlib!\n\n\nWhen you need more control, such as reading line by line or appending to a file, Path objects work seamlessly with open():\n\nfrom pathlib import Path\n\nfile_path = Path(\"sample_prices.csv\")\n\nwith file_path.open(\"r\") as f:\n    header = f.readline()\n    print(f\"Header: {header.strip()}\")\n\nHeader: Date,Ticker,Price\n\n\n\n\n\n\n\n\nTipPath vs string paths\n\n\n\nWhile you can use plain strings for file paths, Path objects offer several advantages:\n\nCross-platform compatibility: Path handles the differences between Windows (\\) and Unix (/) automatically\nClear intent: Code using Path clearly signals you‚Äôre working with file paths\nConvenient methods: Methods like exists(), is_dir(), suffix, and stem make common operations easy\nComposability: The / operator makes building paths readable and safe\n\nMost modern Python code uses pathlib for path manipulation. You‚Äôll see it throughout this book when working with files.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#pattern-matching",
    "href": "python/python-basics/index.html#pattern-matching",
    "title": "2¬† Python Syntax",
    "section": "2.15 Pattern matching",
    "text": "2.15 Pattern matching\nPattern matching is a powerful feature introduced in Python 3.10. It allows you to match the structure of data and execute code based on the shape and contents of that data. It is particularly useful for working with complex data structures and can lead to cleaner and more readable code.\n\n\n\n\n\n\nCautionPython 3.10+ only\n\n\n\nPattern matching is a new feature introduced in Python 3.10, released on October 4, 2021. If you‚Äôre using an older version of Python, or your code will be running on a system with an older version of Python, you should avoid using pattern matching.\n\n\nPattern matching is implemented using the match statement, which is similar to a switch-case statement in other languages but with more advanced capabilities. The match statement takes an expression and a series of cases. Each case is a pattern that is matched against the expression in turn. If the pattern matches, the code in that case is executed, otherwise, the next case is checked. The match statement can also have a case with the wildcard pattern _, which will always match. If no pattern matches, a MatchError is raised.\n\ndef process_transaction(transaction: tuple):\n    match transaction:\n        case (\"deposit\", amount):\n            print(f\"Deposit: {amount:.2f}\")\n        case (\"withdraw\", amount):\n            print(f\"Withdraw: {amount:.2f}\")\n        case (\"transfer\", amount, recipient):\n            print(f\"Transfer {amount:.2f} to {recipient}\")\n        case _:\n            print(\"Unknown transaction\")\n\nprocess_transaction((\"deposit\", 1000))\nprocess_transaction((\"burn\", 100.00))\nprocess_transaction((\"transfer\", 500.00, \"John Doe\"))\nprocess_transaction((\"withdraw\", 250.00))\n\nDeposit: 1000.00\nUnknown transaction\nTransfer 500.00 to John Doe\nWithdraw: 250.00",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#additional-resources",
    "href": "python/python-basics/index.html#additional-resources",
    "title": "2¬† Python Syntax",
    "section": "2.16 Additional resources",
    "text": "2.16 Additional resources\n\nPython 3.14 documentation\nLubanovic, Bill. Introducing Python, 2nd Edition, O‚ÄôReilly Media, Inc., 2019",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/python-basics/index.html#footnotes",
    "href": "python/python-basics/index.html#footnotes",
    "title": "2¬† Python Syntax",
    "section": "",
    "text": "A pseudo-random number is a sequence of numbers that appear random but are generated using a deterministic algorithm.‚Ü©Ô∏é\nI do not recommend using variable-length parameters unless you have a specific need for them, as they can make your code more complex and harder to read and understand.‚Ü©Ô∏é",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python Syntax</span>"
    ]
  },
  {
    "objectID": "python/oop/index.html",
    "href": "python/oop/index.html",
    "title": "3¬† Object-Oriented Programming Basics",
    "section": "",
    "text": "3.1 Classes and Objects\nObject-Oriented Programming (OOP) is a programming paradigm that organizes code around ‚Äúobjects‚Äù rather than functions and logic. While Python fully supports OOP, you don‚Äôt need to use it for everything you do. In fact, for many data analysis tasks, a procedural or functional approach is simpler and more appropriate.\nThat said, understanding the basics of OOP is valuable for several reasons. First, many of the libraries you‚Äôll use in empirical finance are built using OOP principles, so understanding these concepts will help you use them more effectively. Second, there are certain situations in research code where OOP can make your code cleaner, more organized, and easier to maintain. Finally, OOP provides a way to model real-world entities and relationships in your code, which can be particularly useful when working with financial concepts like trades and limit order books.\nIn this chapter, we‚Äôll cover the fundamentals of OOP in Python, focusing on practical applications relevant to empirical finance. We‚Äôll start with the basic concepts of classes and objects, then discuss when OOP is genuinely useful in research code, and finally introduce Python‚Äôs data classes, which provide a streamlined way to work with structured data.\nAt its core, object-oriented programming is about creating custom data types that bundle together related data and the functions that operate on that data. Let‚Äôs break down the key concepts.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Object-Oriented Programming Basics</span>"
    ]
  },
  {
    "objectID": "python/oop/index.html#classes-and-objects",
    "href": "python/oop/index.html#classes-and-objects",
    "title": "3¬† Object-Oriented Programming Basics",
    "section": "",
    "text": "3.1.1 What is a Class?\nA class is essentially a blueprint or template for creating objects. It defines what data an object will hold (attributes) and what operations can be performed on that data (methods). Think of a class as a cookie cutter and objects as the cookies made from that cutter.\nLet‚Äôs start with a simple example. Suppose you‚Äôre working on a project that involves tracking individual trades. Each trade has certain properties: a ticker symbol, a quantity, a price, and whether it‚Äôs a buy or sell. You could represent each trade as a dictionary:\n\ntrade1 = {\n    \"ticker\": \"AAPL\",\n    \"quantity\": 100,\n    \"price\": 150.50,\n    \"side\": \"buy\"\n}\n\ntrade2 = {\n    \"ticker\": \"MSFT\",\n    \"quantity\": 50,\n    \"price\": 280.25,\n    \"side\": \"sell\"\n}\n\nThis works, but it has some limitations. There‚Äôs no guarantee that every trade dictionary has the same keys. You might accidentally misspell a key, or forget to include one. And if you want to calculate the total value of a trade, you need to write that logic separately.\nA class provides a better solution. Here‚Äôs how we might define a Trade class:\n\nclass Trade:\n1    def __init__(self, ticker: str, quantity: int, price: float, side: str):\n2        self.ticker = ticker\n        self.quantity = quantity\n        self.price = price\n        self.side = side\n\n3    def value(self) -&gt; float:\n        \"\"\"Calculate the total value of the trade.\"\"\"\n        return self.quantity * self.price\n\n4    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation of the trade.\"\"\"\n        return f\"Trade({self.ticker}, {self.quantity}, ${self.price}, {self.side})\"\n\n\n1\n\nWe define the class with class Trade:. By convention, class names use CamelCase. The __init__ method is a special method called a constructor. It runs automatically when you create a new object from the class. The self parameter refers to the instance being created.\n\n2\n\nInside __init__, we set attributes on the object using self.attribute_name. These become the object‚Äôs data.\n\n3\n\nThe value method is a regular method that calculates the trade‚Äôs total value. Like all methods, it takes self as its first parameter.\n\n4\n\nThe __repr__ method is another special method that defines how the object should be displayed. Methods that start and end with double underscores are called ‚Äúdunder‚Äù (double underscore) methods or magic methods.\n\n\n\n\nNow we can create trades as objects:\n\ntrade1 = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade2 = Trade(\"MSFT\", 50, 280.25, \"sell\")\n\nprint(trade1)\nprint(f\"Trade value: ${trade1.value():.2f}\")\n\nTrade(AAPL, 100, $150.5, buy)\nTrade value: $15050.00\n\n\nThis is cleaner and more robust. Every Trade object is guaranteed to have the required attributes, and the logic for calculating value is bundled with the data.\n\n\n3.1.2 Attributes and Methods\nLet‚Äôs clarify some terminology:\n\nAttributes are variables that belong to an object. In our example, ticker, quantity, price, and side are attributes.\nMethods are functions that belong to a class. They operate on the object‚Äôs data. In our example, value() is a method.\nInstance refers to a specific object created from a class. trade1 and trade2 are instances of the Trade class.\n\nYou access attributes and call methods using dot notation:\n\nprint(trade1.ticker)      # Accessing an attribute\nprint(trade1.value())     # Calling a method\n\nAAPL\n15050.0\n\n\n\n\n3.1.3 Adding More Functionality\nLet‚Äôs expand our Trade class to include more useful functionality. Suppose we want to compare trades and calculate profit and loss:\n\nclass Trade:\n    def __init__(self, ticker: str, quantity: int, price: float, side: str):\n        self.ticker = ticker\n        self.quantity = quantity\n        self.price = price\n        self.side = side\n\n    def value(self) -&gt; float:\n        \"\"\"Calculate the total value of the trade.\"\"\"\n        return self.quantity * self.price\n\n1    def pnl(self, current_price: float) -&gt; float:\n        \"\"\"Calculate profit/loss relative to a current price.\"\"\"\n        if self.side == \"buy\":\n            return self.quantity * (current_price - self.price)\n        else:  # sell\n            return self.quantity * (self.price - current_price)\n\n    def __repr__(self) -&gt; str:\n        return f\"Trade({self.ticker}, {self.quantity}, ${self.price:.2f}, {self.side})\"\n\n2    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Check if two trades are equal.\"\"\"\n        if not isinstance(other, Trade):\n            return NotImplemented\n        return (self.ticker == other.ticker and\n                self.quantity == other.quantity and\n                self.price == other.price and\n                self.side == other.side)\n\n\n1\n\nThe pnl method calculates the profit or loss based on a current market price. The logic differs for buys (profit when price goes up) and sells (profit when price goes down).\n\n2\n\nThe __eq__ method defines what it means for two Trade objects to be equal. Here, two trades are equal if all their attributes match.\n\n\n\n\nThe __eq__ method is one of several special comparison methods Python supports. Others include __ne__ (not equal, !=), __lt__ (less than, &lt;), __le__ (less than or equal, &lt;=), __gt__ (greater than, &gt;), and __ge__ (greater than or equal, &gt;=). For a complete list of these ‚Äúrich comparison‚Äù methods and other special methods, see the Python documentation on basic customization.\nNow we can do more with our trades:\n\ntrade = Trade(\"AAPL\", 100, 150.50, \"buy\")\nprint(f\"Trade value: ${trade.value():.2f}\")\n\n# Calculate P&L at a current price\ncurrent_price = 160.00\npnl = trade.pnl(current_price)\nprint(f\"P&L at ${current_price:.2f}: ${pnl:.2f}\")\n\n# Test equality\ntrade2 = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade3 = Trade(\"MSFT\", 50, 280.25, \"sell\")\nprint(f\"trade == trade2: {trade == trade2}\")  # Same attributes\nprint(f\"trade == trade3: {trade == trade3}\")  # Different attributes\n\nTrade value: $15050.00\nP&L at $160.00: $950.00\ntrade == trade2: True\ntrade == trade3: False\n\n\n\n\n3.1.4 A More Complex Example: Portfolio Class\nLet‚Äôs build a more sophisticated example: a Portfolio class that manages a collection of trades. This demonstrates how objects can contain other objects:\n\nclass Portfolio:\n    def __init__(self, name):\n        self.name = name\n        self.trades = []\n\n    def add_trade(self, trade):\n        \"\"\"Add a trade to the portfolio.\"\"\"\n        self.trades.append(trade)\n\n    def total_value(self):\n        \"\"\"Calculate the total value of all trades.\"\"\"\n        return sum(trade.value() for trade in self.trades)\n\n    def positions(self):\n        \"\"\"Calculate net position for each ticker.\"\"\"\n        positions = {}\n        for trade in self.trades:\n            if trade.ticker not in positions:\n                positions[trade.ticker] = 0\n\n            if trade.side == \"buy\":\n                positions[trade.ticker] += trade.quantity\n            else:  # sell\n                positions[trade.ticker] -= trade.quantity\n\n        return positions\n\n    def __repr__(self):\n        return f\"Portfolio('{self.name}', {len(self.trades)} trades)\"\n\n    def summary(self):\n        \"\"\"Print a summary of the portfolio.\"\"\"\n        print(f\"Portfolio: {self.name}\")\n        print(f\"Total trades: {len(self.trades)}\")\n        print(f\"Total value: ${self.total_value():.2f}\")\n        print(\"\\nPositions:\")\n        for ticker, quantity in self.positions().items():\n            print(f\"  {ticker}: {quantity} shares\")\n\nNow we can use our Portfolio class:\n\n# Create a portfolio\nportfolio = Portfolio(\"My Research Portfolio\")\n\n# Add some trades\nportfolio.add_trade(Trade(\"AAPL\", 100, 150.50, \"buy\"))\nportfolio.add_trade(Trade(\"AAPL\", 50, 155.00, \"buy\"))\nportfolio.add_trade(Trade(\"MSFT\", 75, 280.25, \"buy\"))\nportfolio.add_trade(Trade(\"AAPL\", 25, 152.00, \"sell\"))\n\n# View summary\nportfolio.summary()\n\nPortfolio: My Research Portfolio\nTotal trades: 4\nTotal value: $47618.75\n\nPositions:\n  AAPL: 125 shares\n  MSFT: 75 shares\n\n\nThis example shows how OOP allows you to build up layers of abstraction. A Portfolio is a collection of Trade objects, and both have methods that make sense for their level of abstraction.\n\n\n\n\n\n\nTipWhen to Use Classes vs.¬†Functions\n\n\n\nDon‚Äôt create a class just to group functions together. If your class only has one or two methods and no meaningful state (attributes), it should probably just be a function. Classes are most useful when you need to maintain state across multiple operations.\n\n\n\n\n3.1.5 String Representations: __repr__ vs.¬†__str__\nPython provides two different methods for converting objects to strings: __repr__ and __str__. Understanding the difference between them helps you write more useful classes.\n\n__repr__ is meant to produce an unambiguous representation of the object, primarily for developers and debugging. Ideally, it should look like a valid Python expression that could recreate the object.\n__str__ is meant to produce a readable, user-friendly string. It‚Äôs what gets displayed when you use print() on an object.\n\nIf you only implement one, implement __repr__. Python will use it as a fallback for __str__ if __str__ isn‚Äôt defined. Here‚Äôs an example showing both:\n\nclass Trade:\n    def __init__(self, ticker: str, quantity: int, price: float, side: str):\n        self.ticker = ticker\n        self.quantity = quantity\n        self.price = price\n        self.side = side\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Unambiguous representation for developers.\"\"\"\n        return f\"Trade({self.ticker!r}, {self.quantity}, {self.price}, {self.side!r})\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"User-friendly representation.\"\"\"\n        action = \"Buy\" if self.side == \"buy\" else \"Sell\"\n        return f\"{action} {self.quantity} shares of {self.ticker} @ ${self.price:.2f}\"\n\ntrade = Trade(\"AAPL\", 100, 150.50, \"buy\")\n\n# __str__ is used by print()\nprint(trade)\n\n# __repr__ is used in the REPL and for debugging\nprint(repr(trade))\n\nBuy 100 shares of AAPL @ $150.50\nTrade('AAPL', 100, 150.5, 'buy')\n\n\n\n\n3.1.6 Rich Display in Jupyter and Quarto\nJupyter notebooks (and Quarto, a publishing system for creating documents from notebooks and other sources) support special methods for rich display. These methods allow your objects to render as HTML, Markdown, or LaTeX instead of plain text:\n\n_repr_html_() returns HTML that will be rendered in the notebook\n_repr_markdown_() returns Markdown text\n_repr_latex_() returns LaTeX for mathematical notation\n\nHere‚Äôs a simple example:\n\nclass Trade:\n    def __init__(self, ticker: str, quantity: int, price: float, side: str):\n        self.ticker = ticker\n        self.quantity = quantity\n        self.price = price\n        self.side = side\n\n    def __repr__(self) -&gt; str:\n        return f\"Trade({self.ticker!r}, {self.quantity}, {self.price}, {self.side!r})\"\n\n    def _repr_html_(self) -&gt; str:\n        \"\"\"Rich HTML display for Jupyter/Quarto.\"\"\"\n        color = \"green\" if self.side == \"buy\" else \"red\"\n        return f\"\"\"\n        &lt;div style=\"border: 1px solid #ccc; padding: 10px; border-radius: 5px; width: fit-content;\"&gt;\n            &lt;strong&gt;{self.ticker}&lt;/strong&gt;&lt;br&gt;\n            &lt;span style=\"color: {color};\"&gt;{self.side.upper()}&lt;/span&gt;\n            {self.quantity} shares @ ${self.price:.2f}\n        &lt;/div&gt;\n        \"\"\"\n\n    def _repr_latex_(self) -&gt; str:\n        \"\"\"Rich LaTeX display for PDF output.\"\"\"\n        action = \"Buy\" if self.side == \"buy\" else \"Sell\"\n        return (\n            rf\"\\textbf{{{self.ticker}}}: \"\n            rf\"{action} {self.quantity} shares @ \\${self.price:.2f}\"\n        )\n\ntrade = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade  # In Jupyter/Quarto, this displays as formatted HTML or LaTeX\n\n\n        \n            AAPL\n            BUY\n            100 shares @ $150.50\n        \n        \n\n\nMany of the data analysis libraries you‚Äôll use later in this course, such as pandas, use these methods to display data frames as nicely formatted tables.\n\n\n3.1.7 Class Variables vs.¬†Instance Variables\nSo far, we‚Äôve been working with instance variables‚Äîattributes that are unique to each object. Python also supports class variables, which are shared by all instances of a class:\n\nclass Trade:\n    # Class variable\n    commission_rate = 0.001  # 0.1% commission\n\n    def __init__(self, ticker, quantity, price, side):\n        # Instance variables\n        self.ticker = ticker\n        self.quantity = quantity\n        self.price = price\n        self.side = side\n\n    def value(self):\n        \"\"\"Calculate the total value of the trade.\"\"\"\n        return self.quantity * self.price\n\n    def net_value(self):\n        \"\"\"Calculate value after commission.\"\"\"\n        gross_value = self.value()\n        commission = gross_value * Trade.commission_rate\n        return gross_value - commission\n\n    def __repr__(self):\n        return f\"Trade({self.ticker}, {self.quantity}, ${self.price:.2f}, {self.side})\"\n\n# All trades share the same commission rate\ntrade1 = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade2 = Trade(\"MSFT\", 50, 280.25, \"sell\")\n\nprint(f\"Trade 1 net value: ${trade1.net_value():.2f}\")\nprint(f\"Trade 2 net value: ${trade2.net_value():.2f}\")\n\n# Changing the class variable affects all instances\nTrade.commission_rate = 0.002\nprint(f\"Trade 1 net value (new rate): ${trade1.net_value():.2f}\")\n\nTrade 1 net value: $15034.95\nTrade 2 net value: $13998.49\nTrade 1 net value (new rate): $15019.90\n\n\nClass variables are useful for values that should be consistent across all instances, like constants, default settings, or shared configuration.\n\n\n3.1.8 Property Decorators\nSometimes you want to compute a value on-the-fly rather than storing it as an attribute. Python‚Äôs @property decorator makes this look like a simple attribute access:\n\nclass Trade:\n    def __init__(self, ticker, quantity, price, side):\n        self.ticker = ticker\n        self.quantity = quantity\n        self.price = price\n        self.side = side\n\n    @property\n    def value(self):\n        \"\"\"Calculate the total value of the trade.\"\"\"\n        return self.quantity * self.price\n\n    @property\n    def is_buy(self):\n        \"\"\"Check if this is a buy trade.\"\"\"\n        return self.side == \"buy\"\n\n    def __repr__(self):\n        return f\"Trade({self.ticker}, {self.quantity}, ${self.price:.2f}, {self.side})\"\n\ntrade = Trade(\"AAPL\", 100, 150.50, \"buy\")\n\n# No parentheses needed - looks like an attribute\nprint(f\"Trade value: ${trade.value:.2f}\")\nprint(f\"Is buy: {trade.is_buy}\")\n\nTrade value: $15050.00\nIs buy: True\n\n\nThe advantage of using @property is that it allows you to start with a simple attribute and later change it to a computed value without changing how the class is used. It also makes the code more readable when the value is conceptually an attribute rather than an action.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Object-Oriented Programming Basics</span>"
    ]
  },
  {
    "objectID": "python/oop/index.html#when-oop-is-useful-in-research-code",
    "href": "python/oop/index.html#when-oop-is-useful-in-research-code",
    "title": "3¬† Object-Oriented Programming Basics",
    "section": "3.2 When OOP is Useful in Research Code",
    "text": "3.2 When OOP is Useful in Research Code\nNow that you understand the basics of classes and objects, an important question remains: when should you actually use OOP in your research code?\nThe truth is, many data analysis tasks in empirical finance don‚Äôt require OOP. A straightforward script that loads data, performs some analysis, and generates output can be perfectly fine without defining any classes. In fact, overusing OOP can make simple tasks more complicated than they need to be.\nHowever, there are several scenarios where OOP becomes genuinely useful in research code.\n\n3.2.1 Scenario 1: Managing Complex State\nIf you find yourself passing around many related variables to multiple functions, a class might be appropriate. Consider a simulation configuration that needs to be loaded, validated, and used across multiple functions:\nWithout OOP:\n\nimport json\nfrom pathlib import Path\n\ndef load_simulation_config(filepath):\n    with open(filepath) as f:\n        data = json.load(f)\n    return {\n        \"name\": data[\"name\"],\n        \"n_simulations\": data[\"n_simulations\"],\n        \"initial_value\": data[\"initial_value\"],\n        \"drift\": data[\"drift\"],\n        \"volatility\": data[\"volatility\"],\n        \"results\": None\n    }\n\ndef validate_config(config):\n    if config[\"n_simulations\"] &lt;= 0:\n        raise ValueError(\"n_simulations must be positive\")\n    if config[\"initial_value\"] &lt;= 0:\n        raise ValueError(\"initial_value must be positive\")\n    if config[\"volatility\"] &lt; 0:\n        raise ValueError(\"volatility must be non-negative\")\n    return config\n\ndef run_simulation(config):\n    if config[\"n_simulations\"] &lt;= 0:\n        raise ValueError(\"Invalid config\")\n    # Simulation logic would go here...\n    config[\"results\"] = {\"mean\": 105.2, \"std\": 12.3}\n    return config\n\n# Usage - must remember the correct sequence of function calls\nconfig = load_simulation_config(\"sim_config.json\")\nconfig = validate_config(config)\nconfig = run_simulation(config)\nprint(config[\"results\"])\n\nWith OOP:\n\nimport json\nfrom pathlib import Path\n1import pprint\n\nclass SimulationConfig:\n    def __init__(\n        self,\n        name: str,\n        n_simulations: int,\n        initial_value: float,\n        drift: float,\n        volatility: float,\n    ):\n        self.name = name\n        self.n_simulations = n_simulations\n        self.initial_value = initial_value\n        self.drift = drift\n        self.volatility = volatility\n        self.results: dict | None = None\n\n        # Validation happens automatically on creation\n        self._validate()\n\n    def _validate(self) -&gt; None:\n        \"\"\"Validate the configuration parameters.\"\"\"\n        if self.n_simulations &lt;= 0:\n            raise ValueError(\"n_simulations must be positive\")\n        if self.initial_value &lt;= 0:\n            raise ValueError(\"initial_value must be positive\")\n        if self.volatility &lt; 0:\n            raise ValueError(\"volatility must be non-negative\")\n\n2    @classmethod\n    def from_json(cls, filepath: str | Path) -&gt; \"SimulationConfig\":\n        \"\"\"Create a SimulationConfig from a JSON file.\"\"\"\n        with open(filepath) as f:\n            data = json.load(f)\n        return cls(\n            name=data[\"name\"],\n            n_simulations=data[\"n_simulations\"],\n            initial_value=data[\"initial_value\"],\n            drift=data[\"drift\"],\n            volatility=data[\"volatility\"],\n        )\n\n    def run(self) -&gt; \"SimulationConfig\":\n        \"\"\"Run the simulation.\"\"\"\n        # Simulation logic would go here...\n        self.results = {\"mean\": 105.2, \"std\": 12.3}\n        return self\n\n    def __repr__(self) -&gt; str:\n        status = \"completed\" if self.results else \"not run\"\n        return f\"SimulationConfig({self.name!r}, {self.n_simulations} sims, {status})\"\n\n\n1\n\nThe pprint (pretty print) module from Python‚Äôs standard library formats complex data structures like dictionaries and lists in a more readable way, with proper indentation and line breaks. This is especially useful when displaying nested structures or long lists.\n\n2\n\nThe @classmethod decorator creates a class method‚Äîa method that receives the class itself (conventionally named cls) as its first argument instead of an instance. Class methods are often used as alternative constructors, like from_json() here. In contrast, a @staticmethod doesn‚Äôt receive any implicit first argument and behaves like a regular function that happens to live inside a class.\n\n\n\n\nNow we can use the class:\n\n# Load from file using the class method\nconfig = SimulationConfig.from_json(\"sim_config.json\")\n\n# Or create directly\nconfig = SimulationConfig(\n    name=\"Test Simulation\",\n    n_simulations=1000,\n    initial_value=100.0,\n    drift=0.05,\n    volatility=0.2,\n)\n\n# Run and display results\nconfig.run()\npprint.pprint(config.results)  # Pretty print the results dictionary\n\n\n\n{'mean': 105.2, 'std': 12.3}\n\n\nThe OOP version is cleaner because the state is bundled together, and you don‚Äôt need to pass around a configuration dictionary. The simulation object maintains its own state, making the code more organized and less error-prone.\nBeyond reducing errors, the class also makes your code more clearly defined. With a dictionary, nothing prevents you from accessing a misspelled key like config[\"n_simulaitons\"]‚Äîyou‚Äôll only discover the typo at runtime. With a class, your editor (like VS Code) can immediately flag config.n_simulaitons as an error because it knows exactly which attributes SimulationConfig has. This kind of immediate feedback makes development faster and catches bugs before you even run the code.\n\n\n3.2.2 Scenario 2: Multiple Related Variants\nIf you need to implement several variants of a similar concept, OOP with inheritance can reduce code duplication. For example, different return calculation methods:\n\nimport math\n\nclass Returns:\n    \"\"\"Base class for return calculations.\"\"\"\n\n    def __init__(self, prices: list[float]):\n        self.prices = prices\n\n    def calculate(self) -&gt; list[float]:\n        raise NotImplementedError(\"Subclasses must implement calculate()\")\n\nclass SimpleReturns(Returns):\n    \"\"\"Calculate simple returns: (P_t / P_{t-1}) - 1\"\"\"\n\n    def calculate(self) -&gt; list[float]:\n        return [\n            (self.prices[i] / self.prices[i - 1]) - 1\n            for i in range(1, len(self.prices))\n        ]\n\nclass LogReturns(Returns):\n    \"\"\"Calculate log returns: log(P_t / P_{t-1})\"\"\"\n\n    def calculate(self) -&gt; list[float]:\n        return [\n            math.log(self.prices[i] / self.prices[i - 1])\n            for i in range(1, len(self.prices))\n        ]\n\nclass ExcessReturns(Returns):\n    \"\"\"Calculate excess returns over risk-free rate.\"\"\"\n\n    def __init__(self, prices: list[float], risk_free_rate: float):\n        super().__init__(prices)\n        self.risk_free_rate = risk_free_rate\n\n    def calculate(self) -&gt; list[float]:\n        simple_returns = [\n            (self.prices[i] / self.prices[i - 1]) - 1\n            for i in range(1, len(self.prices))\n        ]\n        return [r - self.risk_free_rate for r in simple_returns]\n\n# Usage\nprices = [100.0, 102.0, 101.0, 105.0, 108.0]\n\nsimple = SimpleReturns(prices)\nprint(\"Simple returns:\", [f\"{r:.4f}\" for r in simple.calculate()])\n\nlog_ret = LogReturns(prices)\nprint(\"Log returns:\", [f\"{r:.4f}\" for r in log_ret.calculate()])\n\nexcess = ExcessReturns(prices, risk_free_rate=0.001)\nprint(\"Excess returns:\", [f\"{r:.4f}\" for r in excess.calculate()])\n\nSimple returns: ['0.0200', '-0.0098', '0.0396', '0.0286']\nLog returns: ['0.0198', '-0.0099', '0.0388', '0.0282']\nExcess returns: ['0.0190', '-0.0108', '0.0386', '0.0276']\n\n\nThis pattern is useful when you want to ensure different variants share a common interface or when you want to write code that works with any of the variants.\n\n\n\n\n\n\nWarningDon‚Äôt Overuse Inheritance\n\n\n\nInheritance can create tight coupling between classes and make code harder to understand. Often, composition (having one class use another as an attribute) is a better choice. Only use inheritance when you have a genuine ‚Äúis-a‚Äù relationship and need to substitute one type for another.\nFor cases where you want classes to share a common interface without inheritance, Python 3.8+ offers Protocols (from the typing module). A Protocol defines what methods and attributes a class should have, without requiring the class to explicitly inherit from anything. This is sometimes called ‚Äústructural subtyping‚Äù or ‚Äúduck typing with type hints.‚Äù\n\n\n\n\n3.2.3 Scenario 3: Encapsulating Complex Data Structures\nWhen working with complex data structures that need validation or computed properties, classes provide a clean way to manage this complexity:\n\nclass EventStudyWindow:\n    \"\"\"Represents an event study window with validation.\"\"\"\n\n    def __init__(self, event_date, estimation_start, estimation_end,\n                 event_start, event_end):\n        self.event_date = event_date\n        self.estimation_start = estimation_start\n        self.estimation_end = estimation_end\n        self.event_start = event_start\n        self.event_end = event_end\n\n        # Validate the window\n        self._validate()\n\n    def _validate(self):\n        \"\"\"Validate that the window makes sense.\"\"\"\n        if self.estimation_end &gt;= self.event_date:\n            raise ValueError(\"Estimation window must end before event date\")\n\n        if self.event_start &gt; self.event_date:\n            raise ValueError(\"Event window start must be at or before event date\")\n\n        if self.event_end &lt; self.event_date:\n            raise ValueError(\"Event window end must be at or after event date\")\n\n    @property\n    def estimation_length(self):\n        \"\"\"Length of the estimation window in days.\"\"\"\n        return (self.estimation_end - self.estimation_start).days\n\n    @property\n    def event_length(self):\n        \"\"\"Length of the event window in days.\"\"\"\n        return (self.event_end - self.event_start).days\n\n    def __repr__(self):\n        return (f\"EventStudyWindow(event={self.event_date}, \"\n                f\"estimation={self.estimation_length} days, \"\n                f\"event_window={self.event_length} days)\")\n\n# Usage\nfrom datetime import date, timedelta\n\nevent_date = date(2024, 6, 15)\nwindow = EventStudyWindow(\n    event_date=event_date,\n    estimation_start=event_date - timedelta(days=260),\n    estimation_end=event_date - timedelta(days=10),\n    event_start=event_date - timedelta(days=1),\n    event_end=event_date + timedelta(days=1)\n)\n\nprint(window)\nprint(f\"Estimation period: {window.estimation_length} days\")\nprint(f\"Event window: {window.event_length} days\")\n\nEventStudyWindow(event=2024-06-15, estimation=250 days, event_window=2 days)\nEstimation period: 250 days\nEvent window: 2 days\n\n\nThe class encapsulates both the data and the logic for validation and computation, making it easier to work with event study windows correctly.\n\n\n3.2.4 Scenario 4: Building Reusable Components\nIf you‚Äôre building functionality that will be reused across multiple projects, classes provide a clean interface:\n\nimport statistics\nimport random\n\nclass RollingWindow:\n    \"\"\"Calculate rolling window statistics.\"\"\"\n\n    def __init__(self, data: list[float], window_size: int):\n        self.data = data\n        self.window_size = window_size\n\n        if len(self.data) &lt; window_size:\n            raise ValueError(\"Data must be longer than window size\")\n\n    def mean(self) -&gt; list[float]:\n        \"\"\"Calculate rolling mean.\"\"\"\n        return [\n            statistics.mean(self.data[i : i + self.window_size])\n            for i in range(len(self) )\n        ]\n\n    def std(self) -&gt; list[float]:\n        \"\"\"Calculate rolling standard deviation.\"\"\"\n        return [\n            statistics.stdev(self.data[i : i + self.window_size])\n            for i in range(len(self))\n        ]\n\n    def sharpe(self, risk_free_rate: float = 0) -&gt; list[float]:\n        \"\"\"Calculate rolling Sharpe ratio.\"\"\"\n        means = self.mean()\n        stds = self.std()\n        return [(m - risk_free_rate) / s for m, s in zip(means, stds)]\n\n    def __len__(self) -&gt; int:\n        return len(self.data) - self.window_size + 1\n\n    def __repr__(self) -&gt; str:\n        return f\"RollingWindow(data_length={len(self.data)}, window={self.window_size})\"\n\n# Generate some sample returns\nrandom.seed(42)\nreturns = [random.gauss(0.001, 0.02) for _ in range(100)]\nrolling = RollingWindow(returns, window_size=10)\n\nprint(f\"Rolling windows: {len(rolling)}\")\nprint(f\"Mean rolling mean: {statistics.mean(rolling.mean()):.4f}\")\nprint(f\"Mean rolling Sharpe: {statistics.mean(rolling.sharpe()):.4f}\")\n\nRolling windows: 91\nMean rolling mean: 0.0019\nMean rolling Sharpe: 0.1290\n\n\n\n\n3.2.5 When to Avoid OOP\nJust as important as knowing when to use OOP is knowing when not to use it. Avoid OOP when:\n\nYou‚Äôre doing one-off analysis: If you‚Äôre exploring data or doing a quick calculation, a simple script is fine.\nYour code is primarily a sequence of transformations: Data pipelines that transform data step-by-step are often clearer as functions rather than classes.\nYou‚Äôre wrapping a single function: Don‚Äôt create a class with only one method. Just use a function.\nIt makes the code more complex: If OOP is making your code harder to understand, you‚Äôre probably not in a situation where it helps.\n\nRemember: the goal is clarity and maintainability, not using OOP for its own sake.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Object-Oriented Programming Basics</span>"
    ]
  },
  {
    "objectID": "python/oop/index.html#data-classes",
    "href": "python/oop/index.html#data-classes",
    "title": "3¬† Object-Oriented Programming Basics",
    "section": "3.3 Data Classes",
    "text": "3.3 Data Classes\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video provides a good introduction to data classes.\n\n\n\nPython 3.7 introduced data classes, which provide a streamlined way to create classes that are primarily used to store data. They automatically generate common methods like __init__, __repr__, and __eq__, reducing boilerplate code significantly.\n\n3.3.1 Basic Data Classes\nLet‚Äôs revisit our Trade class, but this time using a data class:\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass Trade:\n    ticker: str\n    quantity: int\n    price: float\n    side: str\n\n    @property\n    def value(self) -&gt; float:\n        \"\"\"Calculate the total value of the trade.\"\"\"\n        return self.quantity * self.price\n\n# Create trades\ntrade1 = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade2 = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade3 = Trade(\"MSFT\", 50, 280.25, \"sell\")\n\nprint(trade1)\nprint(f\"Value: ${trade1.value:.2f}\") \nprint(f\"trade1 == trade2: {trade1 == trade2}\")\nprint(f\"trade1 == trade3: {trade1 == trade3}\")\n\nTrade(ticker='AAPL', quantity=100, price=150.5, side='buy')\nValue: $15050.00\ntrade1 == trade2: True\ntrade1 == trade3: False\n\n\nWith just the @dataclass decorator and type annotations, we get:\n\nAn __init__ method that accepts all the attributes\nA __repr__ method that shows a useful string representation\nAn __eq__ method that compares instances by their attributes\n\nThis is much less code than writing these methods manually, and it‚Äôs less error-prone.\n\n\n3.3.2 Default Values\nData classes make it easy to specify default values:\n\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass Trade:\n    ticker: str\n    quantity: int\n    price: float\n    side: str = \"buy\"  # default value\n    commission: float = 0.0\n    notes: Optional[str] = None\n\n    def value(self):\n        \"\"\"Calculate the total value of the trade.\"\"\"\n        return self.quantity * self.price\n\n    def net_value(self):\n        \"\"\"Calculate value after commission.\"\"\"\n        return self.value() - self.commission\n\n# Use defaults\ntrade1 = Trade(\"AAPL\", 100, 150.50)\nprint(trade1)\n\n# Override defaults\ntrade2 = Trade(\"MSFT\", 50, 280.25, side=\"sell\", commission=14.00)\nprint(trade2)\nprint(f\"Net value: ${trade2.net_value():.2f}\")\n\nTrade(ticker='AAPL', quantity=100, price=150.5, side='buy', commission=0.0, notes=None)\nTrade(ticker='MSFT', quantity=50, price=280.25, side='sell', commission=14.0, notes=None)\nNet value: $13998.50\n\n\n\n\n3.3.3 Immutable Data Classes\nYou can make a data class immutable by setting frozen=True. This means that once created, the attributes cannot be changed:\n\nfrom dataclasses import dataclass\n\n@dataclass(frozen=True)\nclass Trade:\n    ticker: str\n    quantity: int\n    price: float\n    side: str\n\n    def value(self):\n        return self.quantity * self.price\n\ntrade = Trade(\"AAPL\", 100, 150.50, \"buy\")\nprint(trade)\n\n# This would raise an error:\n# trade.price = 160.00  # FrozenInstanceError\n\nTrade(ticker='AAPL', quantity=100, price=150.5, side='buy')\n\n\nImmutable data classes are useful when you want to ensure that data doesn‚Äôt change unexpectedly, or when you need to use instances as dictionary keys or in sets.\n\n\n3.3.4 Data Classes vs.¬†Regular Classes\nWhen should you use a data class instead of a regular class?\nUse data classes when:\n\nYour class is primarily for storing data\nYou want automatic generation of common methods\nYou want type hints for all attributes\nYou need value-based equality (comparing by content, not identity)\n\nUse regular classes when:\n\nYou need more control over initialization\nThe class has complex behavior with little data\nYou need inheritance from non-dataclass parents\n\n\n\n3.3.5 Data Classes and Type Checking\nData classes work particularly well with static type checkers. The type annotations are not just documentation‚Äîthey can be validated by tools like ty (a fast type checker from Astral, the creators of uv and ruff) or directly in VS Code.\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass Trade:\n    ticker: str\n    quantity: int\n    price: float\n    side: str\n\n# These work fine\ntrade1 = Trade(\"AAPL\", 100, 150.50, \"buy\")\ntrade2 = Trade(ticker=\"MSFT\", quantity=50, price=280.25, side=\"sell\")\n\n# Runtime Python won't stop these, but type checkers will flag them:\n# trade3 = Trade(ticker=\"AAPL\", quantity=\"100\", price=150.50, side=\"buy\")  # wrong type\n# trade4 = Trade(\"AAPL\", 100, 150.50)  # missing argument\n\nThe real advantage is that VS Code (with the Python or Pylance extension) can highlight these errors as you type, before you even save the file. This immediate feedback helps catch bugs early and makes development faster.\n\n\n\n\n\n\nTipPydantic for Data Validation\n\n\n\nIf you need runtime data validation (not just static type checking), consider Pydantic. It‚Äôs a third-party library that offers functionality similar to dataclasses but validates data types at runtime, converts values to the correct types when possible, and provides detailed error messages when validation fails. Pydantic is particularly useful when working with external data sources like JSON files or API responses.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Object-Oriented Programming Basics</span>"
    ]
  },
  {
    "objectID": "python/code-quality/index.html",
    "href": "python/code-quality/index.html",
    "title": "4¬† Code Quality and Documentation",
    "section": "",
    "text": "4.1 Code Organization and Readability\nWriting code is like writing prose for a dual audience: computers that execute it and humans who read, maintain, and extend it. While any code that runs correctly serves its immediate purpose, the real measure of quality lies in how easily others (including your future self) can understand, trust, and build upon your work.\nIn empirical research, where reproducibility and transparency are paramount, code quality takes on additional importance. A subtle bug in your data processing pipeline can invalidate months of work. Poorly documented functions can make it impossible for reviewers to verify your methodology. Code that works but cannot be understood becomes a liability rather than an asset.\nThis chapter covers the practical tools and techniques for writing clean, clear, readable, reproducible, and reliable code. We will explore how to organize your code for readability, document it effectively, catch errors before they cause problems, and maintain consistent style across your projects. These practices are not about perfectionism‚Äîthey are about making your research more reliable, your collaboration more effective, and your future work easier.\nThe foundation of code quality is organization. Well-organized code reveals its structure and intent at a glance, making it easier to navigate, debug, and modify. This section covers the principles that make code readable and the practical techniques for achieving them.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Code Quality and Documentation</span>"
    ]
  },
  {
    "objectID": "python/code-quality/index.html#code-organization-and-readability",
    "href": "python/code-quality/index.html#code-organization-and-readability",
    "title": "4¬† Code Quality and Documentation",
    "section": "",
    "text": "4.1.1 The Principle of Least Surprise\nGood code should behave the way readers expect. This means following established conventions, using descriptive names, and structuring your logic in clear, predictable ways. When you need to deviate from conventions, document why.\nConsider two approaches to calculating portfolio returns:\n# Unclear: cryptic names and unexpected structure\ndef calc(d, w):\n    r = []\n    for i in range(len(d)):\n        r.append(sum([d[i][j] * w[j] for j in range(len(w))]))\n    return r\n\n# Clear: descriptive names and explicit structure\ndef calculate_portfolio_returns(asset_returns, weights):\n    \"\"\"Calculate portfolio returns given asset returns and weights.\"\"\"\n    portfolio_returns = []\n    for period_returns in asset_returns:\n        period_portfolio_return = sum(\n            asset_return * weight\n            for asset_return, weight in zip(period_returns, weights)\n        )\n        portfolio_returns.append(period_portfolio_return)\n    return portfolio_returns\nThe second version is longer, but its intent is immediately clear. The function name describes what it does, parameter names indicate what inputs are expected, and the logic is explicit rather than compressed.\n\n\n4.1.2 Project Directory Structure\nA well-organized directory structure makes projects easier to navigate and understand. For empirical research projects, we recommend the following layout:\nmy-project/\n1‚îú‚îÄ‚îÄ conf/\n‚îÇ   ‚îî‚îÄ‚îÄ config.yaml\n2‚îú‚îÄ‚îÄ data/\n3‚îÇ   ‚îú‚îÄ‚îÄ raw/\n4‚îÇ   ‚îú‚îÄ‚îÄ clean/\n5‚îÇ   ‚îî‚îÄ‚îÄ results/\n6‚îú‚îÄ‚îÄ notebooks/\n7‚îú‚îÄ‚îÄ notes/\n8‚îú‚îÄ‚îÄ paper/\n9‚îú‚îÄ‚îÄ slides/\n10‚îú‚îÄ‚îÄ src/my_project/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n11‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py\n12‚îÇ   ‚îî‚îÄ‚îÄ utils/\n13‚îú‚îÄ‚îÄ tests/\n14‚îú‚îÄ‚îÄ .gitignore\n15‚îú‚îÄ‚îÄ pyproject.toml\n16‚îî‚îÄ‚îÄ README.md\n\n1\n\nConfiguration files for your analysis pipeline\n\n2\n\nAll data files, organized by processing stage\n\n3\n\nOriginal unprocessed data (never modify these files)\n\n4\n\nProcessed and cleaned datasets\n\n5\n\nAnalysis outputs like regression results\n\n6\n\nJupyter notebooks for exploration and prototyping\n\n7\n\nResearch notes and documentation\n\n8\n\nPaper manuscript (Quarto or LaTeX)\n\n9\n\nPresentation slides\n\n10\n\nMain Python package with your reusable code\n\n11\n\nMain analysis pipeline script\n\n12\n\nUtility functions and helpers\n\n13\n\nUnit tests for your code\n\n14\n\nFiles to exclude from version control\n\n15\n\nProject dependencies and metadata\n\n16\n\nProject overview and setup instructions\n\n\nThis structure separates concerns clearly: raw data stays pristine, processed data is reproducible, and code is organized into reusable modules. The src/ directory pattern keeps your package importable while maintaining a clean project root.\nWhen working with version control, we usually want to keep data and results in the different location. We discuss this in Chapter 8.\n\n\n4.1.3 Function Design\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video covers function design. Note that this is an external resource that may present concepts differently than those covered here.\n\n\n\nFunctions should do one thing well. A function that does multiple unrelated tasks is harder to test, harder to reuse, and harder to understand. Consider this example:\n# Poor: one function doing too much\ndef analyze_data(filepath):\n    # Read data\n    with open(filepath) as f:\n        lines = f.readlines()\n\n    # Parse and clean data\n    values = []\n    for line in lines:\n        parts = line.strip().split(',')\n        if len(parts) &gt;= 2 and parts[1]:\n            values.append(float(parts[1]))\n\n    # Calculate statistics\n    mean = sum(values) / len(values)\n    squared_diffs = [(x - mean) ** 2 for x in values]\n    std = (sum(squared_diffs) / len(values)) ** 0.5\n\n    # Save results\n    with open('stats.txt', 'w') as f:\n        f.write(f'Mean: {mean}\\nStd: {std}')\n\n    return values\n\n# Better: separate concerns into focused functions\ndef read_data_file(filepath):\n    \"\"\"Read lines from a data file.\"\"\"\n    with open(filepath) as f:\n        return f.readlines()\n\ndef parse_values(lines, column=1):\n    \"\"\"Extract numeric values from CSV lines.\"\"\"\n    values = []\n    for line in lines:\n        parts = line.strip().split(',')\n        if len(parts) &gt; column and parts[column]:\n            values.append(float(parts[column]))\n    return values\n\ndef calculate_mean(values):\n    \"\"\"Calculate the arithmetic mean of a list of numbers.\"\"\"\n    if not values:\n        raise ValueError(\"Cannot calculate mean of empty list\")\n    return sum(values) / len(values)\n\ndef calculate_std(values):\n    \"\"\"Calculate the standard deviation of a list of numbers.\"\"\"\n    if len(values) &lt; 2:\n        raise ValueError(\"Need at least 2 values for standard deviation\")\n    mean = calculate_mean(values)\n    squared_diffs = [(x - mean) ** 2 for x in values]\n    return (sum(squared_diffs) / len(values)) ** 0.5\n\ndef save_statistics(stats, output_path):\n    \"\"\"Save statistics dictionary to a text file.\"\"\"\n    with open(output_path, 'w') as f:\n        for key, value in stats.items():\n            f.write(f'{key}: {value}\\n')\nThe refactored version is more verbose, but each function is now:\n\nTestable: You can verify each step independently\nReusable: Functions can be used in other analyses\nReadable: Each function has a clear, single purpose\nMaintainable: Changes to one step don‚Äôt affect others\n\n\n\n4.1.4 Managing Complexity with Abstraction\nAs your analysis grows more sophisticated, you will build up layers of abstraction. Lower-level functions handle details; higher-level functions orchestrate workflow:\n# Low-level: handle specific calculations\ndef calculate_mean(values):\n    \"\"\"Calculate arithmetic mean.\"\"\"\n    return sum(values) / len(values)\n\ndef calculate_variance(values):\n    \"\"\"Calculate population variance.\"\"\"\n    mean = calculate_mean(values)\n    squared_diffs = [(x - mean) ** 2 for x in values]\n    return sum(squared_diffs) / len(values)\n\ndef calculate_covariance(x_values, y_values):\n    \"\"\"Calculate covariance between two lists of values.\"\"\"\n    if len(x_values) != len(y_values):\n        raise ValueError(\"Lists must have same length\")\n    x_mean = calculate_mean(x_values)\n    y_mean = calculate_mean(y_values)\n    products = [(x - x_mean) * (y - y_mean) for x, y in zip(x_values, y_values)]\n    return sum(products) / len(x_values)\n\n# Mid-level: combine calculations\ndef calculate_descriptive_stats(values):\n    \"\"\"Calculate common descriptive statistics.\"\"\"\n    mean = calculate_mean(values)\n    variance = calculate_variance(values)\n    std = variance ** 0.5\n    return {'mean': mean, 'variance': variance, 'std': std}\n\n# High-level: orchestrate entire analysis\ndef analyze_dataset(filepath, column=1):\n    \"\"\"\n    Perform comprehensive analysis of a data file.\n\n    Reads data, calculates descriptive statistics, and\n    returns a complete summary.\n    \"\"\"\n    lines = read_data_file(filepath)\n    values = parse_values(lines, column)\n    stats = calculate_descriptive_stats(values)\n    stats['n'] = len(values)\n    stats['min'] = min(values)\n    stats['max'] = max(values)\n    return stats\n\n\n4.1.5 Code Layout and Readability\nPython‚Äôs readability comes partly from its use of whitespace. Use it deliberately:\n# Cramped and hard to parse\ndef process_records(data,filters=None,transform=True):\n    if filters is not None:data=[x for x in data if all(f(x) for f in filters)]\n    if transform:data=[{'id':x['id'],'value':x['amount']*100} for x in data]\n    return data\n\n# Readable with proper spacing\ndef process_records(data, filters=None, transform=True):\n    \"\"\"Process records with optional filtering and transformation.\"\"\"\n    if filters is not None:\n        data = [x for x in data if all(f(x) for f in filters)]\n\n    if transform:\n        data = [\n            {'id': x['id'], 'value': x['amount'] * 100}\n            for x in data\n        ]\n\n    return data\nGuidelines for spacing:\n\nUse blank lines to separate logical sections\nAdd spaces around operators (=, +, ==, etc.)\nAvoid spaces immediately inside parentheses or brackets\nGroup related items visually\n\nWe discuss how this can be automated in Section 4.4.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Code Quality and Documentation</span>"
    ]
  },
  {
    "objectID": "python/code-quality/index.html#docstrings-and-documentation-standards",
    "href": "python/code-quality/index.html#docstrings-and-documentation-standards",
    "title": "4¬† Code Quality and Documentation",
    "section": "4.2 Docstrings and Documentation Standards",
    "text": "4.2 Docstrings and Documentation Standards\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video covers code documentation. Note that this is an external resource that may present concepts differently than those covered here.\n\n\n\nDocumentation bridges the gap between what your code does and what users (including future you) need to know to use it correctly. In Python, this documentation primarily takes the form of docstrings‚Äîstring literals that appear as the first statement in a module, class, or function.\n\n4.2.1 Why Docstrings Matter\nUnlike comments, which explain how code works, docstrings explain what code does and how to use it. They serve multiple purposes:\n\nIDE integration: Modern editors display docstrings as tooltips and in autocomplete\nGenerated documentation: Tools like Sphinx can extract docstrings to create HTML documentation\nInteractive help: The help() function displays docstrings in the Python REPL\nCode review: Reviewers can understand intent without reading implementation\nAI assistance: AI coding assistants use docstrings to understand your code and provide better suggestions\n\nConsider the difference:\n# Without docstring\ndef clip_values(values, lower, upper):\n    return [max(lower, min(upper, x)) for x in values]\n\n# With docstring\ndef clip_values(values, lower, upper):\n    \"\"\"\n    Limit values to fall within specified bounds.\n\n    Parameters\n    ----------\n    values : list of float\n        Data values to clip\n    lower : float\n        Lower bound (values below this are set to lower)\n    upper : float\n        Upper bound (values above this are set to upper)\n\n    Returns\n    -------\n    list of float\n        Clipped values with the same length as input\n\n    Examples\n    --------\n    &gt;&gt;&gt; clip_values([1, 5, 10, 15], lower=3, upper=12)\n    [3, 5, 10, 12]\n\n    Notes\n    -----\n    This function is useful for reducing the impact of outliers while\n    retaining more information than simple outlier removal.\n    \"\"\"\n    return [max(lower, min(upper, x)) for x in values]\nWhen you call help(clip_values) or hover over the function in VS Code, you will see the formatted docstring with all the information needed to use the function correctly.\nThere are multiple standard docstring styles, but the most relevant for research projects are the NumPy and Google styles. Both provide clear structure for documenting parameters, return values, and examples. AI assistants can help generate well-formatted docstrings, but you should always review the descriptions to ensure they accurately reflect what your code does.\n\n\n4.2.2 NumPy Documentation Style\nThe NumPy documentation style is the standard in the scientific Python community. It is more verbose than some alternatives, but its structured format makes it ideal for technical and scientific code.\nThe basic structure includes:\n\nOne-line summary: Brief description of what the function does\nExtended description (optional): Additional context and details\nParameters: Each parameter with its type and description\nReturns: What the function returns and its type\nExamples (optional): Usage examples with expected output\nNotes (optional): Additional information, warnings, or references\n\nHere‚Äôs a complete example:\ndef calculate_rolling_mean(values, window, min_periods=None):\n    \"\"\"\n    Calculate rolling mean over a sliding window.\n\n    Computes the arithmetic mean over a rolling window of specified size.\n    The function handles edge cases at the beginning of the series.\n\n    Parameters\n    ----------\n    values : list of float\n        Sequence of numeric values\n    window : int\n        Number of values to include in each rolling window\n    min_periods : int, optional\n        Minimum number of observations required to calculate mean.\n        If None, defaults to window size. Windows with fewer observations\n        will return None.\n\n    Returns\n    -------\n    list of float or None\n        Rolling mean values. Returns None for positions where there\n        are fewer than min_periods observations.\n\n    Raises\n    ------\n    ValueError\n        If window is less than 1\n    ValueError\n        If min_periods is greater than window\n\n    Examples\n    --------\n    &gt;&gt;&gt; values = [1.0, 2.0, 3.0, 4.0, 5.0]\n    &gt;&gt;&gt; calculate_rolling_mean(values, window=3)\n    [None, None, 2.0, 3.0, 4.0]\n    &gt;&gt;&gt; calculate_rolling_mean(values, window=3, min_periods=1)\n    [1.0, 1.5, 2.0, 3.0, 4.0]\n\n    Notes\n    -----\n    The rolling mean at position i is calculated as the average of values\n    from position (i - window + 1) to i, inclusive.\n\n    See Also\n    --------\n    calculate_mean : Calculate mean of entire sequence\n    calculate_rolling_std : Calculate rolling standard deviation\n    \"\"\"\n    if window &lt; 1:\n        raise ValueError(\"window must be at least 1\")\n\n    if min_periods is None:\n        min_periods = window\n\n    if min_periods &gt; window:\n        raise ValueError(\"min_periods cannot exceed window\")\n\n    result = []\n    for i in range(len(values)):\n        start = max(0, i - window + 1)\n        window_values = values[start:i + 1]\n        if len(window_values) &gt;= min_periods:\n            result.append(sum(window_values) / len(window_values))\n        else:\n            result.append(None)\n\n    return result\n\n\n4.2.3 Google Documentation Style\nGoogle style is an alternative that is more compact while still providing structure. It uses indented sections rather than underlined headers:\ndef normalize_values(values, method='zscore'):\n    \"\"\"Normalize a list of values using the specified method.\n\n    Transforms values to have comparable scales, which is useful for\n    combining variables measured in different units.\n\n    Args:\n        values (list of float): Numeric values to normalize\n        method (str): Normalization method. Use 'zscore' for zero mean\n            and unit variance, 'minmax' for scaling to [0, 1] range.\n\n    Returns:\n        list of float: Normalized values\n\n    Raises:\n        ValueError: If values has zero standard deviation (for zscore)\n        ValueError: If all values are equal (for minmax)\n        ValueError: If method is not recognized\n\n    Example:\n        &gt;&gt;&gt; data = [10, 20, 30, 40, 50]\n        &gt;&gt;&gt; normalize_values(data, method='minmax')\n        [0.0, 0.25, 0.5, 0.75, 1.0]\n    \"\"\"\n    if method == 'zscore':\n        mean = sum(values) / len(values)\n        squared_diffs = [(x - mean) ** 2 for x in values]\n        std = (sum(squared_diffs) / len(values)) ** 0.5\n        if std == 0:\n            raise ValueError(\"Cannot zscore normalize: zero standard deviation\")\n        return [(x - mean) / std for x in values]\n\n    elif method == 'minmax':\n        min_val, max_val = min(values), max(values)\n        if min_val == max_val:\n            raise ValueError(\"Cannot minmax normalize: all values are equal\")\n        return [(x - min_val) / (max_val - min_val) for x in values]\n\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\nFor this course, we recommend using NumPy style for longer, more complex functions and Google style for simpler utilities. The key is to be consistent within a project. The docstring standards also define conventions for module-level and class-level docstrings, which follow similar patterns.\n\n\n\n\n\n\nTipDocumentation and Research Transparency\n\n\n\nIn empirical research, good documentation is not just helpful‚Äîit is essential for reproducibility. When writing up your analysis, you should be able to point reviewers to specific, well-documented functions that implement your methodology. This makes peer review more effective and helps establish trust in your results.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Code Quality and Documentation</span>"
    ]
  },
  {
    "objectID": "python/code-quality/index.html#type-hints-and-static-typing",
    "href": "python/code-quality/index.html#type-hints-and-static-typing",
    "title": "4¬† Code Quality and Documentation",
    "section": "4.3 Type Hints and Static Typing",
    "text": "4.3 Type Hints and Static Typing\nPython is a dynamically typed language, meaning you do not need to declare variable types. However, Python 3.5+ supports optional type hints that document expected types without changing runtime behavior. Type hints improve code quality by:\n\nMaking function interfaces explicit and self-documenting\nEnabling static analysis tools to catch type errors before runtime\nImproving IDE autocomplete and error detection\nServing as machine-checked documentation\nProviding additional context to AI coding assistants\n\n\n4.3.1 Basic Type Hints\nType hints specify the expected type of variables, parameters, and return values:\ndef calculate_return(initial_price: float, final_price: float) -&gt; float:\n    \"\"\"Calculate simple return between two prices.\"\"\"\n    return (final_price - initial_price) / initial_price\n\n\ndef read_config(filepath: str) -&gt; dict[str, str]:\n    \"\"\"Read configuration from a file.\"\"\"\n    config = {}\n    with open(filepath) as f:\n        for line in f:\n            key, value = line.strip().split('=')\n            config[key] = value\n    return config\nThe syntax parameter: type indicates the expected type, and -&gt; type indicates the return type.\n\n\n4.3.2 Common Type Hints\nHere are the type hints you will use most frequently:\ndef calculate_weighted_sum(\n    values: list[float],\n    weights: list[float]\n) -&gt; float:\n    \"\"\"Calculate weighted sum of values.\"\"\"\n    return sum(v * w for v, w in zip(values, weights))\n\n\ndef process_records(\n    records: list[dict[str, str]],\n    key: str = 'id'\n) -&gt; dict[str, dict[str, str]]:\n    \"\"\"Index records by a key field.\"\"\"\n    return {record[key]: record for record in records}\n\n\ndef calculate_statistics(\n    values: list[float]\n) -&gt; tuple[float, float, float]:\n    \"\"\"Calculate mean, min, and max.\"\"\"\n    mean = sum(values) / len(values)\n    return mean, min(values), max(values)\n\n\n\n\n\n\nNoteLegacy Type Hint Syntax\n\n\n\nWhen type hints were first introduced in Python 3.5, you had to import special types from the typing module like List, Dict, Tuple, and Union. Starting with Python 3.9+, you can use the built-in types directly: list[str] instead of List[str], dict[str, int] instead of Dict[str, int], and int | float instead of Union[int, float]. You will still see the old style in many code examples and libraries, but for new code, prefer the modern syntax.\n\n\n\n\n4.3.3 Optional and Union Types\nUse | None when a parameter might be None:\ndef calculate_mean(values: list[float], default: float | None = None) -&gt; float:\n    \"\"\"\n    Calculate mean of values, with optional default for empty lists.\n\n    Parameters\n    ----------\n    values : list of float\n        Values to average\n    default : float, optional\n        Value to return if list is empty. If None, raises ValueError.\n    \"\"\"\n    if not values:\n        if default is None:\n            raise ValueError(\"Cannot calculate mean of empty list\")\n        return default\n    return sum(values) / len(values)\nUse | (pipe) when a parameter can be one of several types:\ndef format_value(value: int | float | str) -&gt; str:\n    \"\"\"Format a value as a string with appropriate formatting.\"\"\"\n    if isinstance(value, float):\n        return f\"{value:.2f}\"\n    return str(value)\n\n\n4.3.4 Type Checking with ty\nType hints become even more valuable when combined with static type checkers. We recommend ty, a fast type checker from Astral (the same company behind uv and ruff). Run it with:\nuvx ty check your_script.py\nThe type checker will detect type inconsistencies:\ndef calculate_return(initial_price: float, final_price: float) -&gt; float:\n    return (final_price - initial_price) / initial_price\n\n# This will trigger a type error\nresult = calculate_return(\"100\", \"110\")  # Error: expected float, got str\nType hints do not affect runtime behavior; Python will not enforce types unless you use a type checker. This means type hints are documentation and analysis tools, not runtime constraints. You can gradually add type hints to your codebase without breaking existing code.\nFor practical use, VS Code can highlight type errors the same way Word highlights typos, which is very useful to catch bugs early on.\nFor research code, focus type hints where they add the most value: public function interfaces that others will use, complex data transformations where types clarify expected structures, and critical calculations where you want to make assumptions explicit. You do not need to type hint every variable in every function‚Äîuse judgment about where type information improves clarity.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Code Quality and Documentation</span>"
    ]
  },
  {
    "objectID": "python/code-quality/index.html#sec-ruff",
    "href": "python/code-quality/index.html#sec-ruff",
    "title": "4¬† Code Quality and Documentation",
    "section": "4.4 Code Style and Linting with ruff",
    "text": "4.4 Code Style and Linting with ruff\nConsistent code style makes collaboration easier and reduces cognitive load when reading code. Rather than debating style choices, the Python community has converged on automated tools that enforce consistent formatting. This section introduces ruff, the modern standard for Python code quality. Ruff is made by Astral, the same company behind uv and ty.\n\n4.4.1 Why Automated Formatting Matters\nManual formatting is time-consuming and leads to inconsistency. Different developers have different preferences for spacing, line breaks, and indentation. These differences create noise in version control, make code reviews harder, and waste mental energy on decisions that don‚Äôt affect functionality.\nAutomated formatters solve this by making formatting decisions for you. While you might not agree with every choice, the consistency and time savings far outweigh any aesthetic preferences. Additionally, when you get used to a specific style, it increases readability‚Äîyour eyes learn to scan consistently formatted code more quickly.\n\n\n4.4.2 ruff: The All-in-One Linter\nRuff is an extremely fast Python linter and code formatter written in Rust. It replaces multiple tools (flake8, isort, pyupgrade, and more) with a single, consistent interface. Ruff can:\n\nCheck for common errors and bugs\nEnforce code style guidelines\nSort and organize imports\nSuggest modernizations and improvements\nAutomatically fix many issues\n\nThe easiest way to use ruff is through the VS Code extension. Install the ‚ÄúRuff‚Äù extension from the VS Code marketplace, and VS Code can be configured to automatically format and fix your code each time you save. This makes code quality effortless‚Äîjust write your code and save.\nYou can also run ruff from the command line. Check your code with:\nuvx ruff check your_script.py\nRuff will identify issues:\n# example.py\nimport json\nimport os  # Unused import\n\ndef load_data(filepath):\n    with open(filepath) as f:\n        data = json.load(f)\n    return data\n\n# Unused variable\nresult = load_data(\"data.json\")\nRunning uvx ruff check example.py produces:\nexample.py:2:8: F401 [*] `os` imported but unused\nexample.py:10:1: F841 [*] Local variable `result` is assigned to but never used\nFound 2 errors.\n[*] 2 potentially fixable with the `--fix` option.\nAuto-fix issues:\nuvx ruff check --fix example.py\nRuff will automatically remove the unused import and variable.\n\n\n4.4.3 Configuring ruff\nConfigure ruff using a pyproject.toml file in your project root:\n[tool.ruff]\n# Set maximum line length (default is 88)\nline-length = 88\n\n# Target Python version\ntarget-version = \"py311\"\n\n[tool.ruff.lint]\n# Enable specific rule sets\nselect = [\n    \"E\",    # pycodestyle errors\n    \"F\",    # Pyflakes\n    \"I\",    # isort (import sorting)\n    \"B\",    # flake8-bugbear (common bugs)\n    \"SIM\",  # flake8-simplify\n    \"UP\",   # pyupgrade (modernize syntax)\n]\n\n# Disable specific rules if needed\nignore = [\n    \"E501\",  # Line too long (handled by formatter)\n]\n\n# Allow auto-fixing for these rule types\nfixable = [\"ALL\"]\n\n[tool.ruff.lint.per-file-ignores]\n# Allow unused imports in __init__.py files\n\"__init__.py\" = [\"F401\"]\n\n# Relaxed rules for test files\n\"tests/**/*.py\" = [\"S101\"]  # Allow assert statements\nThis configuration enables helpful checks while avoiding overly strict rules that might interfere with research workflows.\n\n\n4.4.4 Common ruff Rules\nSome particularly useful ruff rules:\nImport Organization (I)\nThis rule organizes imports in alphabetical order and grouping them in three groups: standard library imports, third-party imports, and imports from the current project. It will also automatically remove imports that are not used in the code.\nBefore ruff:\nimport json\nimport pandas as pd\nimport sys\nimport os\nfrom myproject.utils import helper\nimport csv\nAfter ruff with isort rules:\nimport csv\nimport json\nimport os\nimport sys\n\nimport pandas as pd\n\nfrom myproject.utils import helper\nBug Detection (B)\nRuff catches bugs such as the mutable default argument bug. When you use a mutable object like a list as a default argument, Python creates that object once when the function is defined‚Äînot each time the function is called. This means all calls share the same list, causing unexpected behavior where the list grows across calls.\nBefore (buggy):\ndef collect_values(value, results=[]):  # B006: Mutable default argument\n    results.append(value)\n    return results\nAfter (fixed):\ndef collect_values(value, results=None):\n    if results is None:\n        results = []\n    results.append(value)\n    return results\nCode Simplification (SIM)\nBefore:\nif condition:\n    return True\nelse:\n    return False\nAfter:\nreturn condition\n\n\n4.4.5 Formatting Code with ruff\nIn addition to linting, ruff includes a powerful code formatter. Ruff‚Äôs formatter is opinionated‚Äîit makes formatting decisions for you, eliminating debates about style. The philosophy is simple: let the tool handle formatting so you can focus on the code itself.\nFormat your code:\nuvx ruff format your_script.py\nBefore formatting:\ndef calculate_mean(values,skip_none=True):\n    if skip_none:values=[v for v in values if v is not None]\n    return sum(values)/len(values)\nAfter formatting:\ndef calculate_mean(values, skip_none=True):\n    if skip_none:\n        values = [v for v in values if v is not None]\n    return sum(values) / len(values)\nYou can use both linting and formatting together:\nuvx ruff format your_script.py && uvx ruff check --fix your_script.py\nThis gives you a single, fast tool to automatically improve your code quality.\n\n\n\n\n\n\nTipCode Quality in Jupyter Notebooks\n\n\n\nRuff can also format Jupyter notebooks:\nuvx ruff format analysis.ipynb\nuvx ruff check analysis.ipynb\nConfigure per-cell ignores for exploratory code while maintaining standards for final analysis code.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Code Quality and Documentation</span>"
    ]
  },
  {
    "objectID": "python/code-quality/index.html#summary-and-practical-guidelines",
    "href": "python/code-quality/index.html#summary-and-practical-guidelines",
    "title": "4¬† Code Quality and Documentation",
    "section": "4.5 Summary and Practical Guidelines",
    "text": "4.5 Summary and Practical Guidelines\nCode quality is not about perfectionism‚Äîit is about making your research more reliable, your collaboration more effective, and your future work easier. The practices covered in this chapter form the foundation of professional Python development:\n\nOrganization and readability: Structure code to reveal intent clearly\nDocumentation: Write docstrings that explain what code does and how to use it\nType hints: Make data types explicit to catch errors early\nAutomated formatting: Use ruff to maintain consistent style\n\nFor research projects, we recommend:\n\nStart simple: Begin with basic ruff configuration, add rules gradually\nFormat early: Run ruff format regularly, not just before commits\nFix what matters: Use ruff check --fix to auto-fix safe issues\nTeam consistency: Ensure all collaborators use the same tools and configuration\n\nThese practices require some initial investment, but they pay dividends throughout your research career. Code that is well-organized, well-documented, and consistently formatted is easier to debug, easier to extend, and easier to share with collaborators and reviewers.\nAs you develop your empirical finance projects, make these practices habitual. Configure your tools once, integrate them into your workflow, and let automation handle the details. This frees you to focus on what matters: designing sound research, implementing correct methodology, and drawing valid conclusions from your data.\nThe practices in this chapter work best alongside testing, which we will cover in the next chapter. While code quality ensures your code is readable and well-documented, testing ensures it is correct. Together, they form the foundation of reliable research software.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Code Quality and Documentation</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html",
    "href": "python/testing/index.html",
    "title": "5¬† Error Handling and Testing",
    "section": "",
    "text": "5.1 Exceptions and Error Handling\nIn empirical research, the quality and reliability of your code directly impacts the quality of your results. A small bug in your data processing pipeline or statistical calculation can invalidate months of work. Yet many researchers write code without systematic error handling or testing. The result? Papers retracted due to coding errors, results that can‚Äôt be replicated, and countless hours spent debugging problems that could have been caught early.\nThis chapter introduces two complementary practices that will make your research code more robust: error handling and testing. Error handling is about writing code that fails gracefully and provides useful information when something goes wrong. Testing is about systematically verifying that your code does what you think it does. Together, these practices form the foundation of reliable, reproducible research.\nThink of error handling as defensive driving for your code. You anticipate what might go wrong and plan for it. Testing, on the other hand, is like having a checklist before takeoff. You verify that everything works as expected before committing to your results. Both practices require a small upfront investment that pays enormous dividends in time saved and confidence gained.\nWhen something goes wrong in a Python program, the interpreter raises an exception. An exception is Python‚Äôs way of signaling that an error has occurred. If you‚Äôve written any Python code, you‚Äôve likely encountered exceptions: TypeError, ValueError, KeyError, FileNotFoundError, and so on. By default, an unhandled exception stops your program and prints a traceback showing where the error occurred.\nWhile this default behavior is useful during development, it‚Äôs often not what you want in production code or long-running research scripts. What if a file is missing, but you can use a default dataset instead? What if one stock in your analysis has corrupted data, but you want to continue processing the others? What if a network request fails, but you can retry it? This is where error handling comes in.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html#exceptions-and-error-handling",
    "href": "python/testing/index.html#exceptions-and-error-handling",
    "title": "5¬† Error Handling and Testing",
    "section": "",
    "text": "5.1.1 Understanding Exceptions\nLet‚Äôs start with a simple example. Suppose you‚Äôre calculating portfolio returns and need to divide returns by portfolio values:\ndef calculate_return_percentage(return_dollars, portfolio_value):\n    return (return_dollars / portfolio_value) * 100\n\n# This works fine\nprint(calculate_return_percentage(1000, 50000))  # 2.0\n\n# But this crashes\nprint(calculate_return_percentage(1000, 0))  # ZeroDivisionError!\nWhen you try to divide by zero, Python raises a ZeroDivisionError. The program stops, and you see a traceback. While this is informative, it‚Äôs not helpful if you‚Äôre processing thousands of portfolios and one happens to have zero value.\n\n\n5.1.2 The try-except Block\nPython‚Äôs try-except block allows you to handle exceptions gracefully. The basic structure is:\ntry:\n1    result = risky_operation()\n2except SomeException:\n3    result = default_value\n\n1\n\nCode that might raise an exception goes in the try block.\n\n2\n\nSpecify which exception type(s) to catch.\n\n3\n\nHandle the error and provide a fallback.\n\n\nLet‚Äôs apply this to our portfolio example:\ndef calculate_return_percentage(return_dollars, portfolio_value):\n    try:\n        return (return_dollars / portfolio_value) * 100\n    except ZeroDivisionError:\n        # Portfolio has zero value, return None or a special value\n        return None\n\n# Now this doesn't crash\nprint(calculate_return_percentage(1000, 50000))  # 2.0\nprint(calculate_return_percentage(1000, 0))      # None\nThis is better, but we‚Äôve lost information. We know the calculation failed, but we don‚Äôt know which portfolio or why. In research code, you almost always want to preserve this information:\ndef calculate_return_percentage(return_dollars, portfolio_value):\n    try:\n        return (return_dollars / portfolio_value) * 100\n    except ZeroDivisionError:\n        print(f\"Warning: Cannot calculate return for portfolio with zero value\")\n        return None\nWhile print() statements are commonly used to log errors and warnings, there are better ways to handle logging in production code. We introduce proper logging techniques in Chapter 6.\n\n\n\n\n\n\nTipWhen to Catch Exceptions\n\n\n\nA common mistake is catching exceptions too broadly or too often. Don‚Äôt catch exceptions just because you can. Catch them when you have a specific, sensible way to handle the error. If you can‚Äôt do anything useful with the exception, it‚Äôs often better to let it propagate and fail fast rather than hiding the problem.\n\n\n\n\n5.1.3 Catching Multiple Exceptions\nOften, several different things can go wrong, and you want to handle them differently:\ndef load_stock_data(filename):\n    try:\n        with open(filename, 'r') as f:\n            lines = f.readlines()\n        if len(lines) == 0:\n            raise ValueError(\"File is empty\")\n        # Parse header and validate columns\n        header = lines[0].strip().split(',')\n        required_columns = ['date', 'close', 'volume']\n        missing = set(required_columns) - set(header)\n        if missing:\n            raise ValueError(f\"Missing required columns: {missing}\")\n        return lines\n    except FileNotFoundError:\n        print(f\"Error: File '{filename}' not found\")\n        return None\n    except ValueError as e:\n        print(f\"Error: Invalid data format - {e}\")\n        return None\nYou can also catch multiple exceptions in a single except block if you want to handle them the same way:\ndef load_data(filename):\n    try:\n        with open(filename, 'r') as f:\n            return f.read()\n    except (FileNotFoundError, PermissionError) as e:\n        print(f\"Error loading '{filename}': {e}\")\n        return None\n\n\n5.1.4 The else and finally Clauses\nThe try-except block can include two additional clauses: else and finally.\nThe else clause runs if no exception was raised:\ndef process_file(filename):\n    try:\n        with open(filename, 'r') as f:\n            lines = f.readlines()\n    except FileNotFoundError:\n        print(f\"File not found: {filename}\")\n        return None\n    else:\n        # This runs only if no exception occurred\n        print(f\"Successfully loaded {len(lines)} lines\")\n        return lines\nThe finally clause always runs, whether an exception occurred or not. This is useful for cleanup:\ndef analyze_large_dataset(filename):\n    file_handle = None\n    try:\n        file_handle = open(filename, 'r')\n        data = process(file_handle)\n        return data\n1    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return None\n2    finally:\n        if file_handle:\n            file_handle.close()\n\n1\n\nThe except block handles errors.\n\n2\n\nThe finally block always runs, ensuring the file is closed even if an error occurs or the function returns early.\n\n\n\n\n\n\n\n\nNoteContext Managers vs.¬†finally\n\n\n\nFor file handling and similar resources, Python‚Äôs context managers (the with statement) are usually cleaner than finally:\ndef analyze_large_dataset(filename):\n    try:\n        with open(filename, 'r') as file_handle:\n            data = process(file_handle)\n            return data\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return None\nThe context manager automatically closes the file, even if an exception occurs.\n\n\n\n\n5.1.5 Raising Exceptions\nSometimes you need to signal an error in your own code. Use the raise statement:\ndef calculate_sharpe_ratio(returns, risk_free_rate):\n    \"\"\"Calculate Sharpe ratio.\n\n    Parameters\n    ----------\n    returns : array-like\n        Series of returns\n    risk_free_rate : float\n        Risk-free rate\n\n    Raises\n    ------\n    ValueError\n        If returns is empty or risk_free_rate is negative\n    \"\"\"\n    if len(returns) == 0:\n        raise ValueError(\"Returns array cannot be empty\")\n\n    if risk_free_rate &lt; 0:\n        raise ValueError(\"Risk-free rate cannot be negative\")\n\n    excess_returns = returns - risk_free_rate\n    return excess_returns.mean() / excess_returns.std()\nThis is much better than returning a special value like -999 or None and hoping the caller checks for it. An exception forces the caller to explicitly handle the error.\n\n\n5.1.6 Creating Custom Exceptions\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video covers similar topics to this section.\n\n\n\nFor complex projects, you might want to define your own exception types. In Python, all exceptions are classes that inherit from the built-in Exception class (see Chapter 3 for more on classes and inheritance). This makes it easier to catch specific errors:\nclass DataQualityError(Exception):\n    \"\"\"Raised when data fails quality checks.\"\"\"\n    pass\n\nclass InsufficientDataError(Exception):\n    \"\"\"Raised when there's not enough data for analysis.\"\"\"\n    pass\n\ndef calculate_rolling_beta(stock_returns, market_returns, window=60):\n    \"\"\"Calculate rolling beta with data quality checks.\"\"\"\n    if len(stock_returns) &lt; window:\n        raise InsufficientDataError(\n            f\"Need at least {window} observations, got {len(stock_returns)}\"\n        )\n\n    # Check for too many missing values\n    missing_pct = stock_returns.isna().sum() / len(stock_returns)\n    if missing_pct &gt; 0.1:\n        raise DataQualityError(\n            f\"Too many missing values: {missing_pct:.1%}\"\n        )\n\n    # Calculate beta...\nNow calling code can handle different errors appropriately:\ntry:\n    beta = calculate_rolling_beta(stock_returns, market_returns)\nexcept InsufficientDataError as e:\n    print(f\"Skipping stock: {e}\")\n    beta = None\nexcept DataQualityError as e:\n    print(f\"Data quality issue: {e}\")\n    beta = None\n\n\n\n\n\n\nWarningDon‚Äôt Swallow Exceptions\n\n\n\nA common antipattern is the bare except: clause that catches everything:\n# BAD: This hides all errors, including bugs in your code\ntry:\n    result = complex_calculation()\nexcept:\n    result = None\nThis will catch not just the errors you expect, but also bugs in your code, keyboard interrupts, and system errors. Always catch specific exceptions, or at least use except Exception: which won‚Äôt catch system-exiting exceptions.\n\n\n\n\n5.1.7 Error Handling in Data Pipelines\nIn empirical research, you often process many items (stocks, firms, countries) where some might fail. Here‚Äôs a pattern for handling this gracefully:\ndef process_stock(ticker, start_date, end_date):\n    \"\"\"Process a single stock, raising exceptions on failure.\"\"\"\n    # This function doesn't handle exceptions - it lets them propagate\n    data = download_data(ticker, start_date, end_date)\n    returns = calculate_returns(data)\n    return calculate_statistics(returns)\n\ndef process_all_stocks(tickers, start_date, end_date):\n    \"\"\"Process multiple stocks, collecting both successes and failures.\"\"\"\n    results = {}\n    errors = {}\n\n    for ticker in tickers:\n        try:\n            results[ticker] = process_stock(ticker, start_date, end_date)\n        except Exception as e:\n            # Log the error but continue processing\n            errors[ticker] = str(e)\n            print(f\"Error processing {ticker}: {e}\")\n\n    print(f\"\\nProcessed {len(results)} stocks successfully\")\n    print(f\"Failed to process {len(errors)} stocks\")\n\n    return results, errors\n\n# Usage\ntickers = ['AAPL', 'MSFT', 'INVALID_TICKER', 'GOOGL']\nresults, errors = process_all_stocks(tickers, '2020-01-01', '2023-12-31')\nThis pattern separates the logic (in process_stock) from the error handling (in process_all_stocks). The individual function can be tested in isolation, while the batch function handles partial failures gracefully.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html#unit-testing-with-pytest",
    "href": "python/testing/index.html#unit-testing-with-pytest",
    "title": "5¬† Error Handling and Testing",
    "section": "5.2 Unit Testing with pytest",
    "text": "5.2 Unit Testing with pytest\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video covers similar topics to this section.\n\n\n\nError handling helps your code fail gracefully when things go wrong. Testing helps ensure things don‚Äôt go wrong in the first place. A test is simply code that verifies other code works correctly. You write a test that calls your function with known inputs and checks that it produces the expected output.\n\n5.2.1 Why Test?\nYou might think: ‚ÄúI‚Äôll just run my code and check the results. Why write separate tests?‚Äù Here‚Äôs why testing matters:\n\nConfidence in changes: When you modify code, tests verify you didn‚Äôt break anything.\nDocumentation: Tests show how your code is meant to be used.\nBetter design: Code that‚Äôs easy to test is usually better designed.\nCatch bugs early: Tests find problems before they affect your results.\nReproducibility: Tests verify your code produces consistent results.\nValidation of AI-generated code: Tests provide an additional layer of verification when using AI coding assistants.\n\nIn research, there‚Äôs an additional benefit: tests help you understand your methods. Writing tests forces you to think clearly about what your code should do, edge cases, and assumptions. This deeper understanding often reveals problems in your research design.\n\n\n\n\n\n\nTipAI Coding Assistants and Testing\n\n\n\nAI coding assistants are particularly good at writing tests, making it easier to build a comprehensive test suite. However, always review AI-generated tests carefully. AI may miss important edge cases or make incorrect assumptions about expected behavior. Use AI-generated tests as a starting point, then add your own tests for edge cases and domain-specific scenarios that the AI might overlook.\n\n\n\n\n5.2.2 Getting Started with pytest\npytest is Python‚Äôs most popular testing framework. It‚Äôs simple to use but powerful enough for complex projects. The easiest way to run pytest is using uvx, which runs the tool without requiring explicit installation:\nuvx pytest test_math.py\nIf you‚Äôre working on a project and want pytest available as a development dependency, add it to your project‚Äôs dev group:\nuv add --dev pytest\nThen run it with:\nuv run pytest\nA pytest test is just a function whose name starts with test_. Here‚Äôs the simplest possible test:\ndef test_simple():\n    assert 1 + 1 == 2\nSave this in a file called test_math.py and run:\nuvx pytest test_math.py\nYou‚Äôll see output indicating the test passed. The assert statement is the heart of testing. If the expression after assert is True, the test passes. If it‚Äôs False, the test fails.\n\n\n5.2.3 Testing a Real Function\nLet‚Äôs test a function that calculates simple returns:\n# finance_utils.py\ndef calculate_simple_returns(prices):\n    \"\"\"Calculate simple returns from a price series.\n\n    Parameters\n    ----------\n    prices : list\n        Series of prices\n\n    Returns\n    -------\n    list\n        Simple returns (length is len(prices) - 1)\n    \"\"\"\n    returns = []\n    for i in range(1, len(prices)):\n        ret = (prices[i] - prices[i-1]) / prices[i-1]\n        returns.append(ret)\n    return returns\nNow write tests:\n# test_finance_utils.py\nfrom finance_utils import calculate_simple_returns\n\ndef test_simple_returns_basic():\n    \"\"\"Test simple returns calculation with known values.\"\"\"\n    prices = [100, 110, 121]\n    returns = calculate_simple_returns(prices)\n\n    # Expected: (110-100)/100 = 0.10, (121-110)/110 = 0.10\n    assert abs(returns[0] - 0.10) &lt; 1e-10\n    assert abs(returns[1] - 0.10) &lt; 1e-10\n\ndef test_simple_returns_length():\n    \"\"\"Test that output length is correct.\"\"\"\n    prices = [100, 110, 121, 133.1]\n    returns = calculate_simple_returns(prices)\n    assert len(returns) == len(prices) - 1\n\ndef test_simple_returns_constant_prices():\n    \"\"\"Test with constant prices (zero returns).\"\"\"\n    prices = [100, 100, 100]\n    returns = calculate_simple_returns(prices)\n    assert returns == [0, 0]\nRun the tests:\nuvx pytest test_finance_utils.py\nEach test function checks a different aspect of the behavior. This is much more thorough than running the function once and eyeballing the output.\n\n\n\n\n\n\nTipTest One Thing Per Test\n\n\n\nEach test should verify one specific behavior. This makes failures easier to diagnose. When a test fails, you want to immediately know what‚Äôs wrong, not spend time figuring out which of five assertions in the test failed.\n\n\n\n\n5.2.4 Understanding Test Output\nWhen a test fails, pytest provides detailed information:\ndef test_log_returns_incorrect():\n    \"\"\"This test will fail to demonstrate pytest output.\"\"\"\n    prices = [100, 110]\n    returns = calculate_log_returns(prices)\n    assert returns[0] == 0.1  # This is wrong - log(1.1) ‚âà 0.0953\nRunning this produces:\ntest_finance_utils.py::test_log_returns_incorrect FAILED\n\n================================== FAILURES ===================================\n________________________ test_log_returns_incorrect __________________________\n\n    def test_log_returns_incorrect():\n        prices = [100, 110]\n        returns = calculate_log_returns(prices)\n&gt;       assert returns[0] == 0.1\nE       assert 0.09531017980432493 == 0.1\n\ntest_finance_utils.py:8: AssertionError\nThe output shows exactly which assertion failed and what the actual value was. This makes debugging straightforward.\n\n\n5.2.5 Testing with Fixtures\nOften, you need the same data for multiple tests. pytest fixtures let you set up reusable test data:\nimport pytest\n\n@pytest.fixture\ndef sample_prices():\n    \"\"\"Create sample price data for testing.\"\"\"\n    return [100, 105, 103, 108, 112, 110, 115]\n\ndef test_returns_are_correct(sample_prices):\n    \"\"\"Test returns calculation using fixture.\"\"\"\n    returns = calculate_simple_returns(sample_prices)\n    # First return: (105-100)/100 = 0.05\n    assert abs(returns[0] - 0.05) &lt; 1e-10\n\ndef test_data_has_correct_length(sample_prices):\n    \"\"\"Test using the same fixture.\"\"\"\n    assert len(sample_prices) == 7\nThe @pytest.fixture decorator marks a function as a fixture. When you include the fixture name as a test function parameter, pytest automatically calls the fixture and passes its return value to your test.\nFixtures can also handle setup and teardown:\nimport pytest\nimport tempfile\nimport os\n\n@pytest.fixture\ndef temp_data_file():\n    \"\"\"Create a temporary file with test data.\"\"\"\n1    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv') as f:\n        f.write('date,close\\n')\n        f.write('2020-01-01,100\\n')\n        f.write('2020-01-02,110\\n')\n        temp_path = f.name\n\n2    yield temp_path\n\n3    os.unlink(temp_path)\n\n4def test_load_data_from_file(temp_data_file):\n    \"\"\"Test loading data from a CSV file.\"\"\"\n    with open(temp_data_file) as f:\n        lines = f.readlines()\n    assert len(lines) == 3  # Header + 2 data rows\n\n1\n\nSetup: Create temporary file with test data.\n\n2\n\nYield: Provide the file path to the test.\n\n3\n\nTeardown: Clean up the file after the test completes.\n\n4\n\nThe fixture name as a parameter tells pytest to inject the fixture‚Äôs return value.\n\n\nThe yield statement separates setup from teardown. Everything before yield runs before the test, and everything after runs after the test (even if the test fails).\n\n\n5.2.6 Parametrized Tests\nWhen you want to test the same function with multiple inputs, use parametrization:\nimport pytest\n\n@pytest.mark.parametrize(\"prices,expected_length\", [\n    ([100, 110], 1),\n    ([100, 110, 121], 2),\n    ([100, 110, 121, 133], 3),\n    ([100], 0),  # Edge case: single price\n])\ndef test_returns_length_parametrized(prices, expected_length):\n    \"\"\"Test that returns have correct length for various inputs.\"\"\"\n    returns = calculate_simple_returns(prices)\n    assert len(returns) == expected_length\nThis creates four separate tests, one for each parameter set. This is cleaner than writing four separate test functions and makes the pattern clear.\nYou can parametrize multiple arguments:\n@pytest.mark.parametrize(\"initial_price,final_price,expected_return\", [\n    (100, 110, 0.10),   # 10% price increase\n    (100, 90, -0.10),   # 10% price decrease\n    (100, 100, 0),      # No change\n    (50, 100, 1.0),     # 100% increase\n])\ndef test_simple_return_calculation(initial_price, final_price, expected_return):\n    \"\"\"Test simple return calculation with various price changes.\"\"\"\n    returns = calculate_simple_returns([initial_price, final_price])\n    assert abs(returns[0] - expected_return) &lt; 1e-10\n\n\n5.2.7 Testing for Exceptions\nSometimes you want to verify that your code raises an exception in certain situations:\ndef calculate_mean_return(returns):\n    \"\"\"Calculate mean return.\"\"\"\n    if len(returns) == 0:\n        raise ValueError(\"Returns list cannot be empty\")\n\n    return sum(returns) / len(returns)\n\ndef test_mean_return_empty_raises():\n    \"\"\"Test that empty returns raise ValueError.\"\"\"\n    with pytest.raises(ValueError, match=\"cannot be empty\"):\n        calculate_mean_return([])\n\ndef test_mean_return_with_data():\n    \"\"\"Test normal mean return calculation.\"\"\"\n    returns = [0.01, 0.02, -0.01, 0.03]\n    mean = calculate_mean_return(returns)\n    assert abs(mean - 0.0125) &lt; 1e-10\nThe pytest.raises() context manager asserts that the code block raises the specified exception. The match parameter checks that the exception message matches a pattern (using regular expressions).\n\n\n\n\n\n\nNoteTesting with NumPy and pandas\n\n\n\nTesting functions that work with NumPy arrays or pandas DataFrames sometimes requires special handling, such as using np.allclose() for floating-point array comparisons or pd.testing.assert_frame_equal() for DataFrame comparisons.\n\n\n\n\n5.2.8 Organizing Tests\nAs your project grows, organize tests to mirror your code structure:\nmy_research_project/\n‚îú‚îÄ‚îÄ finance_utils/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ returns.py\n‚îÇ   ‚îú‚îÄ‚îÄ risk.py\n‚îÇ   ‚îî‚îÄ‚îÄ portfolio.py\n‚îî‚îÄ‚îÄ tests/\n    ‚îú‚îÄ‚îÄ __init__.py\n    ‚îú‚îÄ‚îÄ test_returns.py\n    ‚îú‚îÄ‚îÄ test_risk.py\n    ‚îî‚îÄ‚îÄ test_portfolio.py\nRun all tests with:\nuvx pytest tests/\nOr if pytest is installed in your project:\nuv run pytest tests/\nRun specific tests:\nuvx pytest tests/test_returns.py\nuvx pytest tests/test_returns.py::test_simple_returns_basic\n\n\n\n\n\n\nNoteConfiguration with pytest.ini\n\n\n\nCreate a pytest.ini file in your project root to configure pytest:\n[tool.pytest.ini_options]\ntestpaths = tests\npython_files = test_*.py\npython_functions = test_*\naddopts = -v --strict-markers\nThis specifies where to find tests and how to run them.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html#test-driven-development-concepts",
    "href": "python/testing/index.html#test-driven-development-concepts",
    "title": "5¬† Error Handling and Testing",
    "section": "5.3 Test-Driven Development Concepts",
    "text": "5.3 Test-Driven Development Concepts\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video by ArjanCodes covers similar topics to this section.\n\n\n\nTest-Driven Development (TDD) is a development approach where you write tests before writing the code they test. This might seem backwards, but it has significant benefits, especially in research.\n\n5.3.1 The TDD Cycle\nTDD follows a simple cycle:\n\nRed: Write tests that fail (because the code doesn‚Äôt exist yet), including edge cases\nGreen: Write just enough code to make all the tests pass\nRefactor: Improve the code while keeping tests passing\n\nLet‚Äôs walk through an example. Suppose you need to calculate the maximum drawdown of a price series.\nStep 1: Write failing tests, including edge cases\n# test_risk.py\nfrom risk import calculate_max_drawdown\n\ndef test_max_drawdown_simple():\n    \"\"\"Test max drawdown with simple price series.\"\"\"\n    prices = [100, 110, 105, 115, 90, 95]\n    # Peak is 115, trough is 90, drawdown is (90-115)/115 ‚âà -0.217\n    assert abs(calculate_max_drawdown(prices) - (-0.217)) &lt; 0.001\n\ndef test_max_drawdown_no_drawdown():\n    \"\"\"Test with monotonically increasing prices (no drawdown).\"\"\"\n    prices = [100, 110, 120, 130]\n    assert calculate_max_drawdown(prices) == 0\n\ndef test_max_drawdown_single_price():\n    \"\"\"Test with single price.\"\"\"\n    prices = [100]\n    assert calculate_max_drawdown(prices) == 0\nRun these tests. They will all fail because calculate_max_drawdown doesn‚Äôt exist yet.\nStep 2: Write minimal code to pass\n# risk.py\ndef calculate_max_drawdown(prices):\n    \"\"\"Calculate maximum drawdown from a price series.\"\"\"\n    if len(prices) &lt;= 1:\n        return 0\n\n    max_drawdown = 0\n    peak = prices[0]\n\n    for price in prices:\n        if price &gt; peak:\n            peak = price\n        drawdown = (price - peak) / peak\n        if drawdown &lt; max_drawdown:\n            max_drawdown = drawdown\n\n    return max_drawdown\nRun the tests again. They should all pass.\nStep 3: Refactor if needed\nThe code is clean and handles all edge cases. We can now move on, or improve our function to make it more efficient.\n\n\n5.3.2 Benefits of TDD for Research\nTDD might feel slow at first, but it pays off:\n\nClarifies thinking: Writing the test first forces you to specify exactly what you want the function to do.\nPrevents scope creep: You implement only what‚Äôs needed to pass tests.\nDocuments intent: Tests show how the function should behave.\nEnables refactoring: You can improve code with confidence because tests verify behavior doesn‚Äôt change.\n\nIn research, TDD is particularly valuable when implementing statistical methods or financial calculations. Write tests based on the formulas in the paper, then implement the method. The tests verify you‚Äôve implemented the method correctly.\n\n\n\n\n\n\nTipTDD for Complex Calculations\n\n\n\nWhen implementing a complex statistical method from a paper:\n\nCreate tests using examples from the paper (if provided)\nCreate tests using results from R or Stata implementations\nCreate tests using simple cases you can verify by hand\nThen implement your Python version\n\nThis approach catches mistakes early and gives you confidence in your implementation.\n\n\n\n\n5.3.3 When Not to Use TDD\nTDD isn‚Äôt always the right approach:\n\nExploratory analysis: When you don‚Äôt know what you‚Äôre looking for, write code first, then add tests\nPrototypes: If you‚Äôre just trying something to see if it works, TDD adds overhead\nSimple scripts: For one-off analyses, informal testing might be enough\n\nBut for any code you‚Äôll reuse or that‚Äôs critical to your results, testing (whether test-first or test-after) is essential.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html#testing-floating-point-calculations",
    "href": "python/testing/index.html#testing-floating-point-calculations",
    "title": "5¬† Error Handling and Testing",
    "section": "5.4 Testing Floating-Point Calculations",
    "text": "5.4 Testing Floating-Point Calculations\nFinancial calculations often involve floating-point arithmetic, which has quirks:\ndef test_floating_point_comparison():\n    \"\"\"Demonstrate floating-point comparison issues.\"\"\"\n    # This might fail due to floating-point precision\n    result = 0.1 + 0.2\n    # Don't do this:\n    # assert result == 0.3  # Might fail!\n\n    # Do this instead:\n    assert abs(result - 0.3) &lt; 1e-10\n    # Or use pytest's approximate comparison:\n    assert result == pytest.approx(0.3)\nAlways use tolerance-based comparisons for floating-point numbers. The pytest.approx() function is particularly nice because it chooses sensible default tolerances:\ndef test_returns_calculation():\n    \"\"\"Test returns calculation with approximate comparison.\"\"\"\n    prices = [100, 105, 110.25]\n    returns = calculate_simple_returns(prices)\n    expected = [0.05, 0.05]\n\n    assert returns == pytest.approx(expected, rel=1e-6)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html#best-practices-summary",
    "href": "python/testing/index.html#best-practices-summary",
    "title": "5¬† Error Handling and Testing",
    "section": "5.5 Best Practices Summary",
    "text": "5.5 Best Practices Summary\nLet‚Äôs consolidate what we‚Äôve learned into actionable practices:\nError Handling:\n\nCatch specific exceptions, not broad ones\nProvide informative error messages\nDon‚Äôt hide errors unless you can handle them meaningfully\nUse custom exceptions for domain-specific errors\nValidate inputs early and explicitly\n\nTesting:\n\nWrite tests for any code you‚Äôll reuse\nTest edge cases, not just happy paths\nOne assertion per test when possible\nUse fixtures for reusable test data\nUse parametrization to test multiple scenarios\nRun tests frequently during development\n\nGeneral:\n\nMake functions testable (pure functions with clear inputs/outputs)\nValidate assumptions with assertions\nDocument expected behavior\nUse type hints to catch errors early\nReview your own code before considering it done\n\n\n\n\n\n\n\nNoteTesting in Research Workflows\n\n\n\nIn empirical research, you often have a mix of:\n\nLibrary code: Functions you‚Äôll reuse across projects (test thoroughly)\nAnalysis scripts: One-off analyses (test key calculations)\nExploratory code: Trying things out (informal testing is fine)\n\nFocus your testing effort on library code and anything that affects your paper‚Äôs results. A bug in a chart‚Äôs formatting is annoying; a bug in your returns calculation invalidates your research.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/testing/index.html#conclusion",
    "href": "python/testing/index.html#conclusion",
    "title": "5¬† Error Handling and Testing",
    "section": "5.6 Conclusion",
    "text": "5.6 Conclusion\nError handling and testing might feel like overhead when you start a project, but they‚Äôre investments that pay enormous dividends. Code that handles errors gracefully is more robust and maintainable. Code with tests is easier to modify, debug, and trust.\nIn empirical research, where your code directly impacts your results and conclusions, this isn‚Äôt just about software engineering best practices‚Äîit‚Äôs about research integrity. A well-tested analysis pipeline gives you confidence in your results. Good error handling helps you identify data quality issues and edge cases. Together, they make your research more reproducible and reliable.\nStart small. Add error handling to functions that interact with external data. Write tests for your key calculations. As these practices become habits, you‚Äôll find yourself writing better code, spending less time debugging, and having more confidence in your results.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Error Handling and Testing</span>"
    ]
  },
  {
    "objectID": "python/logging/index.html",
    "href": "python/logging/index.html",
    "title": "6¬† Logging and Configuration",
    "section": "",
    "text": "6.1 Why Logging Matters\nWhen you run a research pipeline‚Äîdownloading data, cleaning it, estimating models, and generating output‚Äîthings will go wrong. Files will be missing, APIs will fail, and edge cases will surface. Without proper logging, you‚Äôre left guessing what happened and when. This chapter covers Python‚Äôs built-in logging module and introduces Hydra, a powerful framework for managing complex research configurations.\nGood logging practices are essential for reproducible research. When you run an analysis months later or share code with collaborators, logs provide a record of what the code did, what warnings occurred, and where things failed. Combined with proper configuration management, you can recreate any run exactly as it happened.\nMany researchers start by sprinkling print() statements throughout their code:\nThis approach has several problems:\nA proper logging system offers:",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Logging and Configuration</span>"
    ]
  },
  {
    "objectID": "python/logging/index.html#why-logging-matters",
    "href": "python/logging/index.html#why-logging-matters",
    "title": "6¬† Logging and Configuration",
    "section": "",
    "text": "print(\"Starting data download...\")\nprint(f\"Downloaded {len(df)} rows\")\nprint(\"WARNING: Missing values detected\")\nprint(\"Error: API rate limit exceeded\")\n\n\nNo severity levels: You can‚Äôt distinguish informational messages from warnings or errors\nNo timestamps: You don‚Äôt know when events occurred\nNo control: You can‚Äôt easily turn messages on or off or redirect them to files\nNo context: You don‚Äôt know which module or function produced the message\nCluttered output: Everything goes to the same place, making it hard to find important messages\n\n\n\nSeverity levels: DEBUG, INFO, WARNING, ERROR, CRITICAL‚Äîso you can filter by importance\nTimestamps: Know exactly when each event occurred\nSource information: See which module and function generated each message\nFlexible output: Send logs to console, files, or external services\nConfiguration: Control logging behavior without changing code",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Logging and Configuration</span>"
    ]
  },
  {
    "objectID": "python/logging/index.html#pythons-logging-module",
    "href": "python/logging/index.html#pythons-logging-module",
    "title": "6¬† Logging and Configuration",
    "section": "6.2 Python‚Äôs logging Module",
    "text": "6.2 Python‚Äôs logging Module\nPython‚Äôs standard library includes a powerful logging module. While it has a learning curve, understanding its core concepts pays off in any serious project.\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video provides a good overview of Python logging.\n\n\n\n\n6.2.1 Basic Usage\nThe simplest way to use logging:\n\nimport logging\n\n# Configure basic logging\nlogging.basicConfig(level=logging.INFO)\n\n# Create a logger for this module\nlogger = logging.getLogger(__name__)\n\n# Log messages at different levels\nlogger.debug(\"Detailed information for debugging\")\nlogger.info(\"General information about program execution\")\nlogger.warning(\"Something unexpected happened, but program continues\")\nlogger.error(\"A serious problem occurred\")\nlogger.critical(\"Program may not be able to continue\")\n\nINFO:__main__:General information about program execution\nWARNING:__main__:Something unexpected happened, but program continues\nERROR:__main__:A serious problem occurred\nCRITICAL:__main__:Program may not be able to continue\n\n\n\n\n6.2.2 Understanding Log Levels\nLog levels form a hierarchy. When you set a level, you see messages at that level and above:\n\n\n\nLevel\nNumeric Value\nWhen to Use\n\n\n\n\nDEBUG\n10\nDetailed diagnostic information\n\n\nINFO\n20\nConfirmation that things work as expected\n\n\nWARNING\n30\nSomething unexpected but not necessarily wrong\n\n\nERROR\n40\nA serious problem; some functionality failed\n\n\nCRITICAL\n50\nA very serious error; program may crash\n\n\n\n\nimport logging\n\n# Only show WARNING and above\n1logging.basicConfig(level=logging.WARNING, force=True)\nlogger = logging.getLogger(\"level_demo\")\n\nlogger.debug(\"This won't appear\")\nlogger.info(\"This won't appear either\")\nlogger.warning(\"This will appear\")\nlogger.error(\"This will definitely appear\")\n\n\n1\n\nThe force=True parameter is needed here because we already called basicConfig() earlier in this chapter. By default, basicConfig() only configures logging once‚Äîsubsequent calls are ignored. Using force=True removes any existing handlers and reconfigures logging with the new settings.\n\n\n\n\nWARNING:level_demo:This will appear\nERROR:level_demo:This will definitely appear\n\n\nUsing log levels provides two key advantages:\n\nRoute messages to different outputs: You can direct messages of different levels to different destinations‚Äîfor example, send INFO messages to a file while only showing WARNING and above on the console.\nControl verbosity at runtime: You can leave all log messages in your code but choose at runtime which levels to display. This means you can include detailed DEBUG messages during development that won‚Äôt clutter your output in production unless you need them.\n\n\n\n6.2.3 Configuring Log Format\nThe default format is minimal. For research workflows, you typically want more information:\n\nimport logging\n\n# Configure with a custom format\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    force=True\n)\n\nlogger = logging.getLogger(\"format_demo\")\nlogger.info(\"Now you can see when this happened\")\n\n2026-01-03 10:48:27 - format_demo - INFO - Now you can see when this happened\n\n\nCommon format fields:\n\n%(asctime)s: Human-readable timestamp\n%(name)s: Logger name (usually module name)\n%(levelname)s: DEBUG, INFO, WARNING, etc.\n%(message)s: The actual log message\n%(filename)s: Source file name\n%(lineno)d: Line number in source file\n%(funcName)s: Function name\n\n\n\n6.2.4 Logging to Files\nFor long-running research pipelines, you want logs saved to files:\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('research_pipeline.log'),\n        logging.StreamHandler()  # Also print to console\n    ]\n)\n\nlogger = logging.getLogger(__name__)\nlogger.info(\"This goes to both the file and console\")\n\n\n6.2.5 Logging Exceptions\nWhen catching exceptions, use logger.exception() to automatically include the traceback:\n\nimport logging\n\nlogging.basicConfig(level=logging.INFO, force=True)\nlogger = logging.getLogger(\"exception_demo\")\n\ndef risky_calculation(x):\n    return 1 / x\n\ntry:\n    result = risky_calculation(0)\nexcept ZeroDivisionError:\n    logger.exception(\"Calculation failed\")\n    # The traceback is automatically included\n\nERROR:exception_demo:Calculation failed\nTraceback (most recent call last):\n  File \"/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_77347/1524267329.py\", line 10, in &lt;module&gt;\n    result = risky_calculation(0)\n  File \"/var/folders/jr/cn9h86ld68qb5rtvs9gsb1vr0000gn/T/ipykernel_77347/1524267329.py\", line 7, in risky_calculation\n    return 1 / x\n           ~~^~~\nZeroDivisionError: division by zero\n\n\nIncluding the full traceback is a tradeoff: it provides valuable debugging information, but the multi-line output can break the structure of log files, making them harder to parse or query programmatically. For production systems where logs are processed automatically, you might prefer logging just the exception message and using logger.error() instead.\n\n\n6.2.6 Module-Level Loggers\nThe recommended pattern is to create a logger at the top of each module:\n# In portfolio_analysis.py\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef calculate_returns(prices):\n    logger.info(f\"Calculating returns for {len(prices)} observations\")\n    returns = prices.pct_change().dropna()\n\n    if returns.isna().any().any():\n        logger.warning(\"NaN values detected in returns\")\n\n    logger.debug(f\"Returns shape: {returns.shape}\")\n    return returns\nThe __name__ variable becomes the module‚Äôs fully qualified name (e.g., myproject.portfolio_analysis), which helps you trace where messages came from.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Logging and Configuration</span>"
    ]
  },
  {
    "objectID": "python/logging/index.html#logging-best-practices",
    "href": "python/logging/index.html#logging-best-practices",
    "title": "6¬† Logging and Configuration",
    "section": "6.3 Logging Best Practices",
    "text": "6.3 Logging Best Practices\nHere are key practices to follow when implementing logging in your research projects.\nUse appropriate levels. Choose log levels thoughtfully:\n# DEBUG: Detailed diagnostic info, usually only for debugging\nlogger.debug(f\"Processing row {i}: values = {row}\")\n\n# INFO: Key milestones and confirmations\nlogger.info(f\"Loaded {len(df)} rows from {filename}\")\n\n# WARNING: Unexpected but handled situations\nlogger.warning(f\"Missing data for {ticker}, using interpolation\")\n\n# ERROR: Something failed, but program can continue\nlogger.error(f\"Failed to download {ticker}: {e}\")\n\n# CRITICAL: Serious failure, program may need to stop\nlogger.critical(\"Database connection lost, cannot continue\")\nInclude context in messages. Log messages should be self-explanatory:\n# Bad: Not enough context\nlogger.info(\"Processing file\")\nlogger.warning(\"Missing values\")\n\n# Good: Clear context\nlogger.info(f\"Processing file: {filepath}\")\nlogger.warning(f\"Missing values in column '{col}': {count} rows affected\")\nDon‚Äôt log sensitive information. Be careful not to log passwords, API keys, or sensitive data:\n# Bad: Logs the API key\nlogger.info(f\"Connecting with API key: {api_key}\")\n\n# Good: Masks sensitive information\nlogger.info(f\"Connecting with API key: {api_key[:4]}...\")\nUse structured logging for complex data. For data that might be parsed later, consider structured formats:\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Log structured data\nmetrics = {\n    'ticker': 'AAPL',\n    'sharpe_ratio': 1.45,\n    'max_drawdown': -0.15,\n    'n_observations': 252\n}\nlogger.info(f\"Performance metrics: {json.dumps(metrics)}\")\nConfigure logging once at entry point. Configure logging at your application‚Äôs entry point, not in library modules. Include a timestamp in the log filename so that each run generates a new file:\n# In main.py or run_analysis.py\nimport logging\nfrom datetime import datetime\nfrom my_research import run_pipeline\n\ndef setup_logging():\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(f'analysis_{timestamp}.log'),\n            logging.StreamHandler()\n        ]\n    )\n\nif __name__ == \"__main__\":\n    setup_logging()\n    run_pipeline()\nLibrary modules should only create loggers, not configure them:\n# In my_research/analysis.py\nimport logging\n\nlogger = logging.getLogger(__name__)  # Just create the logger\n\ndef run_pipeline():\n    logger.info(\"Starting pipeline\")\n    # ...",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Logging and Configuration</span>"
    ]
  },
  {
    "objectID": "python/logging/index.html#configuration-management-with-hydra",
    "href": "python/logging/index.html#configuration-management-with-hydra",
    "title": "6¬† Logging and Configuration",
    "section": "6.4 Configuration Management with Hydra",
    "text": "6.4 Configuration Management with Hydra\nAs research projects grow, managing configuration becomes a challenge. You might have:\n\nDifferent data sources (local files, APIs, databases)\nMultiple model specifications to compare\nVarious output formats and destinations\nDevelopment vs.¬†production settings\n\nHardcoding these in Python leads to messy code and makes it hard to reproduce specific runs. YAML configuration files help, but you end up writing boilerplate code to load and validate them.\nHydra is a framework developed by Facebook Research (Yadan 2019) that elegantly solves these problems. It provides:\n\nHierarchical configuration: Compose configs from multiple sources\nCommand-line overrides: Change any parameter without editing files\nAutomatic working directories: Each run gets its own output directory\nMulti-run support: Sweep over parameter combinations\n\n\n\n\n\n\n\nNote Video\n\n\n\nThe following video provides a good overview of Hydra for managing project configurations.\n\n\n\n\n6.4.1 Installing Hydra\nuv add hydra-core\n\n\n6.4.2 Basic Hydra Application\nHere‚Äôs a minimal Hydra application:\n# my_analysis.py\nimport hydra\nfrom omegaconf import DictConfig\n\n@hydra.main(version_base=None, config_path=\"conf\", config_name=\"config\")\ndef main(cfg: DictConfig) -&gt; None:\n    print(f\"Processing data from: {cfg.data.source}\")\n    print(f\"Output directory: {cfg.output.dir}\")\n    print(f\"Model: {cfg.model.name}\")\n\nif __name__ == \"__main__\":\n    main()\nWith a configuration file:\n# conf/config.yaml\ndata:\n  source: \"data/returns.parquet\"\n  start_date: \"2020-01-01\"\n  end_date: \"2023-12-31\"\n\nmodel:\n  name: \"ols\"\n  robust_se: true\n\noutput:\n  dir: \"results\"\n  format: \"parquet\"\nRun it:\npython my_analysis.py\nOverride parameters from command line:\npython my_analysis.py data.start_date=2022-01-01 model.name=fama_macbeth\n\n\n6.4.3 Configuration Composition\nHydra‚Äôs power comes from composing configurations. Organize your configs into groups:\nconf/\n‚îú‚îÄ‚îÄ config.yaml          # Main config with defaults\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ crsp.yaml       # CRSP data settings\n‚îÇ   ‚îú‚îÄ‚îÄ compustat.yaml  # Compustat settings\n‚îÇ   ‚îî‚îÄ‚îÄ local.yaml      # Local file settings\n‚îú‚îÄ‚îÄ model/\n‚îÇ   ‚îú‚îÄ‚îÄ ols.yaml\n‚îÇ   ‚îú‚îÄ‚îÄ fama_macbeth.yaml\n‚îÇ   ‚îî‚îÄ‚îÄ panel.yaml\n‚îî‚îÄ‚îÄ output/\n    ‚îú‚îÄ‚îÄ paper.yaml      # Publication-ready output\n    ‚îî‚îÄ‚îÄ debug.yaml      # Quick debug output\nThe main config selects defaults:\n# conf/config.yaml\ndefaults:\n  - data: crsp\n  - model: ols\n  - output: paper\n\nexperiment_name: \"baseline\"\nEach group config defines its settings:\n# conf/data/crsp.yaml\nsource: \"wrds\"\ndatabase: \"crsp\"\ntable: \"msf\"\nstart_date: \"1990-01-01\"\nend_date: \"2023-12-31\"\n# conf/model/fama_macbeth.yaml\nname: \"fama_macbeth\"\nrobust_se: true\nlags: 5\nSwitch configurations easily:\n# Use Compustat data with Fama-MacBeth model\npython my_analysis.py data=compustat model=fama_macbeth\n\n# Quick debug run\npython my_analysis.py output=debug data.end_date=2020-01-31\n\n\n6.4.4 Automatic Output Directories\nHydra automatically creates a unique output directory for each run:\noutputs/\n‚îî‚îÄ‚îÄ 2024-01-15/\n    ‚îî‚îÄ‚îÄ 14-30-22/\n        ‚îú‚îÄ‚îÄ .hydra/\n        ‚îÇ   ‚îú‚îÄ‚îÄ config.yaml      # Full resolved config\n        ‚îÇ   ‚îú‚îÄ‚îÄ hydra.yaml       # Hydra settings\n        ‚îÇ   ‚îî‚îÄ‚îÄ overrides.yaml   # Command-line overrides\n        ‚îú‚îÄ‚îÄ my_analysis.log      # Automatic logging\n        ‚îî‚îÄ‚îÄ results/             # Your output files\nThis makes every run reproducible‚Äîyou can see exactly what configuration was used.\n\n\n6.4.5 Using Hydra for Research Pipelines\nHere‚Äôs a more complete example for an empirical finance pipeline:\n# run_analysis.py\nimport logging\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\nimport pandas as pd\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_data(cfg: DictConfig) -&gt; pd.DataFrame:\n    \"\"\"Load data according to configuration.\"\"\"\n    logger.info(f\"Loading data from {cfg.data.source}\")\n\n    if cfg.data.source == \"local\":\n        df = pd.read_parquet(cfg.data.path)\n    elif cfg.data.source == \"wrds\":\n        # WRDS loading logic\n        pass\n\n    # Apply date filters\n    df = df[(df['date'] &gt;= cfg.data.start_date) &\n            (df['date'] &lt;= cfg.data.end_date)]\n\n    logger.info(f\"Loaded {len(df)} observations\")\n    return df\n\n\ndef run_model(df: pd.DataFrame, cfg: DictConfig) -&gt; dict:\n    \"\"\"Run the specified model.\"\"\"\n    logger.info(f\"Running {cfg.model.name} model\")\n\n    # Model logic here\n    results = {\"coefficients\": {}, \"stats\": {}}\n\n    return results\n\n\ndef save_results(results: dict, cfg: DictConfig) -&gt; None:\n    \"\"\"Save results according to configuration.\"\"\"\n    output_dir = Path(cfg.output.dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save based on format\n    if cfg.output.format == \"parquet\":\n        # Save as parquet\n        pass\n    elif cfg.output.format == \"latex\":\n        # Generate LaTeX tables\n        pass\n\n    logger.info(f\"Results saved to {output_dir}\")\n\n\n@hydra.main(version_base=None, config_path=\"conf\", config_name=\"config\")\ndef main(cfg: DictConfig) -&gt; None:\n    # Log the full configuration\n    logger.info(\"Configuration:\\n\" + OmegaConf.to_yaml(cfg))\n\n    # Run pipeline\n    df = load_data(cfg)\n    results = run_model(df, cfg)\n    save_results(results, cfg)\n\n    logger.info(\"Pipeline completed successfully\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n6.4.6 Multi-Run for Parameter Sweeps\nHydra can automatically run your code with multiple parameter combinations:\n# Run with multiple date ranges\npython run_analysis.py -m data.start_date=2010-01-01,2015-01-01,2020-01-01\n\n# Sweep over models\npython run_analysis.py -m model=ols,fama_macbeth,panel\nEach combination gets its own output directory with full configuration tracking.\nThis feature is particularly useful for testing the sensitivity of your analysis to empirical choices. For example, you can run your analysis with multiple winsorization levels, different sample periods, or alternative variable definitions to ensure your results are robust to these choices.\n\n\n6.4.7 Hydra with Logging\nHydra automatically configures Python‚Äôs logging module. Your log messages go to both the console and a file in the output directory:\nimport logging\nimport hydra\nfrom omegaconf import DictConfig\n\nlogger = logging.getLogger(__name__)\n\n@hydra.main(version_base=None, config_path=\"conf\", config_name=\"config\")\ndef main(cfg: DictConfig) -&gt; None:\n    logger.info(\"Starting analysis\")  # Automatically logged to file\n    logger.debug(\"Debug info\")  # Also captured\n\n    # Your code here\nYou can customize logging in conf/hydra/job_logging.yaml or in your main config.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Logging and Configuration</span>"
    ]
  },
  {
    "objectID": "python/logging/index.html#summary",
    "href": "python/logging/index.html#summary",
    "title": "6¬† Logging and Configuration",
    "section": "6.5 Summary",
    "text": "6.5 Summary\nProper logging and configuration management are essential for reproducible research:\n\nUse Python‚Äôs logging module instead of print statements for production code\nChoose appropriate log levels to distinguish routine information from warnings and errors\nInclude context in log messages so they‚Äôre meaningful when read later\nConfigure logging at the entry point, not in library modules\nUse Hydra for configuration management in complex research pipelines\nLeverage Hydra‚Äôs automatic output directories to make every run reproducible\n\nThese practices might seem like overhead for small scripts, but they pay dividends as projects grow. When you need to debug a failed run from last month or share code with collaborators, good logging and configuration management make the difference between hours of frustration and quickly finding the answer.\n\n\n\n\nYadan, Omry. 2019. ‚ÄúHydra - a Framework for Elegantly Configuring Complex Applications.‚Äù Github. https://github.com/facebookresearch/hydra.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Logging and Configuration</span>"
    ]
  },
  {
    "objectID": "python/tools/index.html",
    "href": "python/tools/index.html",
    "title": "7¬† Development Environment and Tools",
    "section": "",
    "text": "7.1 Fonts for Coding\nThis chapter covers setting up Visual Studio Code (VS Code) for Python data science work. A well-configured development environment makes coding more pleasant and efficient. We‚Äôll cover fonts, themes, extensions for formatting and linting, data exploration tools, and productivity enhancements.\nBefore opening VS Code, choose a good programming font. The right font makes code more readable and reduces eye strain during long sessions. Programming fonts have specific characteristics:\nLigatures are combined symbols that appear when certain characters follow each other. For example, -&gt; becomes a proper arrow ‚Üí, != becomes a not-equal symbol ‚â†, and &gt;= becomes a greater-than-or-equal symbol ‚â•. The underlying characters don‚Äôt change‚Äîyour code still contains -&gt;. Ligatures simply render more elegantly on screen. Whether to use ligatures is a personal choice; some developers love them, others find them distracting.\nHere are some recommended programming fonts:\nAll of these fonts are also available as Nerd Fonts versions. Nerd Fonts are patched versions that include additional icons and symbols. While they don‚Äôt add anything when coding in an editor, they make many terminal-based tools look nicer by enabling icons for file types, git status, and other visual indicators.\nPick the font that feels right to you, install it on your system, and we‚Äôll configure it in VS Code below.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Development Environment and Tools</span>"
    ]
  },
  {
    "objectID": "python/tools/index.html#fonts-for-coding",
    "href": "python/tools/index.html#fonts-for-coding",
    "title": "7¬† Development Environment and Tools",
    "section": "",
    "text": "Monospace: Every character has the same width, essential for code alignment\nDistinguishable characters: Clear differences between similar characters like 1, l, I and 0, O\nLigatures (optional): Special combined glyphs for common character sequences\n\n\n\n\nFiraCode: A popular programming font with excellent readability and comprehensive ligature support. It‚Äôs the font used throughout this book.\nJetBrains Mono: Designed specifically for developers by JetBrains (makers of PyCharm), featuring increased letter height for better readability and distinctive character shapes.\nMonaspace: A family of programming fonts published by GitHub with several variants optimized for different use cases.\nVictor Mono: Particularly nice-looking in terminal applications, with a distinctive style that some developers prefer.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Development Environment and Tools</span>"
    ]
  },
  {
    "objectID": "python/tools/index.html#vs-code-setup",
    "href": "python/tools/index.html#vs-code-setup",
    "title": "7¬† Development Environment and Tools",
    "section": "7.2 VS Code Setup",
    "text": "7.2 VS Code Setup\nIn this section, I present how I configure VS Code for research and data science work. These are my personal preferences‚Äîfeel free to adapt them to your own workflow.\n\n7.2.1 Theme and colors\nA good color theme makes your editor pleasant to look at during long coding sessions. My favorite is Catppuccin, which offers four variants: one light and three dark.\nTo install:\n\nOpen the Extensions panel (Ctrl+Shift+X or Cmd+Shift+X on Mac)\nSearch for ‚ÄúCatppuccin for VSCode‚Äù\nInstall the extension\nChoose your preferred variant (I use Mocha, the darkest)\n\nAlso install Catppuccin Icons for VS Code to get matching file icons in the explorer.\n\n\n\n\n\n\nTipConsistent theming\n\n\n\nCatppuccin is available for many applications beyond VS Code. If you like the color scheme, check out their website to theme your terminal, browser, and other tools consistently.\n\n\n\n\n7.2.2 Configuring your font\nTo set your chosen font in VS Code:\n\nOpen Settings (Ctrl+, or Cmd+, on Mac)\nSearch for ‚Äúfont‚Äù\nIn ‚ÄúEditor: Font Family‚Äù, enter your font name (e.g., Fira Code)\n\nTo enable ligatures, you need to edit the settings JSON directly:\n\nIn Settings, search for ‚ÄúFont Ligatures‚Äù\nClick ‚ÄúEdit in settings.json‚Äù\nAdd or modify:\n\n{\n    \"editor.fontFamily\": \"Fira Code\",\n    \"editor.fontLigatures\": true\n}\nSet fontLigatures to false if you prefer not to use them.\n\n\n7.2.3 Disabling the minimap\nThe minimap (the code preview on the right side of the editor) takes up space without adding much value for most workflows. To disable it:\n\nOpen Settings\nSearch for ‚Äúminimap‚Äù\nUncheck ‚ÄúEditor: Minimap: Enabled‚Äù\n\n\n\n7.2.4 Adding a ruler\nRuff (which we‚Äôll install next) limits lines to 88 characters by default. Adding a visual ruler helps you see this limit:\n\nOpen Settings and search for ‚Äúrulers‚Äù\nClick ‚ÄúEdit in settings.json‚Äù\nAdd:\n\n{\n    \"editor.rulers\": [88]\n}\nThis displays a vertical line at column 88, making it easy to arrange multiple files side by side.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Development Environment and Tools</span>"
    ]
  },
  {
    "objectID": "python/tools/index.html#extensions",
    "href": "python/tools/index.html#extensions",
    "title": "7¬† Development Environment and Tools",
    "section": "7.3 Extensions",
    "text": "7.3 Extensions\n\n7.3.1 Essential extensions\n\n7.3.1.1 Python extension\nThe Python extension is fundamental for Python development in VS Code. It provides:\n\nIntelliSense (smart code completion)\nLinting and error detection\nDebugging support\nJupyter notebook integration\nTest runner integration\n\nInstall it from the Extensions panel by searching for ‚ÄúPython‚Äù (by Microsoft).\n\n\n7.3.1.2 Ruff for formatting and linting\nRuff is a fast Python linter and formatter. Install the Ruff extension from the Extensions panel.\nTo configure automatic formatting on save, add to your settings.json:\n{\n    \"[python]\": {\n        \"editor.formatOnSave\": true,\n        \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n        \"editor.codeActionsOnSave\": {\n            \"source.organizeImports\": \"explicit\"\n        }\n    }\n}\nThis configuration:\n\nFormats your Python code automatically when you save\nUses Ruff as the default formatter\nOrganizes imports (alphabetically, grouped by standard library, third-party, and local imports)\nRemoves unused imports\n\nTo enable formatting for Jupyter notebooks as well:\n{\n    \"notebook.formatOnSave.enabled\": true\n}\n\n\n\n\n\n\nNoteWhat Ruff does\n\n\n\nRuff doesn‚Äôt change your code‚Äôs behavior‚Äîit reformats it to follow consistent conventions. For example, it fixes spacing, line breaks, and quote styles. It also removes unused imports and organizes your import statements.\n\n\n\n\n7.3.1.3 Markdown\nIf you write Markdown documents (notes, README files, documentation), these extensions are useful:\n\nmarkdownlint: Catches formatting issues and enforces consistent style.\nMarkdown All-in-One: Adds productivity features like keyboard shortcuts, table of contents generation, and list editing.\nMarkdown Preview Mermaid Support: Previews Mermaid diagrams in Markdown files. Mermaid is a standard diagramming format supported by GitHub and many documentation tools.\n\n\n\n\n7.3.2 Data science extensions\n\nRainbow CSV: Colorizes CSV files when you open them in VS Code, making it easier to distinguish columns in raw data files.\nData Wrangler: A powerful extension for exploring and cleaning pandas DataFrames interactively. When you load data with pandas, you can click ‚ÄúView Data‚Äù to open an interactive data explorer, filter and sort data visually, and generate Python code that replicates your data transformations. This extension is particularly valuable for data cleaning and initial data exploration.\n\n\n\n7.3.3 Git extensions\nVS Code has built-in Git support, but these extensions enhance it:\n\nGit Graph: Provides a visual representation of your repository‚Äôs branch structure and commit history. It‚Äôs essential for understanding complex branching scenarios.\nGitLens: By GitKraken, adds inline blame annotations showing who changed each line, detailed commit information, and file and line history. The free version includes many useful features; premium features require a subscription.\n\n\n\n7.3.4 AI coding assistant\nGitHub Copilot provides AI-powered code suggestions as you type. Install the GitHub Copilot extension and sign in with your GitHub account. A free tier is available with some limitations. Students and academics can apply for GitHub Education, which includes Copilot access.\n\n\n\n\n\n\nCautionAI tools require verification\n\n\n\nWhile AI assistants can boost productivity, always verify the code they generate. AI models can produce plausible-looking but incorrect code, especially for domain-specific tasks like financial calculations.\n\n\n\n\n7.3.5 Productivity extensions\n\nTODO Tree: Scans your project for comments containing TODO or FIXME and displays them in a sidebar panel. This helps you track tasks and issues scattered throughout your codebase.\nEven Better TOML: Adds syntax highlighting and validation for TOML files. TOML is the standard format for Python configuration files (pyproject.toml), so this extension is useful for any Python project.\nVim (VSCodeVim): For keyboard-driven editing without touching the mouse. Vim bindings have a steep learning curve but dramatically increase editing speed once mastered. If you‚Äôre new to Vim, also install Learn Vim, which provides an interactive tutorial.\n\n\n\n7.3.6 Dev containers\nThe Dev Containers extension lets you develop inside containerized environments, isolating your project dependencies and keeping your system clean. See the Dev Containers chapter for details on setting this up.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Development Environment and Tools</span>"
    ]
  },
  {
    "objectID": "python/tools/index.html#complete-settings-reference",
    "href": "python/tools/index.html#complete-settings-reference",
    "title": "7¬† Development Environment and Tools",
    "section": "7.4 Complete Settings Reference",
    "text": "7.4 Complete Settings Reference\nHere‚Äôs a complete settings.json incorporating the configurations discussed:\n{\n    \"editor.fontFamily\": \"Fira Code\",\n    \"editor.fontLigatures\": true,\n    \"editor.minimap.enabled\": false,\n    \"editor.rulers\": [88],\n    \"[python]\": {\n        \"editor.formatOnSave\": true,\n        \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n        \"editor.codeActionsOnSave\": {\n            \"source.organizeImports\": \"explicit\"\n        }\n    },\n    \"notebook.formatOnSave.enabled\": true\n}\nYou can access your settings file by opening the Command Palette (Ctrl+Shift+P or Cmd+Shift+P) and selecting ‚ÄúPreferences: Open User Settings (JSON)‚Äù.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Development Environment and Tools</span>"
    ]
  },
  {
    "objectID": "python/git/index.html",
    "href": "python/git/index.html",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "",
    "text": "8.1 What is version control?\nVersion control, also known as source control, is a system that records changes to a file or set of files over time so that you can recall specific versions later. It‚Äôs one of the most important tools in the toolkit of any developer or data scientist. It‚Äôs also very useful for researchers, especially those working with code, but in practice, it is underused in academia. The idea behind version control is quite simple: it allows you to track and manage changes to your projects. Think of it as the ‚Äútrack changes‚Äù feature in Microsoft Word, but for all your files and turbocharged with features that make it easy to collaborate with others.\nImagine you‚Äôre working on a research paper and decide to delete a section. A few days later, you realize that section was crucial. Without version control, you‚Äôd have to rewrite that entire section. With version control, you can simply look at your previous versions, find the one that includes the section you need, and restore it. Most of us have some kind of version control in our lives. For example, when you write a paper, you might save different versions of the document as you work on it. This way, if you make a mistake or delete something important, you can go back to a previous version. However, this approach has limitations, and there are better ways to manage versions of your work.\nIn the context of coding, version control is even more important. As you add new features to your code or fix bugs, it‚Äôs essential to be able to track these changes. If something breaks, you need to know what was changed so you can figure out what went wrong and how to fix it. Additionally, version control systems allow multiple people to work on the same project simultaneously, making collaboration easier and more efficient while keeping a detailed record of who made what changes and when.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#what-is-git",
    "href": "python/git/index.html#what-is-git",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.2 What is Git?",
    "text": "8.2 What is Git?\nGit is the most widely used version control system in the world. It was created in 2005 by Linus Torvalds, the creator of the Linux operating system. Torvalds wanted a version control system that was fast, efficient, and capable of handling small to very large projects with ease. Unlike its predecessors, Git was designed to be decentralized, allowing multiple developers to work on the same project simultaneously without stepping on each other‚Äôs toes. Like Linux, Git is free and distributed under an open-source license.\nAt its core, Git allows users to keep a complete history of their project, noting every change made to every file. This feature is akin to having a detailed logbook that captures the evolution of a project over time. With Git, users can branch off from the main project to experiment or work on new features without disrupting the core project. Later, these branches can be merged back into the main project seamlessly. This ability to branch and merge is particularly powerful, preventing conflicts and maintaining the integrity of the original project. Git is also incredibly robust in managing project history, enabling users to revert to previous versions if needed, offering a safety net against errors or unintended consequences of new changes.\n\n\n\n\n\n\nTipNot only for code\n\n\n\nGit is a great tool for version control of any kind of file, especially text files. It turns out that if you mainly use LaTeX or Markdown for writing and presentations, you can use Git to track changes in your documents and collaborate with others. Gone are the days of sending around files with names like paper_v1_final_final_really_final.tex and paper_v1_final_final_really_final_revised.tex!",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#what-is-github",
    "href": "python/git/index.html#what-is-github",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.3 What is GitHub?",
    "text": "8.3 What is GitHub?\nGitHub, launched in 2008 and acquired by Microsoft in 2018, quickly rose to become the de facto online platform for code management and collaboration. While Git is the engine, GitHub can be thought of as the sleek, user-friendly vehicle that houses this engine. It takes the core functionalities of Git and provides a web-based graphical interface that is intuitive and accessible. GitHub‚Äôs rise is not just due to its user-friendly nature but also because it functions like a social network for developers and researchers. Users can host their Git repositories, share their work with others, collaborate on projects, and even contribute to others‚Äô projects.\n\n\n\n\n\n\nNoteNot the only game in town.\n\n\n\nWhile GitHub is the most popular platform for code management and collaboration, it is not the only one. Two other popular platforms are GitLab and Bitbucket. Cloud providers like AWS and Azure also offer Git hosting services. Gitea is an open-source platform that can be self-hosted for free.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#why-use-git-and-github-for-research",
    "href": "python/git/index.html#why-use-git-and-github-for-research",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.4 Why use Git and GitHub for research?",
    "text": "8.4 Why use Git and GitHub for research?\nFor finance researchers, Git and GitHub offer a multitude of benefits. Git is an excellent tool for managing complex research projects. It allows researchers to track changes in their data analysis scripts, models, and even research papers, ensuring a clear audit trail of how the analysis was conducted and conclusions were reached. This level of transparency is crucial not just for personal record-keeping but also for collaborative projects where multiple researchers contribute to a single body of work. In a field where reputation is everything, Git can help researchers maintain a high level of integrity and accountability. The pull request system of GitHub is particularly beneficial for collaborative projects. It enables researchers to propose, discuss, and review changes before they are integrated into the main project. This not only ensures that every change is scrutinized for accuracy and relevance but also fosters a culture of peer review and collective improvement among collaborators as the project progresses. Furthermore, GitHub‚Äôs issue-tracking and project management features help researchers organize their tasks, track bugs, and manage project progress transparently.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#git-workflow",
    "href": "python/git/index.html#git-workflow",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.5 Git workflow",
    "text": "8.5 Git workflow\n\n8.5.1 Understanding the Core Concepts of Git\nGit‚Äôs power lies in its ability to manage and track changes in your projects, and this is achieved through a set of core functionalities. Let‚Äôs demystify these key terms:\n1. Repository (Repo): The heart of any Git project, a repository is like a project folder but with superpowers. It contains all of your project files along with each file‚Äôs revision history. You can have local repositories on your computer and synchronize them with remote repositories on GitHub to share and collaborate.\n2. Staging: Think of staging as a prep area. When you make changes to files, they don‚Äôt automatically get saved into your repository. Instead, you selectively add these changes to the staging area, indicating that you‚Äôve marked these modifications for your next commit.\n3. Commits: Committing is the act of saving your staged changes to the project‚Äôs history. A commit is like a snapshot of your repository at a particular point in time. Each commit has a unique ID and includes a message describing the changes, aiding future you or collaborators in understanding what was modified and why.\n4. Push and Pull: These are the methods by which you interact with a remote repository. When you push, you are sending your committed changes to a remote repo. Conversely, when you pull, you are fetching the latest changes from the remote repo to your local machine.\n5. Branching: Branching allows you to diverge from the main line of development and work independently without affecting the main project, often referred to as the main branch.1 It‚Äôs perfect for developing new features or experimenting.\n6. Merging: After you‚Äôve finished working in a branch, you merge those changes back into the main project. Merging combines the changes in your branch with those in the main branch, creating a single, unified history.\n7. Conflicts: Sometimes, when merging branches, Git encounters conflicts - changes that contradict each other. This can happen when two people make changes to the same file. These conflicts need to be manually resolved before completing the merge process.\nWe will explore these concepts in more detail in the next sections.\n\n\n8.5.2 Creating a repository\nA repository (or repo) is where all the magic happens ‚Äì it‚Äôs where your code, documentation, and all other project-related files reside. To create a new repo, simply log into your GitHub account, click on the + icon in the top right corner, and select New repository.\n\n8.5.2.1 Naming and Describing Your Repository\nChoose a name that succinctly reflects your project. Keep in mind that this name will be part of the URL for your repository, and that it will be used as the default name for the folder when you clone the repository (make a local copy of the repository on your computer). The description field is an opportunity to briefly outline your project‚Äôs objective. This helps others understand the purpose of your repo at a glance.\n\n\n8.5.2.2 Selecting a License\nYou can also define the license for your project. It is not necessary if you don‚Äôt intend on sharing this code publicly, but it is a good practice to include a license. When it comes to research code, transparency and accessibility are key. I recommend opting for a permissive license, like the MIT License. This license allows others to freely use, modify, and distribute your work ‚Äì perfect for fostering open-source collaboration in the research community. GitHub makes it easy to include a license; just select the MIT License from the dropdown menu when creating your repository. Other permissive licenses include the BSD License and the Apache License.\n\n\n8.5.2.3 Adding a .gitignore file\nBefore you start adding files to your repo, consider setting up a .gitignore file. This file tells Git which files or folders to ignore in a project. Typically, you‚Äôll want to exclude certain files from being tracked ‚Äì like temporary files, local configuration files, files containing sensitive information, or large data files. GitHub offers templates for .gitignore files tailored to various programming languages and frameworks, which can be a great starting point. It is available as a dropdown menu option when creating the repository. gitignore.io is another useful resource for generating .gitignore files.\n\n\n8.5.2.4 Adding a README file\nFinally, you‚Äôll want to add a README.md file to your repository. This file is the first thing visitors will see when they visit your repository on GitHub. It‚Äôs an essential component of your project, acting as the introduction and guide. Use the README to explain what your project does, how to set it up, and how to use it. This is important even if your project is not public, as it will help you remember how to use your project in the future and facilitate onboarding new collaborators. This file can be written in plain text or formatted using Markdown, a lightweight markup language that is easy to learn and use. GitHub automatically renders Markdown files, making them easy to read and navigate. You can also include images, links, and code snippets in Markdown files. GitHub offers a handy guide to help you get started with Markdown.\n\n\n\n8.5.3 Cloning a repository\nCloning a repository creates a local copy of the remote repository on your computer. This allows you to work on the project locally and push your changes to the remote repository when you‚Äôre ready to share them with others. To clone a repository, you‚Äôll need the URL of the remote repository. You can find this by clicking on the green Code button on the repository‚Äôs homepage. If you are using GitHub Desktop, you can clone the repository by selecting Open with GitHub Desktop, which will open the repository in GitHub Desktop. You can then select the location where you want to store the repository on your computer and click Clone.\nIf you are not using GitHub Desktop, you can clone the repository using the command line. First, copy the URL of the repository from the repository‚Äôs homepage by clicking on the green Code button, then copying the URL by clicking on the clipboard icon next to the URL. To clone the repository, open the terminal and navigate to the directory where you want to store the repository. Then, run the following command:\ngit clone &lt;url&gt;\nThis will create a new directory with the same name as the repository and download all the files from the remote repository into this directory. You can then open this directory in VS Code and start working on the project. From GitHub Desktop, you can open the repository in VS Code by selecting Open in Visual Studio Code from the Repository menu.\n\n\n8.5.4 Tracking changes\nOnce you have cloned the repository, you can start making changes to the files in the repository. You can create new files, edit existing files, or delete files. You can also move files around or rename them. You can see all your changes in the Source Control tab in VS Code. Files will be listed under Changes with a U if they are new (untracked), a M if they have been modified, or a D if they have been deleted. You can also see the changes you have made to each file by clicking on the file name.\nWhen you create a new file, it will not be tracked by Git until you add it to the staging area. To add a file to the staging area, you use the Stage Changes button in the Source Control tab in VS Code (the little + sign next to a file when you hover over it). You need to do this not only for new files, but for all files that you have modified or deleted since the last commit. Files in the staging area be included in the next commit.\nOnce you have added one or many changed files to the staging area, you can commit those changes to the repository. To commit changes, you need to enter a commit message describing the changes you have made and then click on the Commit button in the Source Control tab in VS Code (the checkmark icon). You can also use the keyboard shortcut (Command+Enter on Mac or Ctrl+Enter on Windows or Linux) to commit your changes. This will create a new commit, i.e., a new snapshot, with the changes you have staged.\nYou can see all your commits in the Source Control tab under Commits. You can click on a commit to see the changes that were made in that commit. You can also right-click on the commit to access the commit details, including the commit message, the author, and the date and time of the commit.\n\n\n8.5.5 Syncing with the remote repository\nAfter committing your changes locally in Visual Studio Code, the next step is to synchronize these changes with your remote repository on GitHub. This process involves two main actions: pulling changes from the remote repository and pushing your local changes to the remote.\n\n8.5.5.1 Pulling changes from the remote repository\nBefore you push your changes, it‚Äôs a good practice to pull any updates that others might have made to the remote repository. This ensures that your local repository is up-to-date. In VS Code, you can pull changes by clicking on the ... (more actions) button in the Source Control tab and selecting Pull. Alternatively, you can use the keyboard shortcut (Command+Shift+P on Mac or Ctrl+Shift+P on Windows/Linux) and type Git: Pull in the command palette. Pulling changes will merge updates from the remote repository into your local branch. If there are no conflicts, the merge will happen automatically.\n\n\n8.5.5.2 Pushing changes to the remote repository\nOnce your local branch is up-to-date and you‚Äôve committed your changes, you‚Äôre ready to push these changes to the remote repository. In the Source Control tab, click on the ... button and select Push. This will upload your commits to the remote repository on GitHub. You can also use the keyboard shortcut (Command+Shift+P on Mac or Ctrl+Shift+P on Windows/Linux) and type Git: Push in the command palette. If you‚Äôre pushing to a branch that doesn‚Äôt exist on the remote, VS Code will automatically create this branch in the remote repository.\n\n\n8.5.5.3 Resolving merge conflicts\nOccasionally, when you pull changes from the remote repository, you may encounter merge conflicts. These occur when changes in the remote repository overlap with your local changes in a way that Git can‚Äôt automatically resolve. VS Code provides tools to help resolve these conflicts. Conflicted files will be marked in the Source Control tab. You can open these files and choose which changes to keep. After resolving conflicts, you‚Äôll need to stage and commit the merged files before pushing.\nRegularly pulling and pushing changes will keep your local and remote repositories synchronized. This is crucial in collaborative projects to ensure everyone is working with the most current version of the project.\n\n\n\n8.5.6 Branching and merging\nBefore using Git, whenever I wanted to try something new in my code, I would make a copy of the entire project folder and work on that copy. This was a tedious process, and it was easy to lose track of which version was the most recent. With Git, branching makes this process much easier. Branching allows you to create a copy of your project, called a branch, and work on that branch without affecting the main project. Once you‚Äôre satisfied with the changes you‚Äôve made in your branch, you can merge those changes back into the main project. This process is much more efficient and less error-prone than manually copying and pasting files.\n\n8.5.6.1 Creating a New Branch\nIn VS Code, you can create a new branch by clicking on the branch name in the bottom left corner, then selecting Create new branch.... Give your branch a descriptive name that reflects its purpose. You can switch between branches by clicking on the branch name in the bottom left corner and selecting the branch you want to work on.\nAfter creating and switching to your new branch, any changes you make are confined to that branch. You can stage and commit changes in this branch as you would in the main branch.\nYou can also choose to publish your branch to the remote repository. This will create a copy of your branch on GitHub. This is useful if you want to collaborate with others on this branch, or to use GitHub to backup the branch. Note that once the branch is published, others who have access to the repository will be able to see that branch. To publish your branch, click on the ... button in the Source Control tab and select Publish Branch....\n\n\n8.5.6.2 Merging Branches\nOnce you‚Äôve completed the work in your branch and you‚Äôre satisfied with the changes, you‚Äôll want to merge these changes back into the main branch. Before merging, ensure your branch is up-to-date with the main branch. You can do this by checking out the main branch and pulling the latest changes, then switching back to your branch and merging the main branch into it. After that, you are ready to merge your branch into the main branch. After merging, you can delete your branch if you no longer need it. This avoids cluttering the repository with branches that are no longer needed.\nIn the Source Control tab, click on the ... button, select Merge Branch..., and choose the branch you want to merge into your current branch. If there are no conflicts, VS Code will complete the merge. VS Code will also ask you if you want to delete the merged branch.\nMerge conflicts happen when the same lines of code have been changed differently in both branches. VS Code will notify you if there are conflicts that need resolution. The first time, Git will also need you to confiure how you want to handle merge conflicts by entering one of the following commands in the terminal:\n\ngit config pull.rebase false: This command sets the pull behavior to merge. When you pull from a remote repository, Git will merge any incoming commits with your current branch. This is the one I usually use.\ngit config pull.rebase true: This command sets the pull behavior to rebase. Instead of merging incoming commits, Git will reapply your local commits on top of the incoming commits, creating a linear commit history.\ngit config pull.ff only: This command sets the pull behavior to fast-forward only. Git will only update your branch if it can fast-forward, meaning the main branch has not changed since you created your branch. If the main branch has new commits, Git will not pull the changes and you‚Äôll need to manually merge or rebase.\n\nConflicted files will be marked in the Source Control tab. Open these files, and VS Code will highlight the conflicting changes. Choose which changes to keep, then save the file, stage, and commit the resolved files. Once all conflicts are resolved and changes are committed, the branches are successfully merged. If you‚Äôve merged into your local main branch, don‚Äôt forget to push these changes to the remote repository to keep everything synchronized.\n\n\n\n8.5.7 Pull requests and code reviews\nA pull request (PR) is a method in GitHub to propose changes from one branch to another, typically from a feature into the main branch. It‚Äôs a request to pull in your changes. The name is a bit misleading because it‚Äôs not related to the pull command in Git. You can think of it as a ‚Äúmerge request‚Äù instead. When you create a PR, you‚Äôre initiating a discussion about your proposed changes. Your collaborators can review the code, leave comments, request changes, or approve the PR.\n\n8.5.7.1 Creating a Pull Request in GitHub\nOnce you have pushed your branch to the remote repository, you can create a PR. Navigate to the repository on GitHub.com. GitHub often shows a prompt to create a PR for recent branches. If not, go to the Pull Requests tab and click New pull request. Select your branch and the branch you want to merge into (usually the main branch). When creating a PR, include a clear title and a detailed description of the changes. This helps reviewers understand the context and purpose of the changes. You can also assign reviewers to the PR, add labels, and set a milestone. Once you‚Äôre satisfied with the PR, click Create pull request. Any assigned reviewers will be notified of the PR and can begin reviewing it.\n\n\n8.5.7.2 Code Reviews\nCollaborators can review the changes in a PR by navigating to the Files changed tab within the PR. Reviewers can leave comments on specific lines of code, general comments on the PR, and suggest changes. They can also pull the branch locally and test the changes themselves. Once the review is complete, the reviewer can approve the PR, request changes, or leave a comment. If changes are requested, the PR author can make the requested changes and push them to the branch. The PR will be automatically updated with the new changes. Once the PR is approved, it can be merged into the target branch. Based on the feedback, you might need to make additional commits to your branch. These updates will automatically appear in the PR. This back-and-forth can continue until the changes are satisfactory.\n\n\n8.5.7.3 Merging the Pull Request\nOnce the PR is approved and any conflicts are resolved, you can merge it into the target branch. This is typically done via the ‚ÄòMerge pull request‚Äô button on GitHub. After merging, it‚Äôs a good practice to delete the feature branch from the remote repository to keep the branch list tidy.\n\n\n8.5.7.4 Best Practices for Pull Requests and Code Reviews\n\nSmall, Focused Changes: Aim for smaller, manageable PRs that focus on a specific feature or fix. This makes code reviews more efficient and less overwhelming.\nClear Communication: Use clear, descriptive messages in both your PRs and commits. This helps reviewers understand your thought process and the changes made.\nConstructive Feedback: When reviewing, offer constructive and respectful feedback. Code reviews are not just about finding mistakes but also about sharing knowledge and improving the codebase collaboratively.\n\nPull requests and code reviews are vital for maintaining high-quality code and fostering collaboration in your finance research projects. While not yet commonly used in academia, I have found them the perfect tools for collaborating on research projects. They ensure that every change is scrutinized and understood by all collaborators, and they foster a culture of peer review and collective improvement as the project progresses.\nGitHub offers many other features that can be useful for research projects. I list them at the end of this post and will cover them in a future post.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#github-for-research-code",
    "href": "python/git/index.html#github-for-research-code",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.6 GitHub for research code",
    "text": "8.6 GitHub for research code\nIn empirical finance research, the ability to reproduce results is more important now than ever, especially that most top journals require authors to share their code and data. In this section, I will discuss some best practices I have adopted for using GitHub to manage research code, with an emphasis on reproducibility, documentation, and effective use of GitHub‚Äôs features.\nThe first thing to consider after creating a new repository is the structure of your project. A well-organized project is easier to navigate and understand, and it makes it easier for others to reproduce your work. There is no one-size-fits-all approach to organizing a project, but the following project structure is a good starting point:\nproject\n‚îú‚îÄ‚îÄ data/\n‚îú‚îÄ‚îÄ docs/\n‚îú‚îÄ‚îÄ output/\n    ‚îú‚îÄ‚îÄ figures/\n    ‚îî‚îÄ‚îÄ tables/\n‚îú‚îÄ‚îÄ src/\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ .env\n‚îú‚îÄ‚îÄ .env-example\n‚îú‚îÄ‚îÄ conf.yaml\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ uv.lock\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ requirements.txt\nSo, which files should you commit to your repository? Here are some guidelines:\nYou should include:\n\nConfiguration files: Files like .json, .yml, or .ini are crucial for ensuring that your project can be set up and run by others with the exact same parameters you used. In my example the conf.yaml file contains the configuration parameters for the project and should be included in the repository.\nSource code: Include all scripts and code files that are essential for your analysis or model. In my example, the src directory contains all the Python scripts used in the project.\nDocumentation: Any files that help explain your project, especially markdown files with notes. In my example, the docs directory contains the documentation for the project and should be included in the repository. If your documentation is generated from source files, such as Markdown or Latex, then you should include the source files in the repository, not the generated files. Your repository should also include a README file at the root of the repository that provides an overview of your project, its purpose, and how to use it.\nDependencies: For projects in languages like Python, a file listing the dependencies is essential. This file lists all the external libraries and their specific versions needed for your project. This ensures that anyone cloning your repository can easily install the necessary dependencies and run your code in an environment identical to yours. In my example, I use uv to manage dependencies, so I include the pyproject.toml and uv.lock files. The lock file records the exact versions of all dependencies, ensuring reproducible environments across machines. I also include a requirements.txt file for users who prefer to install dependencies using pip instead of uv.\n.gitignore file: This file tells Git which files or folders to ignore in a project. Typically, you‚Äôll want to exclude certain files from being tracked ‚Äì like temporary files, local configuration files, files containing sensitive information, or large data files. GitHub offer templates, but they seem to be missing a few things. For example, if you are on Mac you will want to add .DS_Store to your ignore file. gitignore.io is another useful resource for generating .gitignore files that are much more comprehensive. Make sure to also add the .gitignore file to your repository.\n\nFinally, make sure that you include in the .gitignore file all the files and directories that you should not include in the repository.\nYou should not include:\n\nData files: While large datasets might not be feasible to store on GitHub, even small datasets can be problematic if they are updated often. Instead, consider including sample datasets or scripts that automatically fetch or generate data, or sharing your data among collaborators using a cloud storage service like Dropbox or Google Drive. There exist tools like DVC that can help you manage large datasets with version control, but I have not used them myself.\nSensitive data and local configuration files: Do not include sensitive information like passwords or API keys, or computer-specific configuration parameters such as local paths. Instead, you should include an example file with the expected parameters that need to be set in the configuration file. In my example, I use a .env file to store sensitive and local information, and I include a .env-example file that contains the name of the environment variable that needs to be set in the .env file. I would then include the .env-example file in the repository, but not the .env file. I also include the .env file in the .gitignore file so that it is not included in the repository.\nOutput: You should not include output files in the repository. Instead, you should include the code that generates the output files. Every collaborator should be able to generate the results in his environment. In my example, the output directory contains the figures and tables generated by the code in the src directory.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#git-and-jupyter-notebooks",
    "href": "python/git/index.html#git-and-jupyter-notebooks",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.7 Git and Jupyter notebooks",
    "text": "8.7 Git and Jupyter notebooks\nJupyter Notebooks are a popular tool for data analysis and visualization. They allow users to combine code, text, and visuals in one document, making it easy to share and collaborate on data science projects. However, Jupyter Notebooks have many shortcomings when it comes to replicability and using them with Git can be challenging.\n\n8.7.1 Challenges with Git and Jupyter notebooks\nJupyter Notebooks, while an excellent tool for data analysis and visualization, present unique challenges when used with Git. The core issue lies in their format: Notebooks save both the input (code) and the output (results, graphs, etc.) in a single JSON file. This means that even small changes in the code can lead to large changes in the file, making it difficult for Git to handle diffs and merges effectively. The output sections, especially those with visual content, can create ‚Äúnoise‚Äù in version control. When different users run the same notebook, slight differences in output can appear, leading to unnecessary conflicts. Because Git keeps track of the full history of the notebook, the size of the repository can grow quickly, especially if the notebook contains large outputs such as images. This can make it difficult to share and collaborate on notebooks.\n\n\n8.7.2 VS Code Notebook Diff Viewer\nRecognizing these challenges, tools like Visual Studio Code have introduced features to help. The VS Code diff viewer (the tool that shows differences in files due to changes) supports Jupyter notebooks, allowing users to compare and understand changes between notebook versions more easily. This tool provides a clearer visualization of differences in the code, reducing the complexity involved in tracking changes in notebooks in Git.\n\n\n8.7.3 Using Notebooks with Online Platforms\nDespite these challenges, Jupyter Notebooks remain a popular and powerful tool for data analysis and research. Their interactive nature and the ability to combine code, text, and visuals in one document make them invaluable.\nPlatforms like Binder and Google Colab integrate well with Jupyter Notebooks hosted on GitHub. These platforms can automatically create interactive, shareable environments from notebooks, making them more accessible for collaborative work and education. By using these platforms, researchers can share their notebooks in a more user-friendly and interactive format, ensuring that others can easily replicate and experiment with their findings.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#github-for-writing",
    "href": "python/git/index.html#github-for-writing",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.8 GitHub for writing",
    "text": "8.8 GitHub for writing\nGitHub is not just for code; it‚Äôs also an excellent platform for tracking your writing, especially if you are using formats based on plain-text files such as Markdown (like the Quarto publishing system) and LaTeX.\nThe same principles for organizing code projects apply to writing projects. You should include all the files that are essential to generate the output of your project, such as the source (e.g.¬†.md, .tex, and .bib) files, configuration files, and tables and figures. You should avoid including the output files, such as PDFs, or HTML files.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#tagging-releases-for-milestones",
    "href": "python/git/index.html#tagging-releases-for-milestones",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.9 Tagging releases for milestones",
    "text": "8.9 Tagging releases for milestones\nThere are times when you want to create a snapshot of your project, including the output, at a specific point in time. For example, when you submit a paper to a journal, you want to create a snapshot of the project at that point in time. This allows you to keep track of the changes made in between revisions. GitHub provides a way to do this using tags and releases.\nWhen you reach a significant milestone in your writing ‚Äì such as the completion of a draft, submission to a journal, or final revisions ‚Äì you can create a tag and a corresponding release.\nTo create a tag and release, head to the repository on GitHub.com and click on Create a new release under Releases in the right sidebar. Enter a tag version number and a title for the release. You can also add release notes summarizing the changes or updates in this version. Finally, attach the output files (e.g.¬†PDFs) to the release. Click Publish release to create the release. You can then download the release files or share the release link with others.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#publishing-your-code-on-github",
    "href": "python/git/index.html#publishing-your-code-on-github",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.10 Publishing your code on GitHub",
    "text": "8.10 Publishing your code on GitHub\nIn empirical finance academic research, sharing your code has become increasingly important. Publishing your code enhances the transparency and reproducibility of your research. It allows peers to review, replicate, and build upon your work, contributing to the collective knowledge of the field. Making your code available can also increase the citation and impact of your research, as it provides tangible artifacts that others can use and reference. Finally, it is also a requirement for publishing in many journals, including the top ones.\nJournals will publish your code alongside your paper, so why should you also publish it on GitHub?\nFor me, the main reason is to keep control over my code. By publishing your code on GitHub, you retain control over it. You can continue to make changes to it, and update it as needed. Other researchers who visit your GitHub repository can also be exposed to your other work, increasing the visibility of your research. Finally, GitHub offers a platform for collaboration and feedback, allowing others to flag issues, contribute to your work, and build upon it.\nTo publish your code on GitHub, all you need to do is set the visibility of your repository to public. If you don‚Äôt want to share the full history of your code, you can create a new repository and upload the latest version of your code.\nMake sure to include a README file that explains what your project does, how to set it up, and how to use it. Documentation is key to making your code accessible to others (and to reducing the number of questions you get about your code). You can also include instructions on how to cite your code in the README file. Finally, you should also include a LICENSE file to clearly state how others can use your code.\nOnce you have completed these steps, your code is published! If you want a DOI for your repository, you can use Zenodo, which allows you to mint a DOI for your GitHub repository.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#other-github-features-for-academic-researchers",
    "href": "python/git/index.html#other-github-features-for-academic-researchers",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "8.11 Other GitHub features for academic researchers",
    "text": "8.11 Other GitHub features for academic researchers\nIn addition to the core features of GitHub, many other tools and functionalities can be useful for academic researchers. I plan on covering most of them in future posts. Here are the ones that I use the most:\n\n8.11.1 Project Management Tools\nGitHub offers several tools to help you manage your projects, including Projects, Issues, Discussions, and Wikis. These tools can be used to organize your work, track tasks, and collaborate with others.\n\n\n8.11.2 GitHub Copilot\nGitHub Copilot is an AI coding assistant. It can do code completion, suggest functions, and even generate code based on comments. There is also a Copilot Chat powered by GPT-4 that can answer questions about code while being aware of the context. When you allow it, it can consult your private repositories to provide more relevant suggestions.\nSeriously, if you haven‚Äôt tried it yet, you should. It‚Äôs a game-changer, and new features are being added all the time. And it will work for text too if you write your Markdown or LaTeX files in VS Code.\n\n\n8.11.3 GitHub Pages\nGitHub Pages is a free service that allows you to host static websites directly from GitHub. This can be useful for hosting project websites, blogs, or personal websites. My personal website and this blog are both hosted on GitHub Pages.\n\n\n8.11.4 GitHub Classroom\nGitHub Classroom simplifies the use of GitHub in classroom settings. It‚Äôs a toolset that automates the repetitive tasks involved in grading and feedback, making it easier to use GitHub for coursework and assignments in a research or academic context. I have been using it for three years and it has been a game-changer for me. While it‚Äôs not bug-free, it has saved me countless hours of grading and feedback. Automated grading has a monthly limit after which you need to pay, but the cost is minimal and well worth it.\n\n\n8.11.5 GitHub Actions\nGitHub Actions is a powerful tool that allows you to automate workflows. You can set up CI/CD pipelines2 to automate testing, building, and deploying your applications or research code. GitHub Actions are small scripts that run in response to events in your repository, such as commit or pull requests. For researchers, it can be used to automate the testing of code or even automate routine data processing tasks.\n\n\n8.11.6 GitHub Codespaces\nGitHub Codespaces provide a fully featured cloud development environment accessible directly from GitHub. Your code lives in a remote server, and you get a complete VS Code environment in your browser. This can be particularly useful for researchers who want to quickly experiment with code or collaborate without the need to set up a local development environment. It is also great to ensure maximum replicability of the code you distribute, as the environment is identical for everyone.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/git/index.html#footnotes",
    "href": "python/git/index.html#footnotes",
    "title": "8¬† Version Control using Git and GitHub",
    "section": "",
    "text": "Historically, this branch was called master, but GitHub has recently changed the default branch name to main to avoid the racially charged connotations of the word master.‚Ü©Ô∏é\nContinuous integration and continuous development‚Ü©Ô∏é",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Version Control using Git and GitHub</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html",
    "href": "python/ai-for-coding/index.html",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "",
    "text": "9.1 Why This Chapter Exists\nArtificial intelligence tools have transformed how programmers write code. Large language models can explain syntax, generate functions from natural language descriptions, and debug error messages in seconds. For students learning Python for empirical finance research, these tools offer an appealing shortcut: why struggle with cryptic error messages when an AI can explain them? Why write boilerplate code by hand when an AI can generate it instantly?\nThis chapter addresses a fundamental tension. AI coding tools can genuinely accelerate your work and reduce frustration, but they can also undermine the skill formation that makes you a capable researcher. The difference between these outcomes depends entirely on how you use these tools. Used wisely, AI becomes a force multiplier for your growing expertise. Used carelessly, it becomes a crutch that prevents you from developing the understanding you need.\nWe begin with the core trade-offs, then cover the practical landscape of AI coding tools, and conclude with a workflow designed to help you benefit from AI assistance while still building genuine programming competence.\nThe premise of this chapter is straightforward: AI can accelerate learning, but it cannot replace it. If you cannot read, understand, and debug the code that AI produces, you are flying blind. Learning Python remains non-negotiable for credible empirical research. AI shifts where your effort goes, not whether effort is required.\nConsider what happens when an AI writes code for you. The tool produces syntactically correct Python that might even run without errors on your first attempt. But what happens when you need to modify that code for a different dataset? What happens when it produces incorrect results that look plausible? What happens when a collaborator asks you to explain your methodology? If you cannot answer these questions, the AI-generated code is a liability rather than an asset.\nThe goal of this chapter is not to discourage you from using AI tools. They are genuinely useful, and most professional programmers now use them in some capacity. The goal is to help you use these tools in ways that build rather than erode your capabilities. This requires understanding what AI tools can and cannot do, recognizing when to rely on them and when to step back, and developing habits that keep you in control of your own code.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#why-this-chapter-exists",
    "href": "python/ai-for-coding/index.html#why-this-chapter-exists",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "",
    "text": "WarningAI does not understand your research\n\n\n\nAI coding assistants have no conception of what makes empirical research valid. They cannot distinguish between code that runs and code that correctly implements your methodology. They cannot verify that your variable definitions match your research design. They cannot ensure that your sample selection avoids look-ahead bias. These judgments require domain expertise that only you can provide.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#ai-skill-formation-and-career-constraints",
    "href": "python/ai-for-coding/index.html#ai-skill-formation-and-career-constraints",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.2 AI, Skill Formation, and Career Constraints",
    "text": "9.2 AI, Skill Formation, and Career Constraints\nThe most important reason to develop genuine Python skills, even when AI tools are available, is that over-reliance on AI slows long-run skill acquisition. Programming proficiency comes from struggling with problems, making mistakes, and building mental models of how code works. When AI removes that struggle, it also removes the learning.\nThink about the difference between using a calculator for arithmetic and understanding how arithmetic works. Calculators are faster and more reliable for computation, but if you never learned arithmetic, you cannot estimate whether an answer is reasonable. You cannot catch errors. You cannot extend the calculation in ways the calculator does not support. The same dynamic applies to AI and coding. AI can produce code faster than you can type it, but if you never developed the underlying understanding, you cannot evaluate, modify, or debug what it produces.\n\n9.2.1 The industry reality\nMany financial institutions restrict or prohibit external AI tools. Banks, asset managers, and hedge funds handle sensitive data and proprietary strategies. Sending code snippets or error messages to external AI services creates compliance and security risks that many firms will not accept. Some institutions run air-gapped systems where internet access is simply unavailable. Others have policies that prohibit sharing any code or data with third parties, including AI providers.\nIf you are targeting industry roles after your degree, this matters. Code you write with AI assistance still needs to be maintained without it. When you join a firm with restrictive policies, you need to be able to read, understand, and modify code on your own. The skills you develop now determine whether you will be effective in those environments.\nEven in academic settings, AI dependence creates problems. Universities and journals increasingly have policies that restrict or require disclosure of AI use in research. Peer reviewers may ask you to explain or modify your methodology. Collaborators may need to extend your code. You may need to debug issues years after the original analysis. In all these cases, you need genuine understanding of what your code does and why.\n\n\n9.2.2 Building transferable skills\nThe bottom line is that AI is a supplement, not a crutch. The goal is to develop skills that remain valuable regardless of what tools are available. This means understanding Python syntax, data structures, and control flow well enough to write code from scratch when necessary. It means being able to read and understand code you did not write. It means having mental models of how your analysis works that go beyond the specific implementation.\nWhen you use AI tools, pay attention to whether you are learning or just producing output. If you find yourself repeatedly asking the AI to solve similar problems, that is a signal to step back and build understanding. If you cannot explain what the AI-generated code does, you should not use it.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#ethical-academic-and-scientific-constraints",
    "href": "python/ai-for-coding/index.html#ethical-academic-and-scientific-constraints",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.3 Ethical, Academic, and Scientific Constraints",
    "text": "9.3 Ethical, Academic, and Scientific Constraints\nBefore discussing how to use AI tools effectively, we need to address the constraints that govern their use in academic and professional settings. These are not optional guidelines. They are requirements that determine whether your work is acceptable.\n\n9.3.1 Academic integrity and authorship\nThe fundamental principle is simple: responsibility for correctness always lies with the researcher. When you submit code as part of a research project or assignment, you are asserting that the code correctly implements your methodology and that you understand what it does. AI assistance does not change this responsibility. If AI-generated code contains errors that invalidate your results, you bear the consequences.\nDifferent institutions and courses have different policies about AI use. Some prohibit it entirely for certain assignments. Others permit it with disclosure requirements. The specific rules vary, but the underlying principle is consistent: you must be able to explain and defend every line of code you submit. If you cannot, the work is not genuinely yours, regardless of how it was produced.\n\n\n9.3.2 Reproducibility and auditability\nEmpirical research must be reproducible. Other researchers should be able to examine your methodology, run your code, and verify your results. This requires that your code be inspectable and explainable. AI-generated code that you do not understand fails this requirement, even if it produces the correct output.\nWhen reviewers or collaborators ask about your implementation, you need to be able to explain why you made specific choices. Why did you winsorize at the 1st and 99th percentiles rather than the 5th and 95th? Why did you cluster standard errors at the firm level rather than the industry level? These questions require understanding that goes beyond the code itself to the research design it implements.\n\n\n9.3.3 Data confidentiality\nNever paste proprietary data, credentials, or restricted information into AI tools. This applies to actual data values, but also to code that reveals the structure of proprietary datasets or the logic of trading strategies. Most AI tools send your input to external servers for processing. Even tools that claim privacy protections may retain data for training or analysis.\nIn practice, this means being careful about what you share. Error messages that include data values should be sanitized before sharing. Code that implements proprietary methodology should be described in general terms rather than pasted directly. When in doubt, assume that anything you share with an AI tool could become public.\n\n\n\n\n\n\nCautionA simple rule for submissions\n\n\n\nIf you would not submit work without AI, you should not submit it with AI. AI can help you produce better work faster, but it cannot make you capable of work you could not otherwise do. When you submit code, you are asserting that you could have written it yourself given sufficient time. If that is not true, the submission is not appropriate.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#mental-models-for-using-ai-effectively",
    "href": "python/ai-for-coding/index.html#mental-models-for-using-ai-effectively",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.4 Mental Models for Using AI Effectively",
    "text": "9.4 Mental Models for Using AI Effectively\nThe key to using AI tools effectively is having the right mental model for what they are and what they can do. This section provides frameworks for thinking about AI that will help you use it productively, especially as a beginner.\n\n9.4.1 AI as a junior assistant\nThe most useful mental model is to think of AI as a junior assistant that writes drafts quickly and carelessly. Like a junior employee, AI can produce output fast, but that output requires review and correction. The assistant has broad knowledge but limited judgment. They will confidently produce work that contains subtle errors. They will follow instructions literally even when those instructions are ambiguous or misguided.\nThis mental model has several implications. First, you should expect to review and edit everything the AI produces. Just as you would not submit a junior employee‚Äôs first draft, you should not use AI-generated code without careful examination. Second, the quality of your instructions matters enormously. Vague requests produce vague outputs. Specific, well-structured prompts produce better results. Third, you remain responsible for the final product. The assistant helps, but you make the decisions.\n\n\n9.4.2 Why precise prompts matter\nPrecise prompts matter more than model choice. The difference between a good prompt and a bad prompt often exceeds the difference between AI models. A well-crafted prompt includes context about what you are trying to accomplish, specific requirements for the output, and examples of what success looks like. A poor prompt leaves the AI guessing about your intentions.\nConsider the difference between these prompts:\n\nPoor: ‚ÄúWrite code to calculate returns‚Äù\nBetter: ‚ÄúWrite a Python function that calculates simple returns from a list of prices. The function should take a list of floats representing daily closing prices and return a list of floats representing daily returns. Handle the edge case of the first day, which has no previous price, by returning None for that position.‚Äù\n\nThe second prompt specifies the input format, output format, programming language, and edge case handling. It gives the AI enough information to produce something useful. The first prompt leaves all these decisions to the AI, which may or may not match your needs.\n\n\n9.4.3 The iteration loop\nEffective AI use follows an iteration loop: prompt, inspect, test, revise. You start with a prompt describing what you need. You inspect the output to understand what the AI produced. You test the code to verify it works correctly. You revise either the prompt or the code based on what you learned.\n\n\n\n\n\n\nflowchart LR\n    A[Prompt] --&gt; B[Inspect]\n    B --&gt; C[Test]\n    C --&gt; D{Works?}\n    D --&gt;|No| E[Revise]\n    E --&gt; A\n    D --&gt;|Yes| F[Done]\n\n\n\n\nFigure¬†9.1: The AI-assisted coding iteration loop\n\n\n\n\n\nThis loop emphasizes that AI assistance is not a one-shot process. You should expect multiple iterations, especially for complex tasks. Each iteration teaches you something about both the problem and the AI‚Äôs capabilities. Over time, you learn to write better prompts and to anticipate where AI is likely to make mistakes.\n\n\n9.4.4 Treat every response as a hypothesis\nPerhaps the most important mental model is to treat every AI response as a hypothesis, not an answer. The AI is proposing a solution that might be correct. Your job is to test that hypothesis by reading the code, understanding what it does, and verifying that it behaves correctly.\nThis mindset prevents the most dangerous failure mode: accepting AI output without verification. When you treat AI responses as hypotheses, you naturally engage your critical faculties. You ask questions. You check assumptions. You test edge cases. This engagement is precisely what builds understanding.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#general-purpose-chatbots-for-research-coding",
    "href": "python/ai-for-coding/index.html#general-purpose-chatbots-for-research-coding",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.5 General-Purpose Chatbots for Research Coding",
    "text": "9.5 General-Purpose Chatbots for Research Coding\nGeneral-purpose AI chatbots are the most familiar category of AI coding tools. These are web-based interfaces where you type questions in natural language and receive conversational responses. The major tools in this category include OpenAI ChatGPT, Anthropic Claude, Google Gemini, and Microsoft Copilot in chat mode. Each has different strengths and pricing, but for most research coding tasks, the differences are less important than how you use them. For research coding, they serve as explanation-first tools: they excel at helping you understand rather than at producing production-ready code.\n\n9.5.1 What they are good at\nChatbots excel at explaining syntax and error messages. When you encounter a Python error that you do not understand, a chatbot can explain what went wrong in plain language. It can describe what the error message means, why it occurred, and how to fix it. This is often faster and more helpful than searching documentation or Stack Overflow.\nThey are also good at translating ideas into rough code sketches. If you have a conceptual understanding of what you want to do but are unsure how to express it in Python, a chatbot can provide a starting point. The code may need refinement, but it gives you something concrete to work with and learn from.\nClarifying library usage is another strength. Python has thousands of packages with varying documentation quality. A chatbot can explain how to use a specific function, what parameters it accepts, and what it returns. It can compare alternatives and explain trade-offs. This is particularly helpful when learning new libraries.\n\n\n9.5.2 What they are bad at\nChatbots are poor at designing correct empirical workflows. They do not understand your research question, your data structure, or the methodological requirements of your field. The most recent models, especially those with reasoning capabilities, are improving in this area, but not yet to the point where you can trust them to be reliable without verification. They can produce code that runs but implements the wrong analysis. Detecting these errors requires domain expertise that the chatbot lacks.\nThey also struggle with understanding your project context by default. Each conversation with a chatbot starts fresh. The AI does not know what code you have already written, what data you are working with, or what you have tried before. You need to provide this context explicitly, which is time-consuming and error-prone.\n\n\n9.5.3 Typical research uses\nFor empirical finance research, chatbots are most useful for:\n\nDebugging tracebacks: Paste an error message and ask for an explanation. The chatbot can often identify the problem and suggest fixes.\nRefactoring simple logic: Describe what your current code does and ask for a cleaner implementation. Review the suggestion carefully before adopting it.\nLearning new concepts: Ask for explanations of Python features, statistical methods, or library functions. Use the chatbot as an interactive tutorial.\nGenerating boilerplate: Request template code for common patterns like reading CSV files or setting up matplotlib figures. Customize for your specific needs.\n\n\n\n\n\n\n\nTipUsing chatbots for learning\n\n\n\nChatbots make excellent tutors. When you do not understand a piece of code, ask the AI to explain it line by line. When you are confused about a concept, ask for multiple examples at different difficulty levels. When you get stuck on a problem, ask for hints rather than solutions. This approach builds understanding rather than dependence.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#in-editor-code-completion-and-inline-assistance",
    "href": "python/ai-for-coding/index.html#in-editor-code-completion-and-inline-assistance",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.6 In-Editor Code Completion and Inline Assistance",
    "text": "9.6 In-Editor Code Completion and Inline Assistance\nIn-editor completion tools provide a different kind of AI assistance. Rather than conversational interaction, they offer continuous, low-friction suggestions as you type. The major tools in this category are GitHub Copilot, which integrates with Visual Studio Code and other editors, and Cursor, an editor with deep AI integration built in. Cursor is a fork of VS Code, so it offers a familiar interface and supports most VS Code extensions, excluding those published by Microsoft. These tools analyze your current file and context to predict what you are likely to write next, then offer to complete it for you.\nIf you have used auto-complete on your phone, you already understand the basic experience. The AI predicts what you are likely to type next and offers to complete it for you. Most of the time the suggestions are helpful and save keystrokes. But occasionally the AI makes silly mistakes, inserting words that are grammatically correct but contextually wrong. Code completion works the same way: usually helpful, occasionally nonsensical, and always requiring your judgment about whether to accept.\n\n9.6.1 How completion differs from chat\nThe interaction model is fundamentally different from chatbots. With a chatbot, you formulate a request, submit it, wait for a response, then evaluate and possibly revise. With completion tools, suggestions appear automatically as you type. You accept with a keystroke or ignore by continuing to type. The feedback loop is measured in seconds rather than minutes.\nThis difference has important implications. Completion tools are embedded in your normal workflow. You do not need to context-switch to another application. The AI sees your entire file and often your entire project, so it has much more context than a chatbot conversation. Suggestions are smaller and more incremental, completing lines or functions rather than generating entire programs.\n\n\n9.6.2 Why this is often safer for beginners\nIn-editor completion is often safer for beginners because it produces smaller changes that are easier to evaluate. When a chatbot generates a 50-line function, understanding and verifying it requires significant effort. When a completion tool suggests finishing the line you are already writing, you can evaluate it immediately. The suggestion is within the context you already understand.\nThe immediate visual context also helps. You see the suggestion alongside your existing code. You can compare it to what you were planning to write. Obvious errors are more apparent because they conflict with the surrounding code that you wrote and understand.\n\n\n9.6.3 Common risks\nThe primary risk with completion tools is silent logical errors. The AI might suggest syntactically correct code that does not do what you intend. Because suggestions appear quickly and accepting them is easy, you might accept code without fully processing what it does. This is especially dangerous for statistical operations where the code runs without errors but produces incorrect results.\nOver-engineered suggestions are another issue. The AI might suggest more complex solutions than necessary, introducing abstractions or edge case handling that you do not need. This complexity makes the code harder to understand and maintain. Simpler code that you fully understand is usually better than sophisticated code that you do not.\n\n\n9.6.4 Improving suggestions with context\nCompletion tools work better when they have more context about your code. The AI analyzes your file and project to understand what you are trying to do, and richer context leads to better suggestions.\nType hints and docstrings, discussed in Chapter 4, serve double duty here. They make your code more readable for humans and more understandable for AI. When you declare that a function takes a pd.DataFrame and returns a float, the AI can suggest code that correctly uses DataFrame methods and returns an appropriate value. When you write a docstring explaining that a function calculates annualized volatility from daily returns, the AI understands the domain and can suggest relevant implementations.\nThis creates a virtuous cycle. Writing type hints and docstrings is good practice regardless of AI tools. But if you use completion tools, that investment pays additional dividends through better suggestions. The same practices that make your code maintainable also make it more AI-friendly.\nConversely, poorly documented code with no type hints gives the AI little to work with. Suggestions will be more generic and less likely to match your intent. If you find that completion suggestions are consistently unhelpful, consider whether adding context to your code might improve them.\n\n\n9.6.5 When to accept a suggestion\nA useful rule of thumb: accept a suggestion when you could have written it yourself, just more slowly. This standard ensures that you understand what you are accepting. The AI is saving you keystrokes, not doing your thinking.\nIf a suggestion surprises you, pause before accepting. Either you are learning something new (good), or the suggestion is wrong (bad). Either way, the surprise is a signal that you should engage more deeply rather than just accepting. Take time to understand why the AI suggested what it did.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#coding-agents",
    "href": "python/ai-for-coding/index.html#coding-agents",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.7 Coding Agents",
    "text": "9.7 Coding Agents\nCoding agents represent the most powerful and most dangerous category of AI coding tools. Unlike completion tools that suggest small changes, agents can make extensive modifications across multiple files. Unlike chatbots that produce isolated code snippets, agents can execute code, observe results, and iterate. The major tools in this category include GitHub Copilot in agent mode, Anthropic‚Äôs Claude Code, and OpenAI Codex. Both Claude Code and OpenAI Codex are available as command-line tools and as VS Code extensions. Cursor also includes deep agent integration. These tools evolve rapidly, so specific recommendations become outdated quickly, but at the time of writing, Claude Code with the Opus 4.5 model is my tool of choice.\nFor all AI coding tools, but especially agents, reading the product documentation is essential to achieve the best results. Each tool behaves differently, with its own strengths, limitations, and recommended workflows. Adapting your approach to the specific tool you use will greatly improve the quality of the output you get. Time spent learning the tool‚Äôs features and best practices pays off quickly.\n\n\n\n\n\n\nNote Video\n\n\n\nIn this video, I go through an example use case of Claude Code with a financial research application.\n\n\n\n\n9.7.1 What coding agents actually do\nAgents operate with a significant degree of autonomy. You describe a goal, and the agent takes a series of actions to achieve it: writing code, running tests, reading error messages, and revising its approach. Some agents can browse documentation, search codebases, and even interact with external services. They do in minutes what might take you hours.\nThis capability is transformative for experienced developers who can evaluate agent output quickly. An agent can implement a feature, write tests, and handle edge cases while the developer reviews and directs. The human remains in control but operates at a higher level of abstraction.\n\n\n9.7.2 Why they are dangerous for novice programmers\nFor novice programmers, agents are dangerous precisely because of their power. An agent might modify your code in ways you do not understand. It might introduce bugs that are difficult to detect because you do not know what was changed. It might solve problems in ways that work but that you cannot maintain or extend.\nThe core issue is verification. When an agent makes changes across multiple files, verifying correctness requires understanding all those files. A novice lacks this understanding. They may accept changes that seem to work but contain subtle errors. They may lose track of what their code does and why. They may find themselves unable to make further modifications without agent assistance.\n\n\n\n\n\n\nWarningThe danger of ‚Äúfix everything‚Äù requests\n\n\n\nNever ask an agent to ‚Äúfix everything‚Äù or ‚Äúmake it work‚Äù on code you do not understand. This request gives the agent maximum autonomy with minimum oversight. You have no way to evaluate whether the resulting changes are correct. Even if the code runs, it may not implement what you intended. Even if it produces plausible output, that output may be wrong.\n\n\n\n\n9.7.3 Appropriate research use cases\nAgents are appropriate when you already understand the code being modified. Useful applications include:\n\nRefactoring code you already understand: If you know what the code does and want to improve its structure, an agent can help. You can verify that the refactored version preserves the original behavior.\nGenerating tests or documentation drafts: Agents can produce initial test cases or docstrings that you then review and refine. The human provides the specification; the agent provides the implementation.\nApplying consistent changes across files: When you need to rename a variable, update a function signature, or apply a similar change in many places, agents can do this quickly and reliably.\nRapid prototyping: Agents can quickly produce working code to test whether an idea is feasible or to explore how a library works. Treat this as throw-away code for learning and experimentation, not as the foundation for your actual analysis.\n\n\n\n9.7.4 Inappropriate use cases\nAgents are inappropriate when you cannot verify their output:\n\nWriting core empirical logic: The code that implements your research methodology must be code you understand. Agents should not write this code for you.\nModifying unfamiliar code: If you do not understand code before the agent modifies it, you definitely will not understand it after.\nDebugging without understanding: When code does not work, the solution is understanding, not automation. An agent might fix the symptom while leaving the underlying problem.\n\n\n\n9.7.5 Version control is essential\nUsing version control with Git is not optional when working with coding agents. Agents make extensive changes across multiple files, and you need the ability to review those changes before accepting them and to revert if something goes wrong. Git‚Äôs diff tools let you see exactly what the agent modified, line by line. Without version control, you have no systematic way to understand or undo what the agent did.\nCommit frequently when working with agents. Before asking the agent to make changes, ensure your working directory is clean. After the agent finishes, review the diff carefully before committing. If the changes are wrong, you can discard them and try a different approach.\nFor advanced workflows, Git‚Äôs worktree feature lets you maintain multiple working directories from the same repository, each on a different branch. This enables running multiple agents in parallel on separate tasks without conflicts. Each agent works in its own worktree, and you merge the results when ready.\n\n\n\n\n\n\nNote Video\n\n\n\nFor a quick introduction to git worktrees, see this video.\n\n\n\n\n\n9.7.6 Security and permissions\nCoding agents require significant permissions to do their work. They need to read your files, write new code, and often execute commands. This power creates security risks that you should take seriously.\nBe cautious about what you allow agents to access. An agent with permission to execute arbitrary shell commands could potentially damage your system, exfiltrate data, or install malicious software. Most agents have safety measures, but these are not foolproof. Never run agents with elevated privileges, and be wary of granting access to sensitive directories or credentials.\nDevelopment containers, discussed in Chapter 10, provide an effective safety layer. By running agents inside a container, you isolate them from your main system. Even if something goes wrong, the damage is contained. The container can be reset to a clean state, and your host system remains protected. For any serious agent use, especially with less-established tools, running in a devcontainer is a sensible precaution.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#ai-assisted-workflow-for-research",
    "href": "python/ai-for-coding/index.html#ai-assisted-workflow-for-research",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.8 AI-Assisted Workflow for Research",
    "text": "9.8 AI-Assisted Workflow for Research\nThis section presents a workflow designed to help you benefit from AI assistance while maintaining understanding and control. The principles apply regardless of which specific tools you use.\n\n9.8.1 Start from human-written pseudocode\nBefore asking AI for help, write pseudocode describing what you want to accomplish. This forces you to think through the logic before delegating to AI. The pseudocode becomes both your specification for the AI and your reference for evaluating its output.\nPseudocode does not need to be syntactically correct Python. It should describe the steps of your computation in terms you understand:\n# Calculate portfolio returns for each month\n# For each month:\n#   Get the weights at the start of the month\n#   Get the stock returns during the month\n#   Multiply each weight by its corresponding return\n#   Sum to get portfolio return\n# Return the list of monthly portfolio returns\nWith this pseudocode, you can ask AI to help implement specific steps. You have a clear reference for whether the implementation matches your intention.\n\n\n9.8.2 Use AI to reduce syntax friction, not to invent logic\nAI should help you express ideas you already have, not generate ideas for you. If you do not know what analysis to run, AI cannot tell you. If you know what analysis to run but are unsure of the Python syntax, AI can help.\nThis distinction keeps you in the role of researcher rather than consumer of AI output. You make the methodological decisions. AI helps you implement those decisions efficiently. The resulting code reflects your understanding because it implements your logic.\n\n\n9.8.3 Run and test after every AI-assisted change\nNever accumulate multiple AI-generated changes before testing. After each change, run the code and verify it behaves correctly. Check edge cases. Compare results against manual calculations or known benchmarks. This immediate feedback catches errors early when they are easy to diagnose.\nIf you accept several AI suggestions before testing, and something goes wrong, you do not know which change caused the problem. You may need to undo everything and start over. Testing incrementally avoids this.\n\n\n\n\n\n\nWarningAI-generated tests require careful review\n\n\n\nAI tools are good at writing tests, which can accelerate your workflow. However, when AI writes both the code and the tests, you lose an important check on correctness. Review AI-generated tests carefully to ensure they actually test the right behavior and cover edge cases. A test that passes is only meaningful if the test itself is correct.\nBe especially careful with coding agents. When tests fail, agents have a tendency to modify the tests to make them pass rather than fixing the underlying code. This defeats the purpose of testing entirely. Always review diffs carefully to ensure the agent fixed the function, not the test.\n\n\n\n\n\n\n\n\nflowchart TB\n    A[Write pseudocode] --&gt; B[Implement one step]\n    B --&gt; C{AI help&lt;br&gt;needed?}\n    C --&gt;|Yes| D[Prompt AI with&lt;br&gt;specific request]\n    D --&gt; E[Review output&lt;br&gt;line by line]\n    E --&gt; F[Modify if needed]\n    F --&gt; G[Test immediately]\n    C --&gt;|No| G\n    G --&gt; H{Works&lt;br&gt;correctly?}\n    H --&gt;|No| I[Debug with&lt;br&gt;understanding]\n    I --&gt; B\n    H --&gt;|Yes| J[Commit to&lt;br&gt;version control]\n    J --&gt; K{More steps&lt;br&gt;remaining?}\n    K --&gt;|Yes| B\n    K --&gt;|No| L[Done]\n\n\n\n\nFigure¬†9.2: AI-assisted workflow for research coding\n\n\n\n\n\n\n\n9.8.4 Use version control aggressively between iterations\nCommit working code to version control before making AI-assisted changes. If the changes break something, you can easily revert. This safety net encourages experimentation while protecting against mistakes.\nGood commit messages document what you changed and why. When you later review your project history, you can see the evolution of your analysis. This is valuable for debugging, for collaboration, and for your own understanding. AI coding assistants are good at writing detailed commit messages that summarize the changes made. Just make sure you review the message before submitting the commit to ensure it accurately describes the work.\n\n\n9.8.5 Never merge AI-generated code you cannot explain line by line\nThe final check is simple: can you explain every line of the code? If someone asked why a particular line exists, could you answer? If the code stopped working, would you know where to look?\nIf you cannot answer these questions, you should not use the code. Take time to understand it, or ask the AI to explain it, or write it yourself. The short-term cost of slower progress is worth the long-term benefit of genuine understanding.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#local-and-open-models",
    "href": "python/ai-for-coding/index.html#local-and-open-models",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.9 Local and Open Models",
    "text": "9.9 Local and Open Models\nCloud-based AI tools are convenient but not always appropriate. This section covers alternatives that run on your own hardware, avoiding the need to send data to external servers.\nLocal models address several concerns that matter for research. Your prompts and code never leave your machine, eliminating concerns about data retention, training on your inputs, or security breaches at the provider. Organizations that prohibit external AI tools may permit local ones since the code stays within your controlled environment. Local inference has no per-token charges, so after the initial setup, usage costs only electricity. And local models work without internet access, which matters for air-gapped systems or unreliable connections.\nRunning AI models locally requires significant computing resources. Effective local models require Apple Silicon Macs or PCs with modern GPUs; older hardware often cannot run useful models at reasonable speed. Memory is usually the binding constraint, with larger models requiring more of it. On Apple computers, the system uses unified memory where RAM is shared between the CPU and GPU, so a machine with 32GB or more of RAM can run substantial models. On PCs, the relevant constraint is typically VRAM on the GPU rather than system RAM, since the model must fit in GPU memory to run efficiently. Model files themselves are large, often multiple gigabytes each, so keeping several models available requires substantial storage. Local AI also requires more technical setup than cloud services: you need to install software, download models, and configure integration with your editor.\nTwo popular options for running AI models locally are Ollama and LM Studio. Ollama is a command-line tool that makes running local models straightforward, handling model downloads, memory management, and inference. Many editor integrations support Ollama as a backend. LM Studio provides a graphical interface for downloading and running local models, making it more accessible for users uncomfortable with command-line tools, and includes a built-in chat interface.\nSeveral tools support local model backends for coding assistance. Some configurations of GitHub Copilot allow using local models instead of cloud APIs, though capabilities may be reduced. Open-source alternatives like Continue provide Copilot-like functionality with local model support. Command-line agents like Claude Code can also be configured with local model providers, though this requires additional setup.\nLocal models currently trail cloud models in capability. The most capable models require hardware that exceeds what most individuals own, so local models are typically smaller and less capable. Cloud providers invest heavily in tooling that local solutions cannot match, and you are responsible for updates, troubleshooting, and configuration. These trade-offs may be acceptable when privacy or institutional requirements demand local operation, but for learning purposes, cloud tools are usually more practical.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/ai-for-coding/index.html#summary-rules-you-should-actually-follow",
    "href": "python/ai-for-coding/index.html#summary-rules-you-should-actually-follow",
    "title": "9¬† Using AI for Coding in Empirical Research",
    "section": "9.10 Summary: Rules You Should Actually Follow",
    "text": "9.10 Summary: Rules You Should Actually Follow\nThis chapter has covered many considerations, but effective AI use comes down to a few key principles:\nIf you cannot read the code, you should not use the code. This is the fundamental rule. AI can help you write code faster, but you must understand what you are using. If you cannot explain it, you cannot debug it, maintain it, or defend it.\nAI accelerates feedback, not understanding. The AI can produce code quickly and identify errors quickly. But understanding comes from your engagement with the code. Use AI to speed up the feedback loop, not to skip the learning.\nIn research and industry, credibility beats convenience. Your reputation depends on the quality and correctness of your work. AI-generated code that contains errors damages your credibility. Code you understand and can explain builds it.\nStart with what you know. Write pseudocode before asking for help. Use AI to implement your ideas, not to have ideas for you. The research decisions are yours; AI helps with the implementation.\nTest relentlessly. Every AI-generated change should be followed by testing. Catch errors early, when they are easy to diagnose. Never assume that code is correct because AI produced it.\nMaintain version control. Commit working code before making changes. Create a safety net that lets you experiment confidently and recover from mistakes.\nTreat AI responses as hypotheses. The AI is proposing solutions, not providing answers. Your job is to evaluate those proposals critically, accept what works, and reject what does not.\nThese principles will serve you regardless of how AI tools evolve. The specific tools will change. The specific capabilities will improve. But the fundamental dynamic remains: AI is a tool that amplifies your capabilities, for better or worse. Used wisely, it makes you more effective. Used carelessly, it prevents you from becoming effective at all.\nThe choice is yours.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Using AI for Coding in Empirical Research</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html",
    "href": "python/devcontainers/index.html",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "",
    "text": "10.1 Setting Up Devcontainers in VS Code\nIn empirical finance research, the tools and methodologies we employ play a crucial role in ensuring the integrity and reproducibility of our findings. One such powerful tool is containerization, which allows you to encapsulate your code and its dependencies into a standardized unit. Development containers, or devcontainers, provide a convenient way to create isolated environments for your data analysis tasks, ensuring that your code runs consistently across different systems.\nContainers are essentially lightweight, portable environments that package up code and all its dependencies, ensuring that the software runs consistently across different computing environments. This consistency is particularly valuable in research settings where the reproducibility of results is paramount. By containerizing their code, researchers can avoid the ‚Äúit works on my machine‚Äù problem, ensuring that their analyses can be replicated by others, regardless of the underlying system configurations. You might be familiar with the use of tools like uv or pyenv for managing Python environments to separate dependencies for different projects. Containers take this concept a step further by encapsulating the entire environment, including the operating system, runtime, libraries, and configurations. This ensures that the code runs identically on any machine, making it easier for researchers to share their work and for others to verify their findings.\nThis encapsulation also provides an added level of safety and security that is essential in research settings. By isolating the code and its dependencies from the host system, containers protect the environment from potential security vulnerabilities or malware disguised as python librairies. This isolation ensures that the code runs in a controlled environment, reducing the risk of unintended interactions with the host system. For additional security, you can use third-party services like Snyk to scan your dependencies for known vulnerabilities, or use curated package repositories like Anaconda or PyX that vet packages before making them available.\nFinally, containers facilitate collaboration among researchers. When a project is containerized, collaborators can easily set up their environment by simply running the container, eliminating the often cumbersome process of manually installing and configuring dependencies. This ease of setup promotes a more efficient workflow and reduces the likelihood of errors, making collaborative research more streamlined and productive. Even if you work solo, containers improve the resiliency of your research workflows, making it easy to recover from a broken or stolen computer by reinstalling your project on a new computer without worrying about compatibility issues. You can even run your code in the cloud with services like GitHub Codespaces, ensuring that your analyses are not tied to a specific machine or operating system.\nIn this chapter, I provide a step-by-step guide on setting up devcontainers in VS Code, with a focus on supporting Python uv and mounting local directories for file storage.\nAll the code and configurations used in this tutorial are available in the GitHub repository.\nDevelopment containers are a feature in VS Code that leverages Docker1 to create and manage containers specifically for development purposes. This allows developers and researchers to work within a consistent environment, which is crucial for complex data analysis tasks. Setting up devcontainers in VS Code is straightforward, but there are a few prerequisites you need to have in place: Docker, Visual Studio Code, and the Dev Containers extension. If you‚Äôre on macOS, you can install Docker using Homebrew:\nTo begin, you‚Äôll need to create a .devcontainer folder in your project directory. Inside this folder, you should create a devcontainer.json file, which will define the configuration for your development container. This file specifies the base image for the container, any additional tools or libraries that need to be installed, and other settings related to the development environment. By configuring this file, you can tailor the container to meet the specific needs of your data analysis tasks. This can be done using the Dev Containers extension in VS Code, which provides a user-friendly interface for creating and managing devcontainers. To get started, simply invoke Dev Containers: Add Development Container Configuration Files... from the command palette in VS Code and follow the prompts to create your devcontainer.json file.\nThis will prompt you to select a base image for your container, configure any additional tools or libraries, and set up other environment settings. As default options, I like to use the following:\nOnce you have configured your devcontainer.json file, you should get a prompt to reopen the project in the container. This will build the container based on the specified configuration and open your project within the containerized environment. You can verify that the container is running by checking the status bar in VS Code, which should indicate that you are working in a containerized environment.\nYou can always rebuild the container by invoking the Dev Containers: Rebuild and Reopen in Container command from the command palette. This will recreate the container based on the latest configuration settings, ensuring that your development environment is up-to-date and consistent with your project requirements.\nHere is what a simple devcontainer.json file (minus the comments) looks like:\nThis configuration references a shell script that will be executed after the container is created. This script handles installing uv and setting up the project dependencies (more on this later).",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#setting-up-devcontainers-in-vs-code",
    "href": "python/devcontainers/index.html#setting-up-devcontainers-in-vs-code",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "",
    "text": "brew install --cask docker\n\n\n\nSetup location: in workspace\nBase image: Python 3.12 image from Microsoft\nFeatures: Any additional tools you need (e.g., Quarto, GitHub CLI)\n\n\n\n\n\n\n\nTipPython version\n\n\n\nWhile the current latest version is 3.14, a the time of writing the latest Python base image from Microsoft is 3.12. This is not much of an issue because uv will install its own Python version when creating your environment based on your specification in pyproject.toml.\n\n\n\n\n\n{\n  \"name\": \"Python 3\",\n  \"image\": \"mcr.microsoft.com/devcontainers/python:1-3.12-bullseye\",\n  \"postCreateCommand\": \"./.devcontainer/postCreateCommand.sh\"\n}",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#images",
    "href": "python/devcontainers/index.html#images",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "10.2 Images",
    "text": "10.2 Images\nImages are a crucial aspect of containerization, as they define the base environment for your development container. When setting up a devcontainer in VS Code, you can choose from a variety of pre-built images that provide different programming languages, tools, and libraries. These images serve as the foundation for your development environment, ensuring that the necessary dependencies are available for your data analysis tasks. By tying your devcontainer to a specific image, you can guarantee that your code runs consistently across different systems, making it easier to share and replicate your work.\nNote: Most images are based on Linux distributions, so you may need to adjust your code or configurations if you are used to working on Windows or macOS. However, the differences are usually minimal and can be easily managed within the container.\nNote 2: Most images specify the environment (i.e.¬†Linux version, Python version, etc.), but not the architecture, which makes them compatible with most systems. For example, most PCs and older Macs use Intel or AMD CPUs with the x86_64 architecture, while newer Macs with Apple Silicon have the arm64 architecture. The images are usually compatible with both architectures, but that means that the environment will not be 100% identical if you run the container on different architectures. This is usually not a problem for data analysis tasks, but it‚Äôs something to keep in mind if you are having issues with your code running differently on different systems.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#features",
    "href": "python/devcontainers/index.html#features",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "10.3 Features",
    "text": "10.3 Features\nFeatures are additional tools or libraries that can be installed in your development container to enhance its functionality. These features can include language-specific tools, package managers, or development environments that are tailored to your project requirements. By specifying features in your devcontainer.json file, you can extend the capabilities of your container and ensure that it is well-suited for your data analysis tasks. You can find a list of available features at containers.dev/features. For example, you can add Quarto support with the ghcr.io/rocker-org/devcontainer-features/quarto-cli:1 feature, or add the GitHub CLI with ghcr.io/devcontainers/features/github-cli:1.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#uv",
    "href": "python/devcontainers/index.html#uv",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "10.4 uv",
    "text": "10.4 uv\nuv is an extremely fast Python package and project manager written in Rust, designed to replace tools like pip, pip-tools, pipx, poetry, and more. It simplifies the process of creating, managing, and sharing Python projects by providing a unified interface for dependency management, packaging, and virtual environment handling. uv uses a pyproject.toml file to define project dependencies, scripts, and configurations, making it easy to manage project settings and requirements. By integrating uv into your devcontainer setup, you can streamline your development workflow and ensure that your Python projects are well-organized and reproducible.\nFor example here is a pyproject.toml file that defines the dependencies for a Python project:\n[project]\nname = \"my-project\"\nversion = \"0.1.0\"\ndescription = \"My research project\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.13,&lt;4.0\"\ndependencies = [\n    \"pandas&gt;=2.2.2\",\n    \"numpy&gt;=2.0.0\",\n]\n\n[dependency-groups]\ndev = [\n    \"pytest&gt;=7.2.0\",\n    \"ruff&gt;=0.11.5\",\n]\nWith uv, project dependencies are defined in the standard [project] section following PEP 621, rather than in a tool-specific section. This makes your pyproject.toml file more portable across different Python tools. You can also define development dependencies in a separate [dependency-groups] section, which allows you to install them only when needed.\nTo set up uv in your devcontainer, create a postCreateCommand.sh script in your .devcontainer folder:\n#!/usr/bin/env bash\n\n# Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install project dependencies\nuv sync\nThis script installs uv and then runs uv sync to install all project dependencies into a virtual environment (.venv). The uv sync command reads your pyproject.toml file and creates a reproducible environment with all specified dependencies.\nTo use this script, reference it in your devcontainer.json file:\n  \"postCreateCommand\": \"./.devcontainer/postCreateCommand.sh\"\nMake sure the script is executable by running chmod +x .devcontainer/postCreateCommand.sh before committing it to your repository.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#mounting-local-directories",
    "href": "python/devcontainers/index.html#mounting-local-directories",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "10.5 Mounting Local Directories",
    "text": "10.5 Mounting Local Directories\nBy default, the development container is isolated from the host system, except for the workspace directory, which is mounted into the container. This ensures that your project files are accessible within the container, allowing you to work on your code seamlessly. However, there are cases where you may need to access files or directories outside the workspace, such as large data files. To achieve this, you can mount local directories into the development container, making them available within the container environment.\nTo mount a local directory, you can add the mounts property to your devcontainer.json file, specifying the source path on the host and the target path in the container.\nHere is an example configuration for mounting a local directory:\n  \"mounts\": [\"source=/path/to/local/directory,target=/workspace/data,type=bind,consistency=cached\"]\nThis setup ensures that the directory /path/to/local/directory on your host machine is accessible within the container at /workspace/data. This approach provides the flexibility to work with local files while benefiting from the isolated environment of the container, except for the mounted directories.\nNote: This mounting process limits the flexibility of the container, as the source directory must be available on the host system. If you are working on a shared project or need to access files from different locations, you may need to consider alternative approaches, such as using a networked file system or cloud storage. As far as I know, there is no way to specify the source directory using an environment variable, so each collaborator would need to update the devcontainer.json file with their local path.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#vs-code-extensions",
    "href": "python/devcontainers/index.html#vs-code-extensions",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "10.6 VS Code Extensions",
    "text": "10.6 VS Code Extensions\nYou can also use the devcontainer.json file to configure default VS Code seettings and install VS Code extensions in your development container. When working with devcontainers, you will notice that not all the extensions you have installed on your host system are available in the container environment. Specifically, extensions that are mostly used for the host system (think UI), such as themes or language packs, will still be available. However, extensions that require access to the container environment, such as language servers or debuggers, will need to be installed. For example, the python image will install the Python extension, but you may need to install additional extensions for specific tasks, such as the Jupyter extension for working with Jupyter notebooks or with the interactive window. It can also be useful to make sure that all collaborators use the same formatting tools, such as Ruff.\nTo address this, you can specify the extensions and settings you want to install in the devcontainer.json file. For example, this will install Jupyter and Ruff, configure Ruff as the default formatter, and point VS Code to the virtual environment created by uv:\n    \"customizations\": {\n        \"vscode\": {\n            \"extensions\": [\n                \"ms-toolsai.jupyter\",\n                \"charliermarsh.ruff\"\n            ],\n            \"settings\": {\n                \"[python]\": {\n                    \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n                    \"editor.codeActionsOnSave\": {\n                        \"source.fixAll\": \"explicit\",\n                        \"source.organizeImports\": \"explicit\"\n                    }\n                },\n                \"python.defaultInterpreterPath\": \"${workspaceFolder}/.venv/bin/python\",\n                \"editor.formatOnSave\": true\n            }\n        }\n    }\nNote the python.defaultInterpreterPath setting, which tells VS Code to use the Python interpreter in the .venv virtual environment created by uv sync. This ensures that VS Code uses the correct Python environment with all your project dependencies.\nYou can find the extension IDs by looking at the extension in VS Code, then clicking on the gear icon and selecting ‚ÄúCopy Extension ID‚Äù.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#limitations",
    "href": "python/devcontainers/index.html#limitations",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "10.7 Limitations",
    "text": "10.7 Limitations\nDevcontainers are a powerful tool for creating isolated development environments, but they do have some limitations. One of the main drawbacks is the overhead associated with running containers, which can slow down the development process, especially for large projects or resource-intensive tasks. It‚Äôs not an issue that I have found to be significant, but it‚Äôs something to keep in mind if you are working on a particularly demanding project or have limited system resources.\nAdditionally, the isolation provided by containers can make it challenging to use some resources from the host system, such as GPUs or hardware peripherals. While it is possible to pass through devices to the container using Docker, this process can be complex and may not be suitable for all use cases. For example, on Apple Silicon Macs, even if the Linux container could access the GPU, there are no Linux drivers available for the GPU, so it would not be able to use it.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  },
  {
    "objectID": "python/devcontainers/index.html#footnotes",
    "href": "python/devcontainers/index.html#footnotes",
    "title": "10¬† Stay Safe with Devcontainers",
    "section": "",
    "text": "Other alternatives such as Podman and Colima can be used, but the official documentation is centered around Docker. See the VS Code documentation for supported alternatives.‚Ü©Ô∏é",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stay Safe with Devcontainers</span>"
    ]
  }
]